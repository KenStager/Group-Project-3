3
2
0
2

b
e
F
6
2

]
I

A
.
s
c
[

2
v
5
0
2
6
0
.
2
0
3
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2023

ORDER MATTERS: AGENT-BY-AGENT POLICY OPTI-
MIZATION

Xihuai Wang1,2â, Zheng Tian3â , Ziyu Wan1,2, Ying Wen1, Jun Wang2,4, Weinan Zhang1â 
1 Shanghai Jiao Tong University, 2 Digital Brain Lab,
3 ShanghaiTech University, 4 University College London

ABSTRACT

While multi-agent trust region algorithms have achieved great success empiri-
cally in solving coordination tasks, most of them, however, suffer from a non-
stationarity problem since agents update their policies simultaneously. In contrast,
a sequential scheme that updates policies agent-by-agent provides another per-
spective and shows strong performance. However, sample inefï¬ciency and lack
of monotonic improvement guarantees for each agent are still the two signiï¬cant
challenges for the sequential scheme. In this paper, we propose the Agent-by-
agent Policy Optimization (A2PO) algorithm to improve the sample efï¬ciency
and retain the guarantees of monotonic improvement for each agent during train-
ing. We justify the tightness of the monotonic improvement bound compared
with other trust region algorithms. From the perspective of sequentially updating
agents, we further consider the effect of agent updating order and extend the the-
ory of non-stationarity into the sequential update scheme. To evaluate A2PO, we
conduct a comprehensive empirical study on four benchmarks: StarCraftII, Multi-
agent MuJoCo, Multi-agent Particle Environment, and Google Research Football
full game scenarios. A2PO consistently outperforms strong baselines.

1

INTRODUCTION

Trust region learning methods in reinforcement learning (RL) (Kakade & Langford, 2002) have
achieved great success in solving complex tasks, from single-agent control tasks (Andrychowicz
et al., 2020) to multi-agent applications (Albrecht & Stone, 2018; Ye et al., 2020). The methods
deliver superior and stable performances because of their theoretical guarantees of monotonic policy
improvement. Recently, several works that adopt trust region learning in multi-agent reinforcement
learning (MARL) have been proposed, including algorithms in which agents independently update
their policies using trust region methods (de Witt et al., 2020; Yu et al., 2022) and algorithms that
coordinate agentsâ policies during the update process (Wu et al., 2021; Kuba et al., 2022). Most
algorithms update the agents simultaneously, that is, all agents perform policy improvement at the
same time and cannot observe the change of other agents, as shown in Fig. 1c. The simultaneous
update scheme brings about the non-stationarity problem, i.e., the environment dynamic changes
from one agentâs perspective as other agents also change their policies (Hernandez-Leal et al., 2017).

In contrast to the simultaneous update scheme,
algorithms that sequentially execute agent-by-
agent updates allow agents to perceive changes
made by preceding agents, presenting another
perspective for analyzing inter-agent interac-
tion (Gemp et al., 2022). Bertsekas (2021) pro-
posed a sequential update framework, named
Rollout and Policy Iteration for a Single Agent
(RPISA) in this paper, which performs a rollout
every time an agent updates its policy (Fig. 1a).
RPISA effectively turns non-stationary MARL
problems into stationary single agent reinforce-
ment learning (SARL) ones. It retains the theo-

Figure 1: The taxonomy on the rollout scheme
and the policy update scheme.

âWork done at Digital Brain Lab.

â Correspondence to Zheng Tian <tianzheng@shanghaitech.edu.cn>

and Weinan Zhang <wnzhang@sjtu.edu.cn>.

1

RolloutRolloutRolloutRolloutMultipleRolloutsat a Stage SingleRollout ata Stage SequentialPolicyUpdateSimultaneousPolicyUpdateA Stage: All Agents are UpdatedRolloutUpdateUpdateUpdateUpdateUpdateUpdateUpdate(a)(b)(c) 
 
 
 
 
 
Published as a conference paper at ICLR 2023

retical properties of the chosen SARL base algorithm, such as the monotonic improvement (Kakade
& Langford, 2002). However, it is sample-inefï¬cient since it only utilizes 1/n of the collected sam-
ples to update n agentsâ policies. On the other hand, heterogeneous Proximal Policy Optimization
(HAPPO) (Kuba et al., 2022) sequentially updates agents based on their local advantages estimated
from the same rollout samples (Fig. 1b). Although it avoids the waste of collected samples and has
a monotonic improvement on the joint policy, the policy improvement of a single agent is not the-
oretically guaranteed. Consequently, one agentâs policy update may offset previous agentsâ policy
improvement, reducing the overall joint policy improvement.

In this paper, we aim to combine the merits of the existing single rollout and sequential policy update
schemes. Firstly, we show that naive sequential update algorithms with a single rollout can lose
the monotonic improvement guarantee of PPO for a single agentâs policy. To tackle this problem,
we propose a surrogate objective with a novel off-policy correction method, preceding-agent off-
policy correction (PreOPC), which retains the monotonic improvement guarantee on both the joint
policy and each agentâs policy. Then we further show that the joint monotonic bound built on the
single agent bound is tighter than those of other simultaneous update algorithms and is tightened
during updating the agents at a stage1. This leads to Agent-by-agent Policy Optimization (A2PO),
a novel sequential update algorithm with single rollout scheme (Fig. 1b). Further, we study the
signiï¬cance of the agent update order and extend the theory of non-stationarity to the sequential
update scheme. We test A2PO on four popular cooperative multi-agent benchmarks: StarCraftII,
multi-agent MuJoCo, multi-agent particle environment, and Google Research Football full game
scenarios. On all benchmark tasks, A2PO consistently outperforms strong baselines with a large
margin in both performance and sample efï¬ciency and shows an advantage in encouraging inter-
agent coordination. To sum up, the main contributions of this work are as follows:

1. Monotonic improvement bound. We prove that the guarantees of monotonic improvement
on each agentâs policy could be retained under the single rollout scheme with the off-policy
correction method PreOPC we proposed. We further prove that the monotonic bound on the joint
policy achieved given theoretical guarantees of each agent is the tightest among single rollout
algorithms, yielding effective policy optimization.

2. A2PO algorithm. We propose A2PO, the ï¬rst agent-by-agent sequential update algorithm that
retains the monotonic policy improvement on both each agentâs policy and the joint policy and
does not require multiple rollouts when performing policy improvement.

3. Agent update order. We further investigate the connections between the sequential policy up-
date scheme, the agent update order, and the non-stationarity problem, which motivates two novel
methods: a semi-greedy agent selection rule for optimization acceleration and an adaptive clip-
ping parameter method for alleviating the non-stationarity problem.

2 RELATED WORKS
Trust Region Policy Optimization (TRPO) (Schulman et al., 2015) and Proximal Policy Optimiza-
tion (PPO) (Schulman et al., 2017) are popular trust region algorithms with strong performances,
beneï¬ting from the guarantee of monotonic policy improvement (Kakade & Langford, 2002). Sev-
eral recent works delve deeper into understanding these methods (Wang et al., 2019; Liu et al., 2019;
Wang et al., 2020). In the multi-agent scenarios, de Witt et al. (2020) and Papoudakis et al. (2020)
empirically studied the performance of Independent PPO in multi-agent tasks. Yu et al. (2022)
conducted a comprehensive benchmark and analyzed the factor inï¬uential to the performance of
Multi-agent PPO (MAPPO), a variant of PPO with centralized critics. Coordinate PPO (CoPPO)
(Wu et al., 2021) integrates the value decomposition (Sunehag et al., 2017) and approximately per-
forms a joint policy improvement with monotonic improvement. Several further trials to implement
trust region methods are discussed in Wen et al. (2021); Li & He (2020); Sun et al. (2022); Ye et al.
(2022). However, these MARL algorithms suffer from the non-stationarity problem as they update
agents simultaneously. The environment dynamic changes from one agentâs perspective as others
also change their policies. Consequently, agents suffer from the high variance of gradients and re-
quire more samples for convergence (Hernandez-Leal et al., 2017). To alleviate the non-stationarity
problem, Multi-Agent Mirror descent policy algorithm with Trust region decomposition (MAMT)
(Li et al., 2022b) factorizes the trust regions of the joint policy and constructs the connections among
the factorized trust regions, approximately constraining the diversity of joint policy.

1We deï¬ne a stage as a period during which all the agents have been updated once (Fig. 1).

2

Published as a conference paper at ICLR 2023

Rollout and Policy Iteration for a Single Agent (RPISA) (Bertsekas, 2021) and Heterogeneous PPO
(HAPPO) (Kuba et al., 2022) consider the sequential update scheme. RPISA suffers from sample
inefï¬ciency as it requires n times of rollout for n agents to complete their policies update. Addi-
tionally, their work lacks a practical algorithm for complex tasks. In contrast, we propose a practical
algorithm A2PO that updates all agents using the same samples from a single rollout. HAPPO is
derived from the advantage decomposition lemma, proposed as Lemma 1 in Kuba et al. (2022).
It does not consider the distribution shift caused by preceding agents, and has no monotonic pol-
icy improvement guarantee for each agentâs policy. While A2PO is derived without decomposing
the advantage, and has a guarantee of monotonic improvement for each agentâs policy. We further
discuss other MARL methods in Appx. C.

3 TRUST REGION METHOD IN SEQUENTIAL POLICY UPDATE SCHEME

3.1 MARL PROBLEM FORMULATION

,

}

T

S

=

=

A

i
{A

N
A
:

S Ã A (cid:55)â

is the set of agents,

1, . . . , n
{
1
A

, Î³), where
iâN , r,
}
i is the action space of agent i, and
R is the reward function, and

We consider formulating the sequential decision-making problem in multi-agent scenarios as a de-
centralized Markov decision process (DEC-MDP) (Bernstein et al., 2002). An n-agent DEC-MDP
can be formalized as a tuple (
S
n is the joint action
is the state space.
[0, 1] is the dynamics
space. r :
[0, 1) is a reward discount factor. At time step
function denoting the transition probability. Î³
t, each agent i takes action ai
st), simultaneously according to the state st,
Ïn. The
st) = Ï1
forming the joint action at =
Â·|
joint policy Ï of these n agents induces a normalized discounted state visitation distribution dÏ,
where dÏ(s) = (1
[0, 1] is the probability func-
tion under Ï. We then deï¬ne the value function V Ï(s) = EÏ â¼(T ,Ï)[(cid:80)â
t=0 Î³tr(st, at)
s0 = s]
|
and the advantage function AÏ(s, a) = r(s, a) + Î³Es(cid:48)â¼T (Â·|s,a)[V Ï(s(cid:48))]
V Ï(s), where Ï =
(s0, a0), (s1, a1), . . .
denotes one sampled trajectory. The agents maximize their expected return,
{
}
denoted as: Ïâ = argmaxÏ J
3.2 MONOTONIC IMPROVEMENT IN SEQUENTIAL POLICY UPDATE SCHEME

t , . . . , an
a1
t }
{
t=0 Î³tP r(st = s
Ï) and P r(
|

â
t=0 Î³tr(st, at)] ,.

â
t from its policy Ïi(

and the joint policy Ï(

EÏ â¼(T ,Ï)[(cid:80)â

(Ï) = argmaxÏ

S Ã A Ã S (cid:55)â

Ã Â· Â· Â· Ã A

Î³) (cid:80)â

S (cid:55)â

Ï) :

. . .

Ã

Ã

â

Â·|

Â·|

T

We assume agents are updated in the order 1, 2, . . . , n, without loss of generality. We deï¬ne Ï as
the joint base policy from which the agents are updated at a stage, ei =
as the set of
preceding agents updated before agent i, and Â¯Ïi as the updated policy of agent i. We denote the joint
policy composed of updated policies of agents in the set ei, the updated policy of agent i and base
Ïn, and deï¬ne ËÏ0 = Ï and ËÏn = Â¯Ï. A
policies of other agents as ËÏi = Â¯Ï1
Ïi+1
L ËÏiâ1( ËÏi) is the surrogate objective
general sequential update scheme is shown as follows, where
for agent i:

1, . . . , i
{

. . .

. . .

Â¯Ïi

Ã

Ã

Ã

Ã

â

Ã

1

}

Ï = ËÏ0 maxÏ1 LÏ( ËÏ1)
ââââââââââ
Update Ï1

ËÏ1

ËÏnâ1 maxÏn L ËÏnâ1 ( ËÏn)
âââââââââââââ
Update Ïn

ËÏn = Â¯Ï.

ââ Â· Â· Â· ââ

We wish our sequential update scheme retains the desired monotonic improvement guarantee while
improving the sample efï¬ciency. Before going to our method, we ï¬rst discuss why naively up-
dating agents sequentially with the same rollout samples will fail in monotonic improvement for
each agent. Since agent i updates its policy from ËÏiâ1, an intuitive surrogate objective (Schul-
Ï( ËÏi), where
I
ËÏiâ1( ËÏi) =
man et al., 2015) used by agent i could be formulated as
L
E(s,a)â¼(dÏ, ËÏi)[AÏ(s, a)] and the superscript I means âIntuitiveâ. The expected re-
O
turn, however, is not guaranteed to improve with such a surrogate objective, as elaborated in the
following proposition.

Ï( ËÏi) = 1
1âÎ³

( ËÏiâ1) +

O

J

AÏ(s, a)
), where
Proposition 1 For agent i, let (cid:15) = maxs,a
i
âª {
}
|
q) is the total variation distance between distributions p and q and we deï¬ne Dmax
DT V (p
Â¯Ï) =
T V (Ï
(cid:107)
(cid:107)
maxs DT V (Ï(

s)), then we have:

, Î±j = Dmax

T V (Ïj

Â¯Ïj)
(cid:107)

j
â

(ei

s)

â

|

Â¯Ï(
(cid:107)

Â·|

Â·|

( ËÏi)

(cid:12)
(cid:12)

J

â L

I

ËÏiâ1( ËÏi)(cid:12)
(cid:12)

â¤

2(cid:15)Î±i(cid:0)

3

â

1

Î³ â

1

Î³(1

â

â

2
(cid:80)
jâ(eiâª{i}) Î±j)

(cid:1) +

The proof can be found in Appx. A.3.

3

Uncontrollable
(cid:123)
(cid:125)(cid:124)
jâei Î±j
Î³

(cid:122)
2(cid:15) (cid:80)
1

â

= Î²I

i . (1)

Published as a conference paper at ICLR 2023

J

O

( ËÏi) >

( ËÏiâ1) >

( ËÏiâ1) when

I
Î²I
( ËÏi)
Remark. From Eq. (1) and the deï¬nition of
i .
ËÏiâ1 , we know
L
â
J
i , which can be satisï¬ed by constraining Î²I
Ï( ËÏi) > Î²I
i and op-
Thus
O
J
i , the term 2(cid:15) (cid:80)
jâei Î±j/(1
Ï( ËÏi). However, in Î²I
Î³), is uncontrollable by agent i.
timizing
( ËÏi) may not be
Consequently, the upper bound Î²I
i may be large and the expected performance
i even if Î±i is well constrained. Although
Ï( ËÏi) < Î²I
improved after optimizing
one can still prove a monotonic guarantee for the joint policy by summing Eq. (1) for all the agents,
we will show that the monotonic improvement on every single agent, if guaranteed, brings a tighter
monotonic bound on the joint policy and incrementally tightens the monotonic bound on the joint
policy when updating agents during a stage. Uncontrollable terms also appear when similarly ana-
lyzing HAPPO and cause the loss of monotonic improvement for a single agent2.

Ï( ËÏi) when

Ï( ËÏi)

â J

O

O

O

â

J

3.3 PRECEDING-AGENT OFF-POLICY CORRECTION

The uncontrollable term in Prop. 1 is caused by one ignoring how the updating of its preceding
agentsâ policies inï¬uences its advantage function. We investigate reducing the uncontrollable term
in policy evaluation. Since agent i is updated from ËÏiâ1, the advantage function A ËÏiâ1
should be
used in agent iâs surrogate objective rather than AÏ. However, A ËÏiâ1
is impractical to estimate
using samples collected under Ï due to the off-policyness (Munos et al., 2016) of these samples.
Nevertheless, we can approximate A ËÏiâ1
by correcting the discrepancy between ËÏiâ1 and Ï at
each time step (Harutyunyan et al., 2016). To retain the monotonic improvement properties, we
propose preceding-agent off-policy correction (PreOPC), which approximates A ËÏiâ1
using samples
collected under Ï by correcting the state probability at each step with truncated product weights:

AÏ, ËÏiâ1

(st, at) = Î´t +

(cid:88)

Î³k(cid:0)

Î» min (cid:0)1.0,

k
(cid:89)

ËÏiâ1(at+j
Ï(at+j

st+j)
|
st+j)
|

(cid:1)(cid:1)Î´t+k ,

(2)

j=1

kâ¥1
V (st) is the temporal difference for V (st), Î» is a parameter

where Î´t = r(st, at) + Î³V (st+1)
controlling the bias and variance, as used in Schulman et al. (2016). min(1.0, ËÏiâ1(at+j |st+j )
Ï(at+j |st+j ) )
â
are truncated importance sampling weights, approximating the probability of st+k at
1, . . . , k
{
time step t + k under ËÏiâ1. The derivation of Eq. (2) can be found in Appx. A.8. With PreOPC, the
surrogate objective of agent i becomes
(s, a)]
, and we summarize the surrogate objective of updating all agents as follows:

E(s,a)â¼(dÏ, ËÏi)[AÏ, ËÏiâ1

( ËÏiâ1) + 1
1âÎ³

j
â

â

J

}

Ï(Â¯Ï) =

G

J

(Ï) +

1

E(s,a)â¼(dÏ, ËÏi)[AÏ, ËÏiâ1

(s, a)] .

(3)

L ËÏiâ1( ËÏi) =
n
1
(cid:88)

Î³

i=1

â

Note that Eq. (3) takes the sum of expectations of the global advantage function approximated under
different joint policies, different from the advantage decomposition lemma in Kuba et al. (2022)
which decomposes the global advantage function into local ones.

We can now prove that the monotonic policy improvement guarantee of both updating one agentâs
policy and updating the joint policy is retained by using Eq. (3) as the surrogate objective. The
detailed proofs can be found in Appx. A.4.
Theorem 1 (Single Agent Monotonic Bound) For agent i, let (cid:15)i = maxs,a
maxs,a

A ËÏiâ1
, Î¾i =
(s, a)
|
|
), then we have:
i
}

AÏ, ËÏiâ1
|

T V (Ïj

(s, a)

j
â

Â¯Ïj)

âª {

(ei

(cid:107)

A ËÏiâ1
(s, a)
â
(cid:12)
â L ËÏiâ1( ËÏi)
(cid:12)

â¤

(cid:12)
(cid:12)

( ËÏi)

J

, Î±j = Dmax
|
1
4(cid:15)iÎ±i(cid:0)

â
1
(cid:80)
jâ(eiâª{i}) Î±j)

(cid:1) +

Î¾i

1

1

Î³ â
â
â
(cid:0)Î±i (cid:88)

4Î³(cid:15)i

Î³(1

â
Î±j(cid:1) +

â¤

(1

â

Î³)2

jâ(eiâª{i})

Î¾i

â

1

.

Î³

Î³

1

â

(4)

The single agent monotonic bound depends on (cid:15)i, Î¾i, and Î±i and the total variation distances of
preceding agents. Unlike Eq. (1), we can effectively constrain the monotonic bound by control-
ling Î±i since Î¾i decreases as agent i updating its value function (Munos et al., 2016) and does not
lead to an unsatisï¬able bound when Î±i is well constrained, providing the guarantee for monotonic
improvement when updating a single agent. Given the above bound, we can prove the monotonic
improvement of the joint policy.

2More discussions about why HAPPO fails to guarantee monotonic improvement for a single agentâs policy

can be found in Appx. A.6.

4

Published as a conference paper at ICLR 2023

Table 1: Comparisons of trust region MARL algorithms. The proofs of the monotonic bounds
can be found in Appx. A. Note that we also provide the monotonic bound of RPISA-PPO, which
implements RPISA with PPO as the base algorithm. We separate RPISA-PPO from other methods
as it has low sample efï¬ciency and thus does not constitute a fair comparison.

Algorithm Rollout

Update

Sample Efï¬ciency Monotonic Bound

RPISA-PPO Multiple

Sequential

MAPPO
CoPPO

Single
Single

Simultaneous
Simultaneous

HAPPO

Single

Sequential

A2PO (ours)

Single

Sequential

Low

High
High

High

High

1
1âÎ³(1âÎ±i) )

1âÎ³ â

1âÎ³ â

Î±i
i=1
1âÎ³
i=1 Î±i( 1
i=1 Î±i( 1

4(cid:15) (cid:80)n
i=1 Î±i( 1
Single Agent: 4(cid:15)Î±i( 1
4(cid:15) (cid:80)n
4(cid:15) (cid:80)n
4(cid:15) (cid:80)n
Single Agent: No Guarantee
4(cid:15) (cid:80)n
i=1 Î±i( 1
Single Agent: 4(cid:15)iÎ±i( 1

1âÎ³ â
1âÎ³ â

1âÎ³ â

1

1

1âÎ³(1â(cid:80)

1âÎ³(1â(cid:80)n

1âÎ³(1â(cid:80)n

1âÎ³ â

1
1âÎ³(1âÎ±i) )

j=1 Î±j ) )
j=1 Î±j ) )

(cid:80)n

i=1 Î¾i
1âÎ³

1

jâ(eiâª{i}) Î±j ) ) +
1âÎ³(1â(cid:80)

1

jâ(eiâª{i}) Î±j ) ) + Î¾i

1âÎ³

Theorem 2 (Joint Monotonic Bound) For each agent i
A ËÏiâ1
Î±i = Dmax

AÏ, ËÏiâ1

(s, a)

T V (Ïi

Â¯Ïi), Î¾i = maxs,a
|
(cid:107)
n
(cid:88)

Ï(Â¯Ï)

4(cid:15)

| â¤

(Â¯Ï)

|J

â G

1

Î±i(cid:0)

â

, let (cid:15)i = maxs,a
(s, a)
â N
|
, and (cid:15) = maxi (cid:15)i, then we have:
(s, a)
|

A ËÏiâ1

|

,

i=1

4Î³(cid:15)

â¤

(1

Î³)2

â

1

Î³ â

1

Î³(1

â
â
n
(cid:0)Î±i (cid:88)
(cid:88)

i=1

jâ(eiâª{i})

1
(cid:80)
jâ(eiâª{i}) Î±j)
(cid:80)n
i=1 Î¾i
Î³

â
Î±j(cid:1) +

.

1

â

(cid:1) +

(cid:80)n

i=1 Î¾i
Î³

1

â

(5)

Eq. (5) suggests a condition for monotonic improvement of the joint policy, similar to that in the
remark under Prop. 1. We further prove that the joint monotonic bound is incrementally tight-
ened when performing the policy optimization agent-by-agent during a stage due to the single agent
(Â¯Ï) is relaxed and more likely to be satis-
monotonic bound, i.e., the condition for improving
ï¬ed. The details can be found in Appx. A.5. We present the monotonic bounds of other algorithms
j=1 Î±j ) , Eq. (5) achieves the tightest bound
in Tab. 1. Since
compared to other single rollout algorithms, with Î¾i
small enough. The assumption about Î¾i
is valid since preceding-agent off-policy correction is a contraction operator, which is a corollary of
Theorem 1 in Munos et al. (2016). A tighter bound improves expected performance by optimizing
the surrogate objective more effectively (Li et al., 2022a).

jâ(eiâª{i}) Î±j ) <

1âÎ³(1â(cid:80)n

1âÎ³(1â(cid:80)

â N

i
â

J
1

â

â

1

4 AGENT-BY-AGENT POLICY OPTIMIZATION

G

â

J

(ei

(ei

i.e.,

j
â

i
â
}
âª {
( ËÏiâ1) + 1
1âÎ³

Ï(Â¯Ï). When up-
We ï¬rst give a practical implementation for optimizing the surrogate objective
dating agent i, the monotonic bound in Eq. (4) consists of the total variation distances related to the
preceding agents and agent i, i.e., Î±i (cid:80)
jâ(eiâª{i}) Î±j. It suggests that we can control the monotonic
bound by controlling total variation distances Î±j
i
j
), to effectively improve the expected
}
âª{
â
performance. We consider applying the clipping mechanism to control the total variation distances
Î±j
) (Queeney et al., 2021; Sun et al., 2022). In the surrogate objective of agent i,
E(s,a)â¼(dÏ,Ï)[
( ËÏiâ1) has no dependence to agent
Â¯Ïi (cid:81)
Ïi (cid:81)

Â¯Ïi (cid:81)
Ïi (cid:81)
jâei Â¯Ïj
i, while the joint policy ratio
jâei Ïj in the advantage estimation is appropriate for applying
the clipping mechanism. We further consider reducing the instability in estimating agent iâs policy
gradient by clipping the joint policy ratio of preceding agents ï¬rst, with a narrower clipping range
(Wu et al., 2021). Thus we apply the clipping mechanism on the joint policy ratio twice: once on the
joint policy ratio of preceding agents and once on the policy ratio of agent i. Finally, the practical
objective for updating agent i becomes:
Ë
L ËÏiâ1( ËÏi) = E(s,a)â¼(dÏ,Ï)

(cid:2) min (cid:0)l(s, a)AÏ, ËÏiâ1

jâei Â¯Ïj
jâei Ïj AÏ, ËÏiâ1

, clip (cid:0)l(s, a), 1

(cid:15)i(cid:1)AÏ, ËÏiâ1 (cid:1)(cid:3) ,

where l(s, a) = Â¯Ïi(ai|s)
(cid:15)i
2 ). The clipping parameter
(cid:15)i is selected as (cid:15)i =
) is the clipping param-
(
Â·
eter adapting function. We summarize our proposed Agent-by-agent Policy Optimization (A2PO) in

Â±
((cid:15), i), where (cid:15) is the base clipping parameter and

Ïi(ai|s) g(s, a), and g(s, a) = clip(

jâei Â¯Ïj (aj |s)
jâei Ïj (aj |s) , 1

(s, a)],

(cid:81)
(cid:81)

(6)

Â±

J

C

C

,

Â·

5

4

5

6

7

8

9

10

11

12

Published as a conference paper at ICLR 2023

Alg. 1. Note that in Line 6, the agent for the next update iteration is selected according to the agent
selection rule

).

(
Â·
R

Algorithm 1: Agent-by-agent Policy Optimization (A2PO)

1 Initialize the joint policy Ï0 =
2 for iteration m = 1, 2, . . . do
3

Collect data using Ïmâ1 =
for Order k = 1, . . . , n do

0, . . . , Ïn
Ï1
0 }
{

, and the global value function V .

mâ1, . . . , Ïn
Ï1
{

mâ1}

.

m = Ïi

mâ1, preceding agents ei =
.
}

Select an agent according to the selection rule as i =
Policy Ïi
m, Ïjâek
Joint policy ËÏi =
Ïi
{
Compute the advantage approximation as AÏ, ËÏiâ1
Compute the value target v(st) = AÏ, ËÏiâ1
for P epochs do

m , ÏjâN âek

{R

mâ1

R
(1), . . . ,

(s, a) + V (s).

(k).
(k

R

.

1)
}

â

(s, a) via Eq. (2).

Ïi
m = arg maxÏi
V = arg minV

m

Ë
L ËÏiâ1( ËÏi) as in Eq. (6).
Esâ¼dÏ
â

2.
(cid:107)

V (s)

v(s)

(cid:107)

Eq. (6) approximates the surrogate objective of a single agent. We remark that the monotonic im-
provement guarantee of a single agent reveals how the update of a single agent affects the overall
objective. We will further discuss
) from the perspective of how to beneï¬t the opti-
(
Â·
C
mization of the overall surrogate objective by coordinating the policy updates of each agent.

) and
Â·

(
R

,
Â·

G

J

Ï(Â¯Ï) =

(Ï) + (cid:80)n
L ËÏiâ1( ËÏi), we recognize maximizing (cid:80)n

Semi-greedy Agent Selection Rule. With the monotonic policy improvement guarantee on the joint
(Â¯Ï) by opti-
policy, as shown in Thm. 2, we can effectively improve the expected performance
J
i=1 L ËÏiâ1( ËÏi). Since the policies
mizing the surrogate objective of all agents
Ë
except Ïi are ï¬xed when maximizing
L ËÏiâ1( ËÏi) as
performing a block coordinate ascent, i.e., iteratively seeking to update a block of chosen coordi-
nates (agents) while other blocks (agents) are ï¬xed. As a special case of the coordinate selection
rule, the agent selection rule becomes crucial for convergence. On the one hand, intuitively, up-
dating agent with a bigger absolute value of the advantage function contributes more to optimizing
Ï(Â¯Ï). Inspired by the Gauss-Southwell rule (Gordon & Tibshirani, 2015), we propose the greedy
G
agent selection rule, under which an agent with a bigger absolute value of the expected advan-
tage function is updated with a higher priority. We will verify that the agents with small absolute
values of the advantage function also beneï¬t from the greedy selelction rule in Appx. B.2.5. On
the other hand, purely greedy selection may lead to early convergence which harms the perfor-
mance. Therefore, we introduce randomness into the agent selection rule to avoid converging too
early (Lu et al., 2018). Combining the merits, we propose the semi-greedy agent selection rule as
(cid:40)

i=1

(k) = arg maxiâ(N âe)
(k)

R
R
is a uniform distribution. We verify that the semi-greedy agent selection rule contributes to the

N â

â¼ U

{R

e),

R

â

}

(

|

(1), . . . ,

(k

, where e =

1)

and

], k2 = 0
k2 = 1

AÏ, ËÏR(kâ1)
|

Es,ai[

U
performance of A2PO in Sec. 5.2.

E(s,a)â¼(dÏ, ËÏi)[AÏ, ËÏiâ1

Adaptive Clipping Parameter. We improve the sample efï¬ciency by updating all agents using
the samples collected under the base joint policy Ï. However, when updating agent i by optimiz-
ing 1
(s, a)], the expectation of advantage function is estimated using the
1âÎ³
states sampled under Ï instead of ËÏiâ1, which reintroduces the non-stationarity since agent i can
not perceive the change of the preceding agents. With the non-stationarity modeled by the state tran-
sition shift (Sun et al., 2022), we deï¬ne the state transition shift encountered when updating agent i
as âÂ¯Ï1,...,Â¯Ïiâ1,Ïi,...,Ïn
s, a)( ËÏiâ1(a
s))]. The state transition shift has
Ï(a
|
|
|
the following property.

s) = (cid:80)
|

Ï1,...,Ïn

(s(cid:48)

(s(cid:48)

a[

s)

â

T

Proposition 2 The state transition shift âÂ¯Ï1,...,Â¯Ïiâ1,Ïi,...,Ïn

Ï1,...,Ïn

s) can be decomposed as follows.

(s(cid:48)

|

âÂ¯Ï1,...,Â¯Ïiâ1,Ïi,...,Ïn

Ï1,...,Ïn

= âÂ¯Ï1,Ï2,...,Ïn

Ï1,...,Ïn + âÂ¯Ï1,Â¯Ï2,Ï3,...,Ïn

Â¯Ï1,Ï2,...,Ïn +

+ âÂ¯Ï1,...,Â¯Ïiâ1,Ïi,...,Ïn

Â¯Ï1,...,Â¯Ïiâ2,Ïiâ1,...,Ïn

Â· Â· Â·

6

Published as a conference paper at ICLR 2023

Prop. 2 shows that the total state transition shift encountered by agent i can be decomposed into the
sum of state transition shift caused by each agent whose policy has been updated. Shifts caused by
agents with higher priorities will be encountered by more following agents and thus contribute more
to the non-stationarity problem. Recall that the state transition shift effectively measures the total
variation distance between policies. Therefore, in order to reduce the non-stationarity brought by
the agentsâ policy updates, we can adaptively clip each agentâs surrogate objective according to their
update priorities. We propose a simple yet effective method, named adaptive clipping parameter, to
k/n,
adjust the clipping parameters according to the updating order:
where c(cid:15) is a hyper-parameter. We demonstrate how the agents with higher priorities affect the
following agents in Fig. 2. Under the clipping mechanism, the inï¬uence of the agents with higher
priority could be reï¬ected in the clipping ranges of the joint policy ratio. The policy changes of
the preceding agents may constrain the following agents to optimize the surrogate objective within
insufï¬cient clipping ranges, as shown on the left side of Fig. 2. The right side of Fig. 2 demonstrates
that the adaptive clipping parameter method leads to balanced and sufï¬cient clipping ranges.

((cid:15), k) = (cid:15)

c(cid:15) + (cid:15)

c(cid:15))

(1

â

C

Â·

Â·

Â·

Figure 2: The clipping ranges of three agents. The surface a1 + a2 + a3 = 1 demonstrates the
policy space of three discrete actions. The agents are updated in the order of 2, 3, 1. The areas in
gray/pink are the clipping ranges with/without considering the joint policy ratio of preceding agents.
Left: The agents have the same clipping parameters. The clipping range of agent 1 is insufï¬cient
due to the large variation in the policies of agent 2 and agent 3. Right: The clipping ranges are more
balanced and sufï¬cient with the adaptive clipping parameter method.

5 EXPERIMENTS

In this section, we empirically evaluate and analyze A2PO in the widely adopted cooperative multi-
agent benchmarks, including the StarCraftII Multi-agent Challenge (SMAC) (Samvelyan et al.,
2019), Multi-agent MuJoCo (MA-MuJoCo) (de Witt et al., 2020), Multi-agent Particle Environ-
ment (MPE) (Lowe et al., 2017)3, and more challenging Google Research Football (GRF) full-game
scenarios (Kurach et al., 2020). Experimental results demonstrate that 1) A2PO achieves perfor-
mance and efï¬ciency superior to those of state-of-the-art MARL Trust Region methods, 2) A2PO
has strength in encouraging coordination behaviors to complete complex cooperative tasks, and 3)
the PreOPC, the semi-greedy agent selection rule, and the adaptive clipping parameter methods
signiï¬cantly contribute to the performance improvement. 4

We compare A2PO with advanced MARL trust-region methods: MAPPO (Yu et al., 2022), CoPPO
(Wu et al., 2021) and HAPPO (Kuba et al., 2022). We implement all the algorithms as parameter
sharing in SMAC and MPE, and as parameter-independent in MA-MuJoCo and GRF, according to
the homogeneity and heterogeneity of agents. We divide the agents into blocks for tasks with numer-
ous agents to control the training time of A2PO comparable to other algorithms. Full experimental
details can be found in Appx. B.

5.1 PERFORMANCE AND EFFICIENCY

We evaluate the algorithms in 9 maps of SMAC with various difï¬culties, 14 tasks of 6 scenarios in
MA-MuJoCo, and the 5-vs-5 and 11-vs-11 full game scenarios in GRF. Results in Tab. 2, Fig. 3, and
Fig. 4 show that A2PO consistently outperforms the baselines and achieves higher sample efï¬ciency
in all benchmarks. More results and the experimental setups can be found in Appx. B.2.

StarCraftII Multi-agent Challenge (SMAC). As shown in Tab. 2, A2PO achieves (nearly) 100%
win rates in 6 out of 9 maps and signiï¬cantly outperforms other baselines in most maps. In Tab. 2, we
additionally compare the performance with that of Qmix (Rashid et al., 2018), a well known baseline

3We evaluate A2PO in fully cooperative and general-sum MPE tasks respectively, showing the potential of

extending A2PO to general-sum games, see Appx. B.2.3 for full results.

4Code is available at https://anonymous.4open.science/r/A2PO.

7

a1a2a3Agent2Î¸2olda1a2a3Agent3Î¸3olda1a2a3Agent1Î¸1olda1a2a3Agent2Î¸2olda1a2a3Agent3Î¸3olda1a2a3Agent1Î¸1oldPublished as a conference paper at ICLR 2023

in SMAC. We also observe that CoPPO and A2PO have better stability as they consider clipping joint
policy ratios. Multi-agent MuJoCo environment (MA-MuJoCo). We investigate whether A2PO

Table 2: Median win rates and standard deviations on SMAC tasks.

Map

Difï¬culty MAPPO w/ PS CoPPO w/ PS HAPPO w/ PS A2PO w/ PS Qmix w/ PS

96.9(0.988)
MMM
Easy
84.4(4.39)
3s5z
Hard
84.4(2.77)
5m vs 6m
Hard
84.4(2.39)
Hard
8m vs 9m
93.8(18.7)
10m vs 11m Hard
6h vs 8z
Super Hard 87.5(1.53)
3s5z vs 3s6z Super Hard 82.8(19.2)
MMM2
Super Hard 90.6(8.89)
27m vs 30m Super Hard 93.8(3.75)

Overall

/

88.7(6.96)

96.9(1.25)
92.2(2.35)
84.4(2.12)
84.4(2.04)
96.9(2.6)
90.6(0.765)
84.4(2.9)
90.6(6.93)
93.8(2.2)

90.5(2.57)

95.3(2.48)
92.2(1.74)
87.5(2.51)
96.9(3.78)
98.4(2.99)
87.5(1.49)
37.5(13.2)
51.6(9.01)
90.6(4.77)

81.9(4.67)

100(1.07)
98.4(1.04)
90.6(3.06)
100(1.04)
100(0.521)
90.6(1.32)
93.8(19.8)
98.4(1.25)
100(1.55)

96.9(3.41)

95.3(5.2)
88.3(2.9)
75.8(3.7)
92.2(2.0)
95.3(1.0)
9.4(2.0)
82.8(5.3)
87.5(2.6)
39.1(9.8)

74.0(3.83)

can scale to more complex continuous control multi-agent tasks in MA-MuJoCo. We calculate
maximum returnâminimum return over all the 14 tasks in the left of Fig. 3. We also
the normalized score
present part of results in the right of Fig. 3, where the control complexity and observation dimension,
depending on the number of the robotâs joints, increases from left to right. We observe that A2PO
generally shows an increasing advantage over the baselines with increasing task complexity.

returnâminimum return

Figure 3: Experiments in MA-MuJoCo. Left: Normalized scores on all the 14 tasks. Right: Com-
parisons of averaged return on selected tasks. The number of robot joints increases from left to
right.

Google Research Football (GRF). We evaluate A2PO in GRF full-game scenarios, where agents
have difï¬culty discovering complex coordination behaviors. A2PO obtains nearly 100% win rate in
the 5-vs-5 scenario. In both scenarios, we attribute the performance gain of A2PO to the learned
coordination behavior. We analyze the experiments in GRF to verify that A2PO encourages agents
to learn coordination behaviors in complex tasks. In Tab. 3, an âAssistâ is attributed to the player
who passes the ball to the teammate that makes a score, a âPassâ is counted when the passing-and-
receiving process is ï¬nished, âPass Rateâ is the proportion of success passes over the pass attempts.
A2PO have an advantage in passing-and-receiving coordination, leading to more assists and scores.

Table 3: Learned behaviors on the Google Re-
search Football 5-vs-5 scenario. Bigger values
are better except fot the âLostâ metric.

Metric

MAPPO CoPPO HAPPO A2PO

0.04(0.02)
Assist
1.95(1.17)
Goal
0.49(0.11)
Lost
Pass
1.52(0.13)
Pass Rate 19.3(10.0)

0.19(0.08) 0.07(0.05) 0.56(0.20)
4.42(2.08) 2.68(0.86) 9.01(0.95)
0.74(0.33) 1.04(0.12) 0.78(0.15)
3.44(1.04) 4.03(1.97) 6.42(2.23)
35.0(10.3) 48.9(25.7) 67.1(11.7)

Figure 4: Averaged win rate on the Google Re-
search Football full-game scenarios.

5.2 ABLATION STUDY

This section studies how PreOPC, the semi-greedy agent selection rule, and the adaptive clipping
parameter affect the performance. Full ablation details can be found in Appx. B.2.5

8

0.00.20.40.60.81.0StepsÃ1070.00.20.40.60.8NormalizedScoreNormalizedScorein14TasksMAPPOw/oPSCoPPOw/oPSHAPPOw/oPSA2POw/oPS0.000.250.500.751.00StepsÃ107100020003000400050006000EpisodeReturnAnt-v22x4d0.000.250.500.751.00StepsÃ107010002000300040005000EpisodeReturnWalker2d-v23x20.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnHumanoid-v217x1MAPPOw/oPSCoPPOw/oPSHAPPOw/oPSA2POw/oPS050001000015000Episodes0.00.20.40.60.81.0WinRateFootball5vs50250005000075000100000Episodes0.00.20.40.60.81.0WinRateFootball11vs11MAPPOCoPPOHAPPOA2POPublished as a conference paper at ICLR 2023

PreOPC. Fig. 5 shows the effects of utilizing
off-policy correction in two cases: 1) Correc-
tion on all agentsâ policies for simultaneous up-
date algorithms, i.e., MAPPO w/ V-trace (Es-
peholt et al., 2018) and CoPPO w/ V-trace, and
2) Correction on the preceding agentsâ policies
for sequential update algorithms, i.e., HAPPO
w/ PreOC and A2PO. V-trace brings no general
improvement to MAPPO and CoPPO, while
PreOPC signiï¬cantly improves the sequential
update cases. PreOPC improves the perfor-
mance of HAPPO signiï¬cantly, while A2PO
still outperforms HAPPO w/ PreOPC. The performance gap lies in that A2PO clips the joint policy
ratios, which matches the monotonic bound in Thm. 1. The results verify that A2PO reaches or
outperforms the asymptotic performance of RPISA-PPO using an approximated advantage function
and updating all the agents with the same rollout samples. Additionally, preceding-agent off-policy
correction does not increase the sensitivity of the hyper-parameter Î», as shown in Appx. B.2.5.

Figure 5: Ablation experiments on preceding-
agent off-policy correction.

Agent Selection Rule. We provide comparisons of different agent selection rules in Fig. 6. The
âCyclicâ rule means select agents in the order 1, . . . , n, and other rules have been introduced in
sec. 4. The semi-greedy rule considers the optimization acceleration and the performance balance
among agents and thus performs the best in all tasks.

Adaptive Clipping Parameter. We propose the adaptive clipping parameter method for balanced
and sufï¬cient clipping ranges of agents. As shown in Fig. 7, the adaptive clipping parameter con-
tributes to the performance gain of A2PO.

Figure 6: Ablation experiments on the agent se-
lection rules.

Figure 7: Ablation experiments on the adaptive
clipping parameter method.

6 CONCLUSION

In this paper, we investigate the potential of the sequential update scheme in coordination tasks. We
introduce A2PO, a sequential algorithm using a single rollout at a stage, which guarantees monotonic
improvement on both the joint policy and each agentâs policy. We also justify that the monotonic
bound achieved by A2PO is the tightest among existing trust region MARL algorithms under single
rollout scheme. Furthermore, A2PO integrates the proposed semi-greedy agent selection rule and
adaptive clipping parameter method. Experiments in various benchmarks demonstrate that A2PO
consistently outperforms state-of-the-art methods in performance and sample efï¬ciency and encour-
ages coordination behaviors for completing complex tasks. For future work, we plan to analyze
the theoretical underpinnings of the agent selection rules and study the learnable methods to select
agents and clipping parameters.

9

0.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnRPISA-PPOAsymptoticPerformanceHumanoid-v29|80.000.250.500.751.00StepsÃ1070.00.20.40.60.81.0WinRateRPISA-PPOAsymptoticPerformanceMMM2MAPPOMAPPOw/V-traceCoPPOCoPPOw/V-traceHAPPOHAPPOw/PreOPCA2POw/oPreOPCA2PO0.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnHumanoid-v29|80.000.250.500.751.00StepsÃ1070.00.20.40.60.81.0WinRateMMM2HAPPO-CyclicHAPPO-RandomHAPPO-GreedyHAPPO-Semi-greedyA2PO-CyclicA2PO-RandomA2PO-GreedyA2PO-Semi-greedy0.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnHumanoid-v29|80.000.250.500.751.00StepsÃ1070.00.20.40.60.81.0WinRateMMM2MAPPOMAPPOw/AdaptCoPPOCoPPOw/AdaptHAPPOHAPPOw/AdaptA2POw/oAdaptA2POPublished as a conference paper at ICLR 2023

Acknowledgements. The SJTU team is partially supported by âNew Generation of AI 2030â Ma-
jor Project (2018AAA0100900), the Shanghai Municipal Science and Technology Major Project
(2021SHZDZX0102), the Shanghai Sailing Program (21YF1421900), the National Natural Science
Foundation of China (62076161, 62106141). Xihuai Wang and Ziyu Wan are supported by Wu Wen
Jun Honorary Scholarship, AI Institute, Shanghai Jiao Tong University. We thank Yan Song and He
Jiang for their help in the football experiments.

Ethics Statement. Our method and algorithm do not involve any adversarial attack, and will not
endanger human security. All our experiments are performed in the simulation environment, which
does not involve ethical and fair issues.

The source code of this paper is available at https://
Reproducibility Statement.
anonymous.4open.science/r/A2PO. We provide proofs in appx. A, including the proofs of
intuitive sequential update, monotonic policy improvement of A2PO, incrementally tightened bound
of A2PO and monotonic policy improvement of MAPPO, CoPPO and HAPPO. We specify all the
experiments implementation details, the experiments setup, and the additional results in the appx. B.
The related works of coordinate descent are shown in appx. D.

REFERENCES

Stefano V. Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive
survey and open problems. Artif. Intell., 258:66â95, 2018. doi: 10.1016/j.artint.2018.01.002.
URL https://doi.org/10.1016/j.artint.2018.01.002.

Agarwal Alekh, Jiang Nan, M. Kakade Sham, and Sun Wen. Reinforcement Learning: Theory and

Algorithms, 2022. URL https://rltheorybook.github.io/.

Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, RaphaÂ¨el
Marinier, LÂ´eonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly,
and Olivier Bachem. What matters in on-policy reinforcement learning? A large-scale empirical
study. CoRR, abs/2006.05990, 2020. URL https://arxiv.org/abs/2006.05990.

Daniel S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of
decentralized control of markov decision processes. Math. Oper. Res., 27(4):819â840, 2002. doi:
10.1287/moor.27.4.819.297. URL https://doi.org/10.1287/moor.27.4.819.297.

Dimitri Bertsekas. Multiagent Reinforcement Learning: Rollout and Policy Iteration. IEEE/CAA
Journal of Automatica Sinica, 8(2):249â272, February 2021. ISSN 2329-9266, 2329-9274. doi:
10.1109/JAS.2021.1003814.

Christian SchrÂ¨oder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip H. S.
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? CoRR, abs/2011.09533, 2020. URL https://arxiv.org/abs/
2011.09533.

Lasse Espeholt, Hubert Soyer, RÂ´emi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu.
IM-
PALA: scalable distributed deep-rl with importance weighted actor-learner architectures. In Jen-
nifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, StockholmsmÂ¨assan, Stockholm, Sweden, July 10-15, 2018, vol-
ume 80 of Proceedings of Machine Learning Research, pp. 1406â1415. PMLR, 2018. URL
http://proceedings.mlr.press/v80/espeholt18a.html.

Ian M. Gemp, Charlie Chen, and Brian McWilliams. The generalized eigenvalue problem as a nash
equilibrium. CoRR, abs/2206.04993, 2022. doi: 10.48550/arXiv.2206.04993. URL https:
//doi.org/10.48550/arXiv.2206.04993.

Tobias Glasmachers and Â¨UrÂ¨un Dogan. Accelerated coordinate descent with adaptive coordinate
frequencies. In Cheng Soon Ong and Tu Bao Ho (eds.), Asian Conference on Machine Learning,
ACML 2013, Canberra, ACT, Australia, November 13-15, 2013, volume 29 of JMLR Workshop
and Conference Proceedings, pp. 72â86. JMLR.org, 2013. URL http://proceedings.
mlr.press/v29/Glasmachers13.html.

10

Published as a conference paper at ICLR 2023

Geoff Gordon and Ryan Tibshirani. Coordinate descent. Optimization, 10(36):725, 2015.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861â1870. PMLR, 2018.

Anna Harutyunyan, Marc G. Bellemare, Tom Stepleton, and RÂ´emi Munos. Q(Î») with off-policy cor-
rections. In Ronald Ortner, Hans Ulrich Simon, and Sandra Zilles (eds.), Algorithmic Learning
Theory - 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceed-
ings, volume 9925 of Lecture Notes in Computer Science, pp. 305â320, 2016. doi: 10.1007/
21. URL https://doi.org/10.1007/978-3-319-46379-7_
978-3-319-46379-7
\
21.

Qiang He and Xinwen Hou. Wd3: Taming the estimation bias in deep reinforcement learning.
In 2020 IEEE 32nd International Conference on Tools with Artiï¬cial Intelligence (ICTAI), pp.
391â398, 2020. doi: 10.1109/ICTAI50040.2020.00068.

Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey
of learning in multiagent environments: Dealing with non-stationarity. CoRR, abs/1707.09183,
2017. URL http://arxiv.org/abs/1707.09183.

Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning.
In Claude Sammut and Achim G. Hoffmann (eds.), Machine Learning, Proceedings of the Nine-
teenth International Conference (ICML 2002), University of New South Wales, Sydney, Australia,
July 8-12, 2002, pp. 267â274. Morgan Kaufmann, 2002.

Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong
Yang. Trust region policy optimisation in multi-agent reinforcement learning. In International
Conference on Learning Representations, 2022. URL https://openreview.net/forum?
id=EcGGFkNTxdJ.

Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajac, Olivier Bachem, Lasse Espeholt,
Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly.
In AAAI 2020, pp.
Google research football: A novel reinforcement learning environment.
4501â4510. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/
article/view/5878.

Chenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang. Cel-
ebrating Diversity in Shared Multi-Agent Reinforcement Learning. In Advances in Neural Infor-
mation Processing Systems, November 2021.

Hepeng Li and Haibo He. Multi-agent trust region policy optimization. CoRR, abs/2010.07916,

2020. URL https://arxiv.org/abs/2010.07916.

Hepeng Li, Nicholas Clavette, and Haibo He. An analytical update rule for general policy optimiza-
tion. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba SzepesvÂ´ari, Gang Niu, and Sivan
Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal-
timore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 12696â
12716. PMLR, 2022a. URL https://proceedings.mlr.press/v162/li22d.html.

Wenhao Li, Xiangfeng Wang, Bo Jin, Junjie Sheng, and Hongyuan Zha. Dealing with non-
stationarity in MARL via trust-region decomposition. In International Conference on Learning
Representations, 2022b. URL https://openreview.net/forum?id=XHUxf5aRB3s.

Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy op-
timization attains globally optimal policy.
In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence dâAlchÂ´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances
in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pp. 10564â10575, 2019. URL https://proceedings.neurips.cc/paper/2019/
hash/227e072d131ba77451d8f27ab9afdfb7-Abstract.html.

11

Published as a conference paper at ICLR 2023

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. In Isabelle Guyon, Ulrike von Luxburg,
Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neu-
ral Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
6379â6390, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
68a9750337a418a86fe06c1991a1d64c-Abstract.html.

Haihao Lu, Robert M. Freund, and Vahab S. Mirrokni. Accelerating greedy coordinate descent
methods. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, StockholmsmÂ¨assan, Stockholm, Sweden, July 10-
15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 3263â3272. PMLR,
2018. URL http://proceedings.mlr.press/v80/lu18b.html.

Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and Efï¬cient Off-
In Advances in Neural Information Processing Systems, vol-

Policy Reinforcement Learning.
ume 29. Curran Associates, Inc., 2016.

Georgios Papoudakis, Filippos Christianos, Lukas SchÂ¨afer, and Stefano V. Albrecht. Comparative
evaluation of multi-agent deep reinforcement learning algorithms. CoRR, abs/2006.07869, 2020.
URL https://arxiv.org/abs/2006.07869.

Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr,
Wendelin Boehmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gra-
dients. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.),
Advances in Neural Information Processing Systems, volume 34, pp. 12208â12221. Curran As-
sociates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/
65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf.

James Queeney, Yannis Paschalidis, and Christos G Cassandras. Generalized proximal policy op-
timization with sample reuse.
In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and
J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34,
pp. 11909â11919. Curran Associates, Inc., 2021. URL https://proceedings.neurips.
cc/paper/2021/file/63c4b1baf3b4460fa9936b1a20919bec-Paper.pdf.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforce-
ment learning. In International conference on machine learning, pp. 4295â4304. PMLR, 2018.

Mikayel Samvelyan, Tabish Rashid, Christian SchrÂ¨oder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob N. Foerster, and Shimon White-
son. The starcraft multi-agent challenge. In Edith Elkind, Manuela Veloso, Noa Agmon, and
Matthew E. Taylor (eds.), Proceedings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems, AAMAS â19, Montreal, QC, Canada, May 13-17, 2019, pp.
2186â2188. International Foundation for Autonomous Agents and Multiagent Systems, 2019.
URL http://dl.acm.org/citation.cfm?id=3332052.

John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust re-
gion policy optimization. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, vol-
ume 37 of JMLR Workshop and Conference Proceedings, pp. 1889â1897. JMLR.org, 2015. URL
http://proceedings.mlr.press/v37/schulman15.html.

John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. In Yoshua Bengio and
Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016,
San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http:
//arxiv.org/abs/1506.02438.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347.

12

Published as a conference paper at ICLR 2023

Hao-Jun Michael Shi, Shenyinying Tu, Yangyang Xu, and Wotao Yin. A Primer on Coordinate

Descent Algorithms. arXiv:1610.00040 [math, stat], January 2017.

Mingfei Sun, Sam Devlin, Katja Hofmann, and Shimon Whiteson. Monotonic Improvement Guar-
antees under Non-stationarity for Decentralized PPO. arXiv:2202.00082 [cs], January 2022.

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, VinÂ´Ä±cius Flores Zam-
baldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Grae-
pel. Value-decomposition networks for cooperative multi-agent learning. CoRR, abs/1706.05296,
2017. URL http://arxiv.org/abs/1706.05296.

Hossein Taheri and Christos Thrampoulidis. Decentralized Learning with Separable Data: General-

ization and Fast Algorithms, September 2022.

J Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan, Luis S
Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo Perez-Vicente, et al. Pettingzoo: Gym
for multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 34:
15032â15043, 2021.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026â5033.
IEEE, 2012. doi: 10.1109/IROS.2012.6386109.

Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: duplex dueling
multi-agent q-learning. In 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.
net/forum?id=Rcmk0xxIQV.

Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
In 8th International Conference on Learning Represen-
optimality and rates of convergence.
tations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL
https://openreview.net/forum?id=BJgQfkSYDS.

Yuhui Wang, Hao He, and Xiaoyang Tan. Truly proximal policy optimization. In Amir Globerson
and Ricardo Silva (eds.), Proceedings of the Thirty-Fifth Conference on Uncertainty in Artiï¬cial
Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, volume 115 of Proceedings of Machine
Learning Research, pp. 113â122. AUAI Press, 2019. URL http://proceedings.mlr.
press/v115/wang20b.html.

Muning Wen, Jakub Grudzien Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong

Yang. Multi-Agent Reinforcement Learning is a Sequence Modeling Problem, May 2022.

Ying Wen, Hui Chen, Yaodong Yang, Zheng Tian, Minne Li, Xu Chen, and Jun Wang. A game-
theoretic approach to multi-agent trust region optimization, 2021. URL https://arxiv.
org/abs/2106.06828.

Zifan Wu, Chao Yu, Deheng Ye, Junge Zhang, Haiyin Piao, and Hankz Hankui Zhuo. Coordi-
In MarcâAurelio Ranzato, Alina Beygelzimer, Yann N.
nated proximal policy optimization.
Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information
Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021,
NeurIPS 2021, December 6-14, 2021, virtual, volume 34, pp. 26437â26448. Curran Asso-
ciates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
de73998802680548b916f1947ffbad76-Abstract.html.

Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang,
Xipeng Wu, Qingwei Guo, Qiaobo Chen, Yinyuting Yin, Hao Zhang, Tengfei Shi, Liang Wang,
Qiang Fu, Wei Yang, and Lanxiao Huang. Mastering complex control in MOBA games with
deep reinforcement learning. In AAAI 2020, pp. 6672â6679. AAAI Press, 2020. URL https:
//ojs.aaai.org/index.php/AAAI/article/view/6144.

Jianing Ye, Chenghao Li, Jianhao Wang, and Chongjie Zhang. Towards Global Optimality in Coop-

erative MARL with Sequential Transformation, August 2022.

13

Published as a conference paper at ICLR 2023

Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The Surprising

Effectiveness of PPO in Cooperative, Multi-Agent Games, July 2022.

Ming Zhou, Ziyu Wan, Hanjing Wang, Muning Wen, Runzhe Wu, Ying Wen, Yaodong Yang,
Weinan Zhang, and Jun Wang. Malib: A parallel framework for population-based multi-agent
reinforcement learning. CoRR, abs/2106.07551, 2021. URL https://arxiv.org/abs/
2106.07551.

14

.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.

16
16
16
19
19
20
20
22
23
23

24
24
24
32
32

34

35

Published as a conference paper at ICLR 2023

Supplementary Material

Table of Contents

A Proofs

.

.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.
.
A.1 Notations
.
.
A.2 Useful Lemmas
.
.
A.3 Proofs of Intuitive Sequential Update .
.
.
A.4 Proofs of Monotonic Policy Improvement of A2PO .
.
A.5 Proofs of Incrementally Tightened Bound of A2PO .
A.6 Proofs of Monotonic Policy Improvement of MAPPO, CoPPO and HAPPO .
.
.
A.7 Comparisons on Monotonic Improvement Bounds
.
A.8 Preceding-agent Off-policy Correction .
.
.
.
A.9 Why Off-policyness is More Serious in Sequential Update Scheme? .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.

.
.
.

.
.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

B Experimental Details

.

.

B.1 Implementation .
.
.
B.2 Experimental Setup and Additional Results .
.
B.3 Wall Time Analysis .
.
.
B.4 Hyper-parameters .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

C The Related Work of Other MARL Methods

D The Related Work of Coordinate Descent

15

Published as a conference paper at ICLR 2023

A PROOFS

A.1 NOTATIONS

We list the main notations used in Tab. 4.

Table 4: The notations and symbols used in this paper.

Notation

Deï¬nition

S
N
n
i
Ai
r
T
Î³
t
st
ai
t
at
dÏ
P r
V
A
Ï
e
ei
Ïi
Â¯Ïi
Ï
Î»
Â¯Ï
ËÏi
J (Ï)

The state space
The set of agents
The number of agents
The agent index
The action space of agent i
The reward function
The transition function
The discount factor
The time-step
The state at time-step t
The action of agent i at time-step t
The joint action at time-step t
The discounted state visitation distribution
The state probability function
The value function
The advantage function
The trajectory of an episode
A set of preceding agents
The set of preceding agents updated before agent i
The policy of agent i
The updated policy of agent i
The joint policy
The bias and variance balance parameter
The joint target policy
The joint policy after updating agent i
The expected return / performance of the joint policy Ï

L ËÏiâ1 ( ËÏi) The surrogate objective of agent i
LI

ËÏiâ1 ( ËÏi) An intuitive surrogate objective of agent i
GÏ(Â¯Ï)
(cid:15)
DT V
Î±
Î¾i
C
R

The surrogate objective of all agents
The upper bound of an advantage function
The total variation distance function
The total variation distance between 2 policies
The off policy correction error of ËÏiâ1
The clipping parameter adaptation function
The agent selection function

A.2 USEFUL LEMMAS

Lemma 1 (Multi-agent Policy Performance Difference Lemma). Given any joint policies Â¯Ï and Ï,
the difference between the performance of two joint policies can be expressed as:

(Â¯Ï)

(Ï) =

â J

J
Î³) (cid:80)â
t=0 Î³tP r(st = s
|

1

â

1

where dÏ = (1

â

E(s,a)â¼(d Â¯Ï,Â¯Ï) [AÏ(s, a)] ,

Î³

Ï) is the normalized discounted state visitation distribution.

Proof. A corollary of the Policy Performance Difference Lemma, see Lemma 1.16 in Alekh et al.
(cid:3)
(2022).

For convenience, we give some properties and deï¬nitions of coupling5 and the deï¬nition of Î±-
coupled policy pair (Schulman et al., 2015) here.

5The deï¬nition of coupling and the properties can be found in any textbook containing Markov Chains.

16

Published as a conference paper at ICLR 2023

Deï¬nition 1 (Coupling) A coupling of two probability distributions Âµ and Î½ is a pair of random
variables (X, Y ) such that the marginal distribution of X is Âµ and the marginal distribution of Y is
Î½. A coupling (X, Y ) satisï¬es the following constraints: P r(X = x) = Âµ(x) and P r(Y = y) =
Î½(y).

Proposition 3 For any coupling (X, Y ) that DT V (Âµ
Î½)
(cid:107)

â¤
Proposition 4 There exists a coupling (X, Y ) that DT V (Âµ
(cid:107)
s)), that P r(a = Â¯a)

Corollary 1 For all s, there exists a coupling (Ï(
for a

Î½) = P r(X

= Y ).

P r(X

s), Â¯Ï(

Â·|

Â·|

s), Â¯a

s).

Â¯Ï(

Ï(

= Y ).

â¼

Â·|

â¼

Â·|

1

â

â¥

Dmax

T V (Ï

Â¯Ï),

(cid:107)

Proof. By prop. 4 there exists a coupling (Ï(

P r(a = Â¯a) = P r(a

1

â

s), Â¯Ï(

s)), s.t.

Â·|

Â·|
= Â¯a) = DT V (Ï, Â¯Ï)

Dmax

T V (Ï

Â¯Ï)

(cid:107)

â¤

(cid:3)

Corollary 2 For all s, DT V (Ï(

Proof. We denote Ï(

s) as Ï(

s))

Â·|

Â·|

Â¯Ï(

s)
(cid:107)
) for brevity.
Â·

(cid:80)n

i=1 DT V (Ïi(

â¤

Â¯Ïi(
s)
Â·|
(cid:107)

Â·|

s)).

DT V (Ï(

s)

Â·|

(cid:88)

a1,a2,...,an

(cid:88)

a1,a2,...,an

Â·|
s))

Â¯Ï(
Â·|
(cid:107)
(cid:12)
n
(cid:12)
(cid:89)
Ïi(ai)
(cid:12)
(cid:12)
(cid:12)
i=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i=1

Ïi(ai)

n
(cid:89)

n
(cid:89)

i=1

Â¯Ïi(ai)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ï1(a1)

n
(cid:89)

i=2

â

â

Â¯Ïi(ai) + Ï1(a1)

n
(cid:89)

i=2

Â¯Ïi(ai)

n
(cid:89)

i=1

(cid:12)
(cid:12)
Â¯Ïi(ai)
(cid:12)
(cid:12)
(cid:12)

â

(cid:88)

(cid:12)
(cid:12)Ï1(a1)

(cid:12)
(cid:12)

(cid:88)

a1

(cid:88)

a2,...,an

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:89)

i=2

a2,...,an

Ïi(ai)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:89)

i=2

n
(cid:89)

Ïi(ai)

n
(cid:89)

i=2

(cid:12)
(cid:12)
Â¯Ïi(ai)
(cid:12)
(cid:12)
(cid:12)

+

1
2

â

(cid:88)

(cid:12)
(cid:12)Ï1(a1)

a1

â

(cid:12)
Â¯Ï1(a1)
(cid:12)

(cid:88)

a2,...,an

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:89)

i=2

Â¯Ïi(ai)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Â¯Ïi(ai)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

1
2

(cid:88)

(cid:12)
(cid:12)Ï1(a1)

a1

Â¯Ï1(a1)(cid:12)
(cid:12)

â

â

i=2

(cid:88)

ai

Ïi(ai)
|

â

Â¯Ïi(ai)

|

DT V (Ïi(
Â·|

s)
(cid:107)

Â¯Ïi(
Â·|

s))

(cid:3)

Deï¬nition 2 (Î±-coupled policy pair) If (Ï, Â¯Ï) is an Î±-coupled policy pair, then (a, Â¯a
s) satisï¬es
|
s), Â¯a
P r(a

Î± for all s, and a

s).

Ï(

Â¯Ï(

s)

= Â¯a
|

â¤

â¼

Â·|

â¼

Â·|

From Corollaries 1 and 2, we know that given any joint policy pair Ï and Â¯Ï, select Î± =
Dmax
Dmax

s)), then (Ï, Â¯Ï) is an Î±-coupled policy pair that for all s, P r(a
T V (Ïi
s))

i=1 Î±i, where Î±i = Dmax

= Â¯a
|

Â¯Ï(
Â¯Ï(

(cid:80)n

s)

â¤

T V (Ï(
Â·|
T V (Ï(
Â·|

s)
(cid:107)
s)
(cid:107)

Â·|
Â·|

Â¯Ïi).
(cid:107)

â¤

Lemma 2 Given any joint policies Â¯Ï and Ï, the following inequality holds:

Eaâ¼Â¯Ï [AÏ(s, a)]

|

2(cid:15)

| â¤

n
(cid:88)

i=1

Î±i ,

where Î±i = Dmax

T V (Â¯Ïi

(cid:107)

Ïi) and (cid:15) = maxs,a

AÏ(s, a)
|
|

.

17

=

=

â¤

=

1
2

1
2

1
2

1
2

n
(cid:88)

Â· Â· Â·
1
2

i=1
n
(cid:88)

i=1

â¤

=

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
Published as a conference paper at ICLR 2023

Proof. Note that Eaâ¼Ï[AÏ(s, a)] = 0. We have

Eaâ¼Â¯Ï [AÏ(s, a)]
|
|

â

Eaâ¼Ï [AÏ(s, a)]
|
AÏ(s, a)](cid:12)
(cid:12)
â
s)E(Â¯a,a)â¼(Â¯Ï,Ï) [AÏ(s, Â¯a)

EÂ¯aâ¼Â¯Ï [AÏ(s, Â¯a)]
=
|
= (cid:12)
(cid:12)E(Â¯a,a)â¼(Â¯Ï,Ï) [AÏ(s, Â¯a)
(cid:12)
(cid:12)P r(Â¯a
=
n
(cid:88)

= a
|
Î±iE(Â¯a,a)â¼(Â¯Ï,Ï) [

AÏ(s, Â¯a)
|

â

â
AÏ(s, a)

(cid:12)
AÏ(s, a)]
(cid:12)

â¤

â¤

i=1
n
(cid:88)

i=1

Î±i

Â·

2 max
s,a |

AÏ(s, a)

|

]
|

(cid:3)

Lemma 3 (Multi-agent Advantage Discrepancy Lemma). Given any joint policies Ï1, Ï2 and Ï3,
the following inequality holds:

E

(cid:12)
(cid:12)
(cid:12)
4(cid:15)Ï1

â¤

Â·

(cid:104)

AÏ1 (cid:105)

E

(st,at)â¼(P rÏ2 ,Ï2)
T V (Ï1

Dmax

â
(1

(st,Â¯at)â¼(P rÏ3 ,Ï2)
T V (Ï2

Dmax

t
Ï3))

) ,

(cid:107)

Ï2)
(cid:107)

(1

Â·

â

â

(cid:104)

AÏ1(cid:105)(cid:12)
(cid:12)
(cid:12)

where (cid:15)Ï1

= maxs,a

AÏ1
(cid:107)

(s, a)

(cid:107)

and we denote A(s, a) as A for brevity.

Proof. Let nt represent the times a

= Â¯a (Ï1 disagrees with Ï3) before timestamp t.

(cid:12)
(cid:12)
(cid:12)

E

(st,at)â¼(P rÏ2 ,Ï2)

(cid:104)

AÏ1(cid:105)

â

E

=P r(nt > 0)

(cid:12)
(cid:12)
(cid:12)

Â·

E

(st,Â¯at)â¼(P rÏ3 ,Ï2)
(cid:104)

AÏ1(cid:105)

â

AÏ1(cid:105)(cid:12)
(cid:104)
(cid:12)
(cid:12)

E

(st,Â¯at)â¼(P rÏ3 ,Ï2)|nt>0

(cid:104)

AÏ1(cid:105)(cid:12)
(cid:12)
(cid:12)

(st,at)â¼(P rÏ2 ,Ï2)|nt>0

P r(nt = 0))

E

Â·

P r(ak = Â¯ak

ak
|

â¼

Ï2(
Â·|

sk), Â¯ak

Ï3(
Â·|

sk)))

E

Â·

â¼

t
(cid:89)

k=1
t
(cid:89)

(1

k=1

(a)
= (1

(1

â¤

(b)

(1

â¤

=(1

â

â

â

â

(1
â¤
â
=4(cid:15)Ï1

Dmax

T V (Ï2

Ï3)))

(cid:107)

E

Â·

â

(1

â

Dmax

(cid:107)

T V (Ï2
T V (Ï2
(cid:107)
Ï2)
(cid:107)

Dmax
(1
â
T V (Ï1
Dmax

t
Ï3))
t
Ï3))

)

)

(1

E

Â·

2

Â·
(1

Â·

Â·

Â·
(st,at)â¼(P rÏ2 ,Ï2)|nt>0[AÏ1
E

â

â

In (a), we denote
â
(b) follows the deï¬nition of Î±-coupled policy pair.

|

]

T V (Ï1

2
Â·
Dmax

Dmax
T V (Ï2

(cid:15)Ï1

Â·

Ï2)
(cid:107)
t
Ï3))
(cid:107)
(st,Â¯at)â¼(P rÏ3 ,Ï2)|nt>0[AÏ1
E

)

]
|

as E for brevity.
(cid:3)

We provide a useful equation of the normalized discounted state visitation distribution here.

Proposition 5

E

(s,a)â¼(dÏ1 ,Ï2) [f (s, a)] = (1

Î³)

â

= (1

Î³)

â

= (1

Î³)

â

s
â
(cid:88)

t=0
â
(cid:88)

t=0

(cid:88)

â
(cid:88)

Î³tP r(st = s
|

Ï1)

(cid:88)

a

Ï2(a
|

s)f (s, a)

t=0

Î³t (cid:88)

s

Ï1)
P r(st = s
|

(cid:88)

a

Ï2(a
s)f (s, a)
|

Î³tE

(st,at)â¼(P rÏ1 ,Ï2)[f (st, at)]

18

(cid:54)
(cid:54)
Published as a conference paper at ICLR 2023

A.3 PROOFS OF INTUITIVE SEQUENTIAL UPDATE

( ËÏi)

( ËÏiâ1)

â

â J

1

(cid:12)
(cid:12)
E(s,a)â¼(dÏ, ËÏi) [AÏ]
(cid:12)
(cid:12)

E

(s,a)â¼(d ËÏi , ËÏi)

E

(s,a)â¼(d ËÏi , ËÏi)

E(s,a)â¼(dÏ, ËÏi)

Î³
1
â
A ËÏiâ1(cid:105)
(cid:104)

A ËÏiâ1(cid:105)
(cid:104)

â

â
A ËÏiâ1(cid:105)

(cid:104)

E(s,a)â¼(dÏ, ËÏi) [AÏ]

(cid:12)
(cid:12)
(cid:12)
A ËÏiâ1(cid:105)(cid:12)
(cid:12)
(cid:12)
(cid:12)
E(s,a)â¼(dÏ, ËÏi) [AÏ]
(cid:12)
(cid:12)

E(s,a)â¼(dÏ, ËÏi)

(cid:104)

â

Î³t(1

(1

â

â

(cid:88)

Î±j)t)

jâ(eiâª{i})

E(s,a)â¼(dÏ, ËÏi)

Î³

(cid:104)(cid:12)
(cid:12)A ËÏiâ1
(cid:12)

(cid:105)

AÏ(cid:12)
(cid:12)
(cid:12)

â

(cid:12)
(cid:12)
(cid:12)
(cid:12)J
1

â¤

1

â¤

1

â
1

â

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Î³

Î³
1

+

1

Î³

â
4(cid:15) ËÏiâ1

Î±i

â¤

â¤

1

+

1

â
4(cid:15) ËÏiâ1

(cid:12)
(cid:12)
(cid:12)
â
(cid:88)

t=0

Î±i(

1

â

1

Î³ â

1

Î³(1

â

â

1
(cid:80)
jâ(eiâª{i}) Î±j)

) +

1

â

1

Î³

ï£®
ï£°4Î±i(cid:15) ËÏiâ1

ï£¹

ï£»

Î±j(cid:15)Ï

+ 2

(cid:88)

jâei

A.4 PROOFS OF MONOTONIC POLICY IMPROVEMENT OF A2PO

Theorem 1 (Single Agent Monotonic Bound) For agent i, let (cid:15)i = maxs,a
maxs,a

A ËÏiâ1

(s, a)

(s, a)

Â¯Ïj)

(ei

, Î±j = Dmax
|

T V (Ïj

(cid:107)

j
â

â

AÏ, ËÏiâ1
|

A ËÏiâ1
, Î¾i =
(s, a)
|
|
), then we have:
i
}

âª {

â

( ËÏi)

(cid:12)
(cid:12)

J

(cid:12)
â L ËÏiâ1( ËÏi)
(cid:12)

â¤

4(cid:15)iÎ±i(cid:0)

1

4Î³(cid:15)i

1

1
(cid:80)
jâ(eiâª{i}) Î±j)

Î³(1

1

Î³ â
â
â
(cid:0)Î±i (cid:88)

â
Î±j(cid:1) +

jâ(eiâª{i})

Î¾i

â

1

.

Î³

(cid:1) +

Î¾i

â

1

Î³

(4)

â¤

(1

â

Î³)2

Proof. Using Lemma 3 and Prop. 5, we get

1

Î³
1
â
A ËÏiâ1(cid:105)
(cid:104)

A ËÏiâ1(cid:105)
(cid:104)

â
A ËÏiâ1(cid:105)

(cid:104)

E(s,a)â¼(dÏ, ËÏi)

(cid:104)

AÏ, ËÏiâ1(cid:105)(cid:12)

E(s,a)â¼(dÏ, ËÏi)

â

E(s,a)â¼(dÏ, ËÏi)

(cid:104)

(cid:104)

(cid:12)
(cid:12)
(cid:12)
AÏ, ËÏiâ1(cid:105)(cid:12)
(cid:12)
(cid:12)
A ËÏiâ1(cid:105)(cid:12)
(cid:12)
(cid:12)
AÏ, ËÏiâ1 (cid:105)(cid:12)
(cid:104)
(cid:12)
(cid:12)

E(s,a)â¼(dÏ, ËÏi)

E(s,a)â¼(dÏ, ËÏi)

â

( ËÏi)

( ËÏiâ1)

â

â J

E

(s,a)â¼(d ËÏi , ËÏi)

E

(s,a)â¼(d ËÏi , ËÏi)

(cid:12)
(cid:12)
(cid:12)
(cid:12)J
1

=

1

â¤

1

â
1

â

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Î³

Î³
1

+

1

Î³

â
4(cid:15) ËÏiâ1

Î±i

â¤

â¤

(cid:12)
(cid:12)
(cid:12)
â
(cid:88)

t=0

Î³t(1

(1

â

â

(cid:88)

Î±j)t) +

jâ(eiâª{i})

1

â

1

Î³

E(s,a)â¼(dÏ, ËÏi)

(cid:104)(cid:12)
(cid:12)A ËÏiâ1
(cid:12)

â

AÏ, ËÏiâ1(cid:12)
(cid:12)
(cid:12)

(cid:105)

4(cid:15) ËÏiâ1

Î±i(

1

â

1

Î³ â

1

Î³(1

â

â

1
(cid:80)
jâ(eiâª{i}) Î±j)

) +

1

â

1

Î¾i

Î³

(cid:3)

19

Published as a conference paper at ICLR 2023

Theorem 2 (Joint Monotonic Bound) For each agent i
A ËÏiâ1
Î±i = Dmax

AÏ, ËÏiâ1

(s, a)

T V (Ïi

, let (cid:15)i = maxs,a
(s, a)
â N
|
, and (cid:15) = maxi (cid:15)i, then we have:
(s, a)
|

A ËÏiâ1

|

,

â

Â¯Ïi), Î¾i = maxs,a
|
(cid:107)
n
(cid:88)

Ï(Â¯Ï)

4(cid:15)

â G

| â¤

(Â¯Ï)

1

Î±i(cid:0)

i=1

4Î³(cid:15)

â¤

(1

Î³)2

â

1

Î³ â

1

Î³(1

â
â
n
(cid:0)Î±i (cid:88)
(cid:88)

i=1

jâ(eiâª{i})

1
(cid:80)
jâ(eiâª{i}) Î±j)
(cid:80)n
i=1 Î¾i
Î³

â
Î±j(cid:1) +

.

1

â

(cid:1) +

(cid:80)n

i=1 Î¾i
Î³

1

â

|J

Proof.

=

=

|J
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)J
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)J
n
(cid:88)

(Â¯Ï)

(Â¯Ï)

Ï(Â¯Ï)
|

â G

(Ï)

â

â J

n
(cid:88)

i=1

E(s,a)â¼(dÏ, ËÏi)

(cid:104)
AÏ, ËÏiâ1

(cid:105)
(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

( ËÏn)

( ËÏnâ1) +

+

J

Â· Â· Â·

( ËÏ1)

â J

( ËÏ0)

â J

E(s,a)â¼(dÏ, ËÏi)

1

â

Î³

1
(cid:80)
jâ(eiâª{i}) Î±j)

â

1

Î³

â
(cid:104)
AÏ, ËÏiâ1

i=1

(s, a)

(cid:105)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:80)n

+

1

i=1 Î¾i
Î³

â

(cid:12)
(cid:12)
(cid:12)
(cid:12)J

( ËÏi)

â J

( ËÏiâ1)

â

1

â¤

i=1

4(cid:15)

â¤

Î±i

n
(cid:88)

i=1

4Î³(cid:15)

â¤

(1

Î³)2

â

(cid:32)

1

1

n
(cid:88)

i=1

Î³(1

Î³ â

1

â
â
ï£«
ï£­Î±i (cid:88)

â

ï£¶

Î±j

ï£¸ +

jâ(eiâª{i})

(cid:80)n

i=1 Î¾i
Î³

1

â

.

1

n
(cid:88)

E(s,a)â¼(dÏ, ËÏi)

(cid:104)
AÏ, ËÏiâ1

(cid:105)
(s, a)

(5)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:3)

A.5 PROOFS OF INCREMENTALLY TIGHTENED BOUND OF A2PO

Assume agent k is updated with order k in the sequence 1, . . . , n, since ËÏkâ1 is known, we have

(Â¯Ï)

Ï(Â¯Ï)

|

â G

(cid:12)
(cid:12)

(cid:12)
(cid:12)

J

J

( ËÏi)

â L ËÏiâ1( ËÏi)(cid:12)

(cid:12) + 4(cid:15)

( ËÏi)

â L ËÏiâ1( ËÏi)(cid:12)

(cid:12) + 4(cid:15)

|J
kâ1
(cid:88)

â¤

i=1

kâ2
(cid:88)

â¤

i=1

...

(cid:32)

n
(cid:88)

i=k

Î±i

n
(cid:88)

Î±i

i=kâ1

1

1
(cid:32)

â

1

(cid:33)

1
(cid:80)
jâ(eiâª{i}) Î±j)

Î³ â

1

Î³(1

â

â

1

â

Î³ â

1

Î³(1

â

â

1
(cid:80)
jâ(eiâª{i}) Î±j)

+

(cid:33)

(cid:80)n

i=k Î¾i
Î³
1

â
(cid:80)n

+

i=kâ1 Î¾i
Î³
1

â

n
(cid:88)

i=1

Î±i

4(cid:15)

â¤

(cid:32)

1

â

1

Î³ â

1

Î³(1

â

â

(cid:33)

1
(cid:80)
jâ(eiâª{i}) Î±j)

(cid:80)n

+

1

i=1 Î¾i
Î³

â

Thus the condition for improving

J

(Â¯Ï) is relaxed during updating agents at a stage.

A.6 PROOFS OF MONOTONIC POLICY IMPROVEMENT OF MAPPO, COPPO AND HAPPO

In this section, we give proof of the monotonic policy improvement of MAPPO, and unify the
formats of the monotonic bounds of CoPPO and HAPPO, without considering the parameter-sharing
method.

MAPPO. For MAPPO,

for agent i,

(Â¯Ï)

J

â J

Ï(Â¯Ï) = (cid:80)n

L
(Ï)

(cid:104)

1
1âÎ³

â

(cid:104)
E(s,a)â¼(dÏ,Ï)
(Ï) + 1
1âÎ³
Ïi AÏ(cid:105)(cid:105)
(cid:104) Â¯Ïi

is bounded.

i=1 J

E(s,a)â¼(dÏ,Ï)

(cid:104) Â¯Ïi
Ïi AÏ(cid:105)(cid:105)

. We ï¬rst prove that

20

Published as a conference paper at ICLR 2023

(cid:12)
(cid:12)
(cid:12)
(cid:12)J
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

=

1
â
â
(cid:88)

Î³

Î³t

t=0

â
(cid:88)

â¤

t=0

(Â¯Ï)

(Ï)

â

1

â J

(cid:20)
E(s,a)â¼(dÏ,Ï)

1

â

Î³

E(s,a)â¼(d Â¯Ï,Â¯Ï) [AÏ]

E(s,a)â¼(dÏ,Ï)

â

E(st,at)â¼(P r Â¯Ï,Â¯Ï)AÏ

â

E(st,at)â¼(P rÏ,Ï)

ï£«

ï£«

2Î³t

ï£­

ï£­

ï£¶

Î±j

ï£¸

Â·

n
(cid:88)

j=1

(cid:15)Ï + Î±i

ï£¶

(cid:15)Ï

ï£¸

Â·

(cid:21)(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20) Â¯Ïi
Ïi AÏ
(cid:20) Â¯Ïi
Ïi AÏ
(cid:20) Â¯Ïi
Ïi AÏ

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

2(cid:15)Ï
Î³
1

â

ï£«
ï£­Î±i +

ï£¶

Î±j

ï£¸

n
(cid:88)

j=1

Sum the bounds for all agents and take the average, we get

1
n

1

1

â

Î³

n
(cid:88)

(cid:20)
E(s,a)â¼(dÏ,Ï)

i=1

(cid:20) Â¯Ïi
Ïi AÏ

(cid:21)(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)J
2(cid:15)Ï
Î³
1

â

â¤

(Â¯Ï)

(Ï)

â

â J

n + 1
n

n
(cid:88)

j=1

Î±j

Finally, the monotonic bound for MAPPO is

(cid:20) Â¯Ïi
Ïi AÏ
(cid:20) Â¯Ïi
Ïi AÏ

(cid:21)(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

(cid:20)
E(s,a)â¼(dÏ,Ï)

Î³

1

â

i=1

n
(cid:88)

i=1

Î³

1
n

1

(cid:20)
E(s,a)â¼(dÏ,Ï)

â
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

n
(cid:88)

(cid:20)
E(s,a)â¼(dÏ,Ï)

(cid:20) Â¯Ïi
Ïi AÏ

(cid:21)(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

j=1

Î±j +

n

â
n

n
(cid:88)

1

1

i=1

1

â

Î±i

Î³

Â·

2(cid:15)Ï

(Â¯Ï)

(Ï)

â

1

â J

1

â

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)J
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)J

â¤

â¤

=

(Â¯Ï)

(Ï)

â J

n

+

â
n

1

1

1

Î³

â
n + 1
n

n
(cid:88)

i=1

Î±i

2(cid:15)Ï
Î³
1

â
4(cid:15)Ï
Î³
1

â

CoPPO. We prove the results of CoPPO in a uniï¬ed and convenient form. For CoPPO,
E(s,a)â¼(dÏ,Â¯Ï)[AÏ(s, a)], we prove the bound using Lemma 3.

(Ï) + 1
1âÎ³

J

Ï(Â¯Ï) =

L

21

Published as a conference paper at ICLR 2023

(cid:12)
(cid:12)
(cid:12)
(cid:12)J
1

â¤

1
â
â
(cid:88)

â¤

t=0

4(cid:15)Ï

â¤

4(cid:15)Ï

â¤

(Â¯Ï)

(Ï)

â

1

â J

(cid:12)
(cid:12)
E(s,a)â¼(dÏ,Â¯Ï)[AÏ]
(cid:12)
(cid:12)

Î³

1

â

(cid:12)
(cid:12)E(s,a)â¼(d Â¯Ï,Â¯Ï)[AÏ]

E(s,a)â¼(dÏ,Â¯Ï)[AÏ](cid:12)
(cid:12)

â

Î³

Î³t (cid:12)

(cid:12)E(s,a)â¼(P r Â¯Ï,Â¯Ï) [AÏ]

E(s,a)â¼(P rÏ,Â¯Ï) [AÏ](cid:12)
(cid:12)

â

â
(cid:88)

t=0
n
(cid:88)

i=1

Î³t

Î±i

n
(cid:88)

i=1
(cid:32)

1

Î±i (cid:0)1

(1

â

â

Dmax

T V (Ï

Â¯Ï))t(cid:1)

(cid:107)

1

â

Î³ â

1

Î³(1

â

â

1
(cid:80)n

j=1 Î±j)

(cid:33)

HAPPO. Following the proof of Lemma 2 in Kuba et al. (2022), we know that HAPPO has
the same monotonic improvement bound as that of CoPPO. For the monotonic improvement of
( ËÏiâ1) +
a single agent, we formulate the surrogate objective of agent i using HAPPO as
E(s,a)â¼(dÏ, ËÏiâ1)[AÏ(s, a)], as shown in Proposition 3 of Kuba

1
1âÎ³
et al. (2022). Following the proof of Thm. 1, we get the following inequality.

E(s,a)â¼(dÏ, ËÏi)[AÏ(s, a)]

1
1âÎ³

â

J

( ËÏi)

( ËÏiâ1)

â

â J

E(s,a)â¼(dÏ, ËÏi) [AÏ] +

1

1

Î³

(cid:12)
(cid:12)
E(s,a)â¼(dÏ, ËÏiâ1)[AÏ(s, a)]
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)J
1

â¤

1

â¤

1

â
1

â

+

Î³

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Î³
(cid:12)
1

Î³

1

â

1

Î³
1
â
(cid:104)
A ËÏiâ1 (cid:105)

(cid:104)
A ËÏiâ1 (cid:105)

E

(s,a)â¼(d ËÏi , ËÏi)

E

(s,a)â¼(d ËÏi , ËÏi)

â

â
(cid:12)
E(s,a)â¼(dÏ, ËÏi) [AÏ]
(cid:12)
(cid:12) +
(cid:104)

E(s,a)â¼(dÏ, ËÏi)

1

Î³

1

â

A ËÏiâ1(cid:105)(cid:12)

(cid:12)
(cid:12)
(cid:12)
1

E(s,a)â¼(dÏ, ËÏi)

(cid:104)

(cid:12)
(cid:12)
(cid:12)

â
A ËÏiâ1(cid:105)

1

â

â
(cid:12)
E(s,a)â¼(dÏ, ËÏi) [AÏ]
(cid:12)
(cid:12) + 2

Î±jeÏ

(cid:88)

jâei

Î³

1

â

1

(cid:12)E(s,a)â¼(dÏ, ËÏiâ1)[AÏ(s, a)](cid:12)
(cid:12)
(cid:12)

Î³

4(cid:15) ËÏiâ1

Î±i

â¤

â
(cid:88)

t=0

Î³t(1

(1

â

â

(cid:88)

Î±j)t)

jâ(eiâª{i})

+

1

1

â

Î³

E(s,a)â¼(dÏ, ËÏi)

(cid:104)(cid:12)
(cid:12)A ËÏiâ1
(cid:12)

(cid:105)

AÏ(cid:12)
(cid:12)
(cid:12)

â

+ 2

1

1

â

Î³

(cid:88)

jâei

Î±jeÏ

4(cid:15) ËÏiâ1

Î±i(

â¤

1

â

1

Î³ â

1

Î³(1

â

â

1
(cid:80)
jâ(eiâª{i}) Î±j)

) +

1

â

1

Î³

ï£®
ï£°4Î±i(cid:15) ËÏiâ1

ï£¹

ï£»

Î±j(cid:15)Ï

+ 4

(cid:88)

jâei

The right side of the last inequality is not a monotonic improvement bound, or it does not provide
jâei Î±j(cid:15)Ï is not con-
a guarantee for improving the expected performance
trollable for agent i, whether through policy improvement or value learning. The uncontrollable
term means the expected performance may not be improved even if the total variation distances of
consecutive policies are well constrained.

( ËÏi) since the term (cid:80)

J

A.7 COMPARISONS ON MONOTONIC IMPROVEMENT BOUNDS

CoPPO and HAPPO have the same monotonic bound that is tighter than that of MAPPO. A2PO
achieves the tightest monotonic bound given mild assumptions about the errors of preceding-agent
off-policy correction, which is valid and easy to achieve since preceding-agent off-policy correction
is a contraction operator. A sufï¬cient condition that A2PO has the tightest bound is that Î¾i <

Î³(1âÎ³) (cid:80)

jâN âeiâ{i} Î±j

(1âÎ³(1â(cid:80)

jâeiâª{i} Î±j ))(1âÎ³(1â(cid:80)n

j=1 Î±j )) , for all i

.

â N

22

Published as a conference paper at ICLR 2023

A.8 PRECEDING-AGENT OFF-POLICY CORRECTION

In Retrace(Î») (Munos et al., 2016), consider the current policy as ËÏi=1 and base policy as Ï, we
have the following deï¬nition:

t = rt + Î³Qt+1 +

R

(cid:88)

Î³k(cid:0)

k
(cid:89)

kâ¥1

j=1

Î» min (cid:0)1.0,

Following that same structure, we have:

t = rt + Î³Vt+1 +

R

(cid:88)

Î³k(cid:0)

k
(cid:89)

kâ¥1

j=1

Î» min (cid:0)1.0,

ËÏiâ1(at+j
Ï(at+j

st+j)

|
st+j)
|

ËÏiâ1(at+j
Ï(at+j

st+j)
|
st+j)

|

(cid:1)(cid:1)(rt+k + Î³Qt+k+1

Qt+k) ,

â

(cid:1)(cid:1)(rt+k + Î³Vt+k+1

Vt+k) ,

â

By subtracting Vt, we get the deï¬nition of PreOPC. Or one can get Î³AÏ, ËÏiâ1
Î³Vt+1 for Qt and subtracting rt + Î³Vt+1.

by substituting rt +

A.9 WHY OFF-POLICYNESS IS MORE SERIOUS IN SEQUENTIAL UPDATE SCHEME?

As shown in Fig. 13, the off policy correction in sequential update algorithms improves the perfor-
mance signiï¬cantly while similar performance gaps are not observed when used in simultaneous
update algorithms. We attribute the difference to the inï¬uence of the clipping mechanism on the
total variation distance.

Â¯Ïi). Although we can not prove exact relations,
From Corollary 2, DT V (Ï
(cid:107)
clipping the agents independently tends to larger total variation distances between the current and
future policies of the agents, leading to more âoff-policynessâ in sequential update algorithms.

i=1 DT V (Ïi

(cid:107)

Â¯Ï) < (cid:80)n

23

Published as a conference paper at ICLR 2023

B EXPERIMENTAL DETAILS

B.1

IMPLEMENTATION

For a fair comparison, we (re)implement A2PO and the baselines based on the implementation of
MAPPO. We keep the same structures for all the algorithms and tune all the algorithms following the
same process, i.e., a grid search over a small collection of hyper-parameters, to avoid the inï¬uence
of different implementation details on the results. The grid search is performed on three hyper-
parameters: the learning rate, Î» and the agent block num in the tasks with numerous agents.

The algorithms, including A2PO and baselines, are implemented into both parameter sharing and
parameter independent versions. A2PO in the parameter sharing version is implemented as in Alg. 2.
The main modiï¬cations are colored in blue. We rearrange the loops of agents and ppo epochs.
The number of ppo epochs is divided by n for comparable updating times with the simultaneous
algorithms. The approximated advantage is estimated by correcting the action probabilities of all
the agents given such ei.

Algorithm 2: Agent-by-agent Policy Optimization (Parameter Sharing)
= Ïn

1 Initialize the shared joint policy Ï0 =

with Ï1

0 =

0, . . . , Ïn
Ï1
0 }
{

Â· Â· Â·

function V .

0 , and the global value

2 for iteration m = 1, 2, . . . do
Collect data using Ïmâ1.
3
Policy Ïm = Ïmâ1.
for

4

5

6

7

8

9

10

11

12

(k), preceding agents ei =

P
epochs do
n (cid:101)
(cid:100)
for k = 1, . . . , n do
Agent i =
R
Joint policy ËÏi = Ïm.
Compute the advantage approximation as AÏ, ËÏiâ1
Compute the value target v(st) = AÏ, ËÏiâ1
Ë
L ËÏiâ1 ( ËÏi) as in eq. (6).
Ïi
m = arg maxÏi
Esâ¼dÏ
V = arg minV
â

v(s)
(cid:107)

2.
(cid:107)

V (s)

{R

m

(1), . . . ,

(n

R

1)

.
}

â

(s, a) via eq. (2).

(s, a) + V (s).

Practically, each agent is equipped with a value function, we generate the agent order at once to
avoid estimating the advantage function n(nâ1)
times. The order becomes [1, . . . , i, . . . , j, . . . , n]
>= E
in which E
Aj
|
|

Ai

2

|

|

.

B.2 EXPERIMENTAL SETUP AND ADDITIONAL RESULTS

B.2.1 STARCRAFTII MULTI-AGENT CHALLENGE

StarCraftII Multi-agent Challenge (SMAC) (Samvelyan et al., 2019) provides a wide range of multi-
agent tasks in the battle scenarios of StarCraftII. Algorithms adopting parameter sharing have shown
superior performance in SMAC, so all the algorithms are implemented as parameter sharing. As
shown in Tab. 5, we evaluate the algorithms in 12 maps of SMAC with various difï¬culties, in which
the baselines can not achieve 100% win rates easily. We use the results of Qmix in Yu et al. (2022).
The learning curves for episode return are summarized in Fig. 8.

B.2.2 MULTI-AGENT MUJOCO

Multi-agent MuJoCo (MA MuJoCo) (Peng et al., 2021) contains a range of multi-agent robot con-
tinuous control tasks, in which an agent controls the composition of robot joints. MA MuJoCo
extends the high-dimensional single-agent locomotion tasks in MuJoCo (Todorov et al., 2012), a
widely adopted benchmark for SARL algorithms (Haarnoja et al., 2018; He & Hou, 2020), into the
multi-agent case. Agents must cooperate in their actions for robot locomotion, and different agents
control different compositions of the robot joints. We use the reward settings of the original paper

24

Published as a conference paper at ICLR 2023

Figure 8: Comparisons of median win rate on SMAC.

25

024StepsÃ1060.000.250.500.751.00WinRateMMM0.000.250.500.751.00StepsÃ1070.000.250.500.751.00WinRate3svs5z0.000.250.500.751.00StepsÃ1070.000.250.500.751.00WinRate2cvs64zg024StepsÃ1060.000.250.500.751.00WinRate8mvs9m0.000.250.500.751.00StepsÃ1070.000.250.500.751.00WinRate5mvs6m0.000.250.500.751.00StepsÃ1070.000.250.500.751.00WinRate3s5z0.000.250.500.751.00StepsÃ1070.000.250.500.751.00WinRate10mvs11m0.000.250.500.751.00StepsÃ1070.000.250.500.751.00WinRateMMM20.000.250.500.751.00StepsÃ1070.000.250.500.751.00WinRate3s5zvs3s6z0.000.250.500.751.00StepsÃ1070.000.250.500.751.00WinRate27mvs30m0.00.51.01.5StepsÃ1070.000.250.500.751.00WinRate6hvs8z0.000.250.500.751.00StepsÃ1070.000.250.500.751.00WinRatecorridorMAPPOw/PSCoPPOw/PSHAPPOw/PSA2POw/PSPublished as a conference paper at ICLR 2023

Table 5: Median win rates and standard deviations on SMAC tasks. âw/ PSâ means the algorithm is
implemented as parameter sharing

Map

Difï¬culty MAPPO w/ PS CoPPO w/ PS HAPPO w/ PS A2PO w/ PS Qmix w/ PS

96.9(0.988)
MMM
Easy
100(1.17)
3s vs 5z
Hard
98.4(1.74)
2c vs 64zg
Hard
84.4(4.39)
3s5z
Hard
84.4(2.77)
5m vs 6m
Hard
84.4(2.39)
Hard
8m vs 9m
10m vs 11m Hard
93.8(18.7)
6h vs 8z
Super Hard 87.5(1.53)
3s5z vs 3s6z Super Hard 82.8(19.2)
MMM2
Super Hard 90.6(8.89)
27m vs 30m Super Hard 93.8(3.75)
corridor

Super Hard 96.9(0)

96.9(1.25)
100(2.08)
96.9(0.521)
92.2(2.35)
84.4(2.12)
84.4(2.04)
96.9(2.6)
90.6(0.765)
84.4(2.9)
90.6(6.93)
93.8(2.2)
100(0.659)

95.3(2.48)
100(0.659)
96.9(0.521)
92.2(1.74)
87.5(2.51)
96.9(3.78)
98.4(2.99)
87.5(1.49)
37.5(13.2)
51.6(9.01)
90.6(4.77)
96.9(0.96)

100(1.07)
100(0.534)
96.9(0.659)
98.4(1.04)
90.6(3.06)
100(1.04)
100(0.521)
90.6(1.32)
93.8(19.8)
98.4(1.25)
100(1.55)
100(0)

95.3(2.5)
98.4(2.4)
92.2(4.0)
88.3(2.9)
75.8(3.7)
92.2(2.0)
95.3(1.0)
9.4(2.0)
82.8(5.3)
87.5(2.6)
39.1(9.8)
84.4(2.5)

overall

/

91.1(5.46)

92.6(2.2)

85.9(3.68)

97.4(2.65)

78.4(3.6)

but set the environment to be fully observable6. The agents are heterogeneous and mostly asymmet-
ric in MA-MuJoCo, so we implement the algorithms as parameter-independent. We test 14 tasks of
6 scenarios in MA MuJoCo, as illustrated in Fig. 9.

B.2.3 MULTI-AGENT PARTICLE ENVIRONMENT

We consider the Navigation task of the Multi-agent Particle Environment (MPE) (Lowe et al., 2017)
implemented in PettingZoo (Terry et al., 2021) which implements MPE with minor ï¬xes and pro-
vides convenience for customizing the number of agents and landmarks, and customizing the global
and local rewards., with 3 and 5 agents and corresponding numbers of landmarks. The agents are
rewarded based on the minimum distance to the landmarks and penalized for colliding with each
other, meaning that the reward is entirely up to the coordination behavior. We adopted two different
reward settings: Fully Cooperative and General-sum. In the Fully Cooperative setting, the agents
share the same reward, while in the General-sum setting, the agents are additionally rewarded based
on the local collision detection. The results in Fig. 10 show that A2PO generally outperforms the
baselines on the average return and the sample efï¬ciency. Noted that A2PO is developed in fully
cooperative games, the results in the General-sum setting reveal the potential of extending A2PO
into general-sum games. Further, the performance gap between A2PO and the baselines enlarges
with the increasing number of agents.

B.2.4 GOOGLE RESEARCH FOOTBALL

In the above experiments, we have evaluated A2PO in tasks where
agents can learn both their micro-operations and coordination behav-
iors (SMAC and MA-MuJoCo) and tasks where agents can only learn
coordination behaviors (the Navigation task). However, the coordina-
tion behaviors in the above tasks are relatively easy to discover, e.g.,
agents learn to concentrate their ï¬re to shoot the enemies and cover each
other in SMAC. Recent works (Wen et al., 2022; Yu et al., 2022) have
conducted experiments on Google Research Football academic scenar-
ios with a small number of players and easily accessible targets, making
the coordination behavior also easy to discover. In contrast, we evaluate
A2PO in the full-game scenarios, where the players of the left team, ex-
cept for the goalkeeper, are controlled to play a football match against
the right team controlled by the built-in AI provided by GRF. The agents
in the full-game scenarios have high-dimensional observations, complex
action spaces, and a long-span timescale (3000 steps). We reconstruct the observation space and

Figure 11: 5-vs-5 sce-
nario with Parameter
sharing.

6Empirically, we ï¬nd the fully observable setting does not make the tasks easier because of the information

redundancy.

26

050001000015000Episodes0.00.20.40.60.8WinRateFootball5vs5(ParameterSharing)MAPPOCoPPOHAPPOA2POPublished as a conference paper at ICLR 2023

Figure 9: Comparisons of average episode return on MA-MuJoCo.

27

0.000.250.500.751.00StepsÃ107200040006000EpisodeReturnAnt-v22x40.000.250.500.751.00StepsÃ107200040006000EpisodeReturnAnt-v22x4d0.000.250.500.751.00StepsÃ107200040006000EpisodeReturnAnt-v24x20.000.250.500.751.00StepsÃ107200040006000EpisodeReturnAnt-v28x10.000.250.500.751.00StepsÃ10701000200030004000EpisodeReturnWalker2d-v22x30.000.250.500.751.00StepsÃ107010002000300040005000EpisodeReturnWalker2d-v23x20.000.250.500.751.00StepsÃ10701000200030004000EpisodeReturnWalker2d-v26x10.000.250.500.751.00StepsÃ1070100020003000EpisodeReturnHopper-v23x10.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnHalfCheetah-v22x30.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnHalfCheetah-v23x20.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnHalfCheetah-v26x10.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnHumanoid-v29|80.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnHumanoid-v217x10.000.250.500.751.00StepsÃ10750000100000150000200000EpisodeReturnHumanoidStandup-v29|8MAPPOw/oPSCoPPOw/oPSHAPPOw/oPSA2POw/oPSPublished as a conference paper at ICLR 2023

Figure 10: Comparisons of averaged return on the Multi-agent Particle Environment Navigation
task. Left: The fully cooperative setting. Right: The general-sum setting.

design a dense reward to facilitate training in these scenarios based on Football-paris. The observa-
tion is formed to be agent-speciï¬c. The reward function estimates the behaviors of the entire team,
including scoring, and carrying the ball to the opponentâs restricted area et al., but not the individual
behaviors such as ball-passing (Li et al., 2021). We implement all the algorithms for the 5-vs-5 sce-
nario as both parameter sharing and parameter-independent. The additional results with algorithms
implemented as parameter sharing are shown in Fig. 11, in which A2PO gets free from the trouble
that the controlled agents have similar behavior and compete for the ball (Li et al., 2021).

We implement all the algorithms on the 11-vs-11 scenario as parameter sharing using MALib (Zhou
et al., 2021) for acceleration and train the algorithms for 300M environment steps. We summarize
the learned behaviors observed in the game videos:

â¢ Basic Skills. The agents trained by MAPPO and CoPPO perform unsatisfactorily in basic
skills such as dribbling, shooting, and the agents even run out of bounds frequently. In
contrast, the agents trained by HAPPO and A2PO perform better in the basic skills. We
attribute the problems to the non-stationarity issue that seriously inï¬uences the simultane-
ous updating algorithms. We also note that the agents trained by all the algorithms fail
to understand the off-side mechanism and occasionally gather together on the opponentâs
bottom line.

â¢ Passing and Receiving Coordination. We analyze the direct way for coordination: pass-
ing and receiving the ball. As illustrated in Tab. 3, the agents trained by MAPPO have the
lowest number of successful passes and the lowest successful pass rate, and we can hardly
observe the agents passing the ball. Agents trained by CoPPO perform better on passing
the ball but suffer from poor basic skills, and get tackled after receiving the ball. Agents
trained by HAPPO prefer passing the ball without considering the teammatesâ situations,
e.g., the receiver is marked by several opponents. Agents trained by A2PO can pass the ball
to their teammates in a way that leads to a score. We attribute the performance gain to the
preceding-agent off-policy correction, which means that agents estimate the teammatesâ
situations and intentions better.

We further visualize the learned behaviors of A2PO in Fig. 12. In the top of Fig. 12, two players
cooperatively break through the opponentâs defense and complete a passing and receiving coordi-
nation for scoring. In the bottom of Fig. 12, three players make a fast thrust by two long passes:
the goalkeeper passes the ball to the player at the edge, and the player at the edge passes the ball to
the player behind the opponents. The complex coordination strategies are hardly observed in other
baselines.

B.2.5 ABLATION

Preceding agent off-policy correction. More ablations on preceding-agent off-policy correction
are shown in Fig. 13. The baselines are:

â¢ MAPPO w/ V-trace, CoPPO w/ V-trace: Simultaneous update methods with advantage

estimation as V-trace.

â¢ HAPPO w/ PreOPC: HAPPO with advantage estimation as PreOPC.

28

0123StepsÃ106â45â40â35â30â25â20â15EpisodeReturn3Agents(FullyCooperative)024StepsÃ106â50â40â30EpisodeReturn5Agents(FullyCooperative)0.000.250.500.751.00StepsÃ107â35.0â32.5â30.0â27.5â25.0â22.5â20.0EpisodeReturn3Agents(GeneralSum)0.000.250.500.751.00StepsÃ107â45.0â42.5â40.0â37.5â35.0â32.5â30.0EpisodeReturn5Agents(GeneralSum)MAPPOCoPPOHAPPOA2POPublished as a conference paper at ICLR 2023

Figure 12: Visualization of trained A2PO policies on the Google Research Football 11-vs-11 sce-
nario, which shows that A2PO encourages complex cooperation behaviors to make a goal. Top:
Player Turing and Johnson cooperate to beat multiple opponents to break through the defense and
make a goal. Bottom: The goalkeeper, player Turing, and Curie achieve the pass and receive coop-
eration twice. A fast thrust is made by consecutively passing the ball.

In this ablation study, the baselines are equipped with off-policy correction methods. The experiment
yields the following three conclusions:

â¢ The results ï¬rstly support the conclusion in Sec. 3.3 that applying PreOPC to sequential
update methods results in a greater performance improvement than applying V-trace to
simultaneous update methods.

â¢ Secondly, the primary distinction between A2PO and HAPPO with PreOPC is the clipping
objective. The results demonstrate that the clipping objective derived from the single-agent
improvement bound contributes to the performance improvement.

â¢ And thirdly, although we were unable to assess the error of PreOPC, we compare A2PO
with RPISA-PPO, which can be viewed as A2PO algorithms with error-free off-policy
correction methods (the advantage estimation is error-free) at the expense of sample inefï¬-
ciency. A2PO reaches or outperforms the asymptotic performance of RPISA-PPO. A2PO
outperforms RPISA-PPO since RPISA-PPO suffers from performance degradation as a re-
sult of agents updating policies with separated data (Taheri & Thrampoulidis, 2022).

We further analyze the sensitivity to the hyper-parameter Î». Results in Fig. 14 illustrate that
preceding-agent off-policy correction does not introduce more sensitivity.

Figure 13: Ablation experiments on preceding-agent off-policy correction.

Agent Selection Rule. More ablations on the agent selection rules are shown in Fig. 13. We com-
pare two additional rules: âReverse-greedyâ and âReverse-semi-greedyâ. âReverseâ means selecting
the agent with the minimal advantage ï¬rst. While we observe that the effect of the selection rule
becomes less signiï¬cant in tasks with homogeneous or symmetric agents.

Going deeper into the effects of agent selection rules, we show that the agents with implicit guid-
ance from the advantage estimation beneï¬t from greedily selecting agents in Fig. 16 and 17. More

29

The player Turing beat an opponent withthe player Johnson marking.Turing beat another opponent.Johnson prepares to take the passfrom Turing.Turing passes the ball to Johnson,then Johnson receives the passand thrusts to shoot.Johnson breaks through the opponent'sgoalkeeper, shoots, and makes a goal.The goalkeeper makes a goal kick andplays a long pass to the player near thesideline.The player Turing receives the pass from thegoalkeeper, then dribbles and passes the ballto the player in the midfield.The player Curie receives the passfrom Turing, then plays a fast break.Curie shoots and makes a goal.location of the playerMotion of the playerMotion of the ball12341234History location of the player0.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnRPISA-PPOAsymptoticPerformanceHumanoid-v29|80.000.250.500.751.00StepsÃ1071000200030004000500060007000EpisodeReturnRPISA-PPOAsymptoticPerformanceAnt-v24x20.000.250.500.751.00StepsÃ1070.00.20.40.60.81.0WinRateRPISA-PPOAsymptoticPerformanceMMM20.000.250.500.751.00StepsÃ1070.00.20.40.60.81.0WinRateRPISA-PPOAsymptoticPerformance3s5zMAPPOMAPPOw/V-traceCoPPOCoPPOw/V-traceHAPPOHAPPOw/PreOPCA2POw/oPreOPCA2POPublished as a conference paper at ICLR 2023

Figure 14: Sensitivity analysis of Î».

30

0.00.20.40.60.81.0StepsÃ1070200040006000EpisodeReturnMAPPO0.00.20.40.60.81.0StepsÃ1070200040006000EpisodeReturnMAPPOw/MA-A-trace0.00.20.40.60.81.0StepsÃ1070200040006000EpisodeReturnA2POw/oMA-A-trace0.00.20.40.60.81.0StepsÃ1070200040006000EpisodeReturnA2POHumanoid-v29â8Î»=0.90Î»=0.93Î»=0.95Î»=0.97Î»=0.990.00.20.40.60.81.0StepsÃ107200040006000EpisodeReturnMAPPO0.00.20.40.60.81.0StepsÃ107200040006000EpisodeReturnMAPPOw/MA-A-trace0.00.20.40.60.81.0StepsÃ107200040006000EpisodeReturnA2POw/oMA-A-trace0.00.20.40.60.81.0StepsÃ107200040006000EpisodeReturnA2POAnt-v24x2Î»=0.90Î»=0.93Î»=0.95Î»=0.97Î»=0.990.00.20.40.60.81.0StepsÃ1070.000.250.500.751.00WinRateMAPPO0.00.20.40.60.81.0StepsÃ1070.000.250.500.751.00WinRateMAPPOw/MA-A-trace0.00.20.40.60.81.0StepsÃ1070.000.250.500.751.00WinRateA2POw/oMA-A-trace0.00.20.40.60.81.0StepsÃ1070.000.250.500.751.00WinRateA2PO3s5zÎ»=0.90Î»=0.93Î»=0.95Î»=0.97Î»=0.990.00.20.40.60.81.0StepsÃ1070.00.51.0WinRateMAPPO0.00.20.40.60.81.0StepsÃ1070.00.51.0WinRateMAPPOw/MA-A-trace0.00.20.40.60.81.0StepsÃ1070.00.51.0WinRateA2POw/oMA-A-trace0.00.20.40.60.81.0StepsÃ1070.00.51.0WinRateA2POMMM2Î»=0.90Î»=0.93Î»=0.95Î»=0.97Î»=0.99Published as a conference paper at ICLR 2023

Figure 15: Ablation experiments on the agent selection rules. Left: Heterogeneous or asymmetric
agents. Right: Homogeneous or symmetric agents.

even bars appear in one ï¬g means the agents are more balanced in terms of the guidance from the
advantage estimation. Take the agent 10 in Fig. 16 for example, under âCyclicâ and âRandomâ rules,
agent 10 perform the worst with high proportions, while it has higher proportions in prior ranks
under âGreedyâ rule.

(a) The imbalance of the agents. The bar of agent i illustrates the proportion of
its ranks in terms of E
|]. Especially, agent 10 has implicit guidance,
i.e., a small absolute value of advantage function when using Cyclic and Ran-
dom selection rule, but is comparable with other agents with Greedy selection
rule.

s,ai [|AÏ, ËÏi

(b) The coefï¬cient of vari-
ance of agent 10âs order
proportions.

Figure 16: Agentsâ imbalance in terms of the estimated advantage. The experiment is conducted on
the MMM2 task of SMAC.

Adaptive Clipping Parameter. More ablations on the adaptive clipping parameter are shown in
Fig. 18. Similarly, we observe that the effect of the adaptive clipping parameter becomes less sig-
niï¬cant in tasks with homogeneous or symmetric agents.

Figure 17: More experiments on the agentsâ imbalance in terms of the estimated advantage. Top:
3s5z task. Bottom: MMM2 task.

31

0.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnHumanoid-v29|80.000.250.500.751.00StepsÃ1070.00.20.40.60.81.0WinRateMMM20.000.250.500.751.00StepsÃ107100020003000400050006000EpisodeReturnAnt-v24x2024StepsÃ1060.00.20.40.60.81.0WinRate8mvs9mHAPPO-CyclicHAPPO-RandomHAPPO-GreedyHAPPO-Semi-greedyHAPPO-Reverse-greedyHAPPO-Reverse-semi-greedyA2PO-CyclicA2PO-RandomA2PO-GreedyA2PO-Semi-greedyA2PO-Reverse-greedyA2PO-Reverse-semi-greedy12345678910Agent0.00.20.40.60.81.0ProportionCyclic12345678910Agent0.00.20.40.60.81.0ProportionRandom12345678910Agent0.00.20.40.60.81.0ProportionGreedyOrder1Order2Order3Order4Order5Order6Order7Order8Order9Order10CyclicRandomGreedySelectionStrategy0.000.050.100.150.200.250.30Coeï¬cientofVariationAgent1012345678Agent0.00.20.40.60.81.0ProportionCyclic12345678Agent0.00.20.40.60.81.0ProportionRandom12345678Agent0.00.20.40.60.81.0ProportionGreedy12345678Agent0.00.20.40.60.81.0ProportionSemi-greedy12345678Agent0.00.20.40.60.81.0ProportionReverse-greedy12345678Agent0.00.20.40.60.81.0ProportionReverse-semi-greedyOrder1Order2Order3Order4Order5Order6Order7Order812345678910Agent0.00.20.40.60.81.0ProportionCyclic12345678910Agent0.00.20.40.60.81.0ProportionRandom12345678910Agent0.00.20.40.60.81.0ProportionGreedy12345678910Agent0.00.20.40.60.81.0ProportionSemi-greedy12345678910Agent0.00.20.40.60.81.0ProportionReverse-greedy12345678910Agent0.00.20.40.60.81.0ProportionReverse-semi-greedyOrder1Order2Order3Order4Order5Order6Order7Order8Order9Order10Published as a conference paper at ICLR 2023

Figure 18: More ablation experiments on the adaptive clipping parameter. Left: Heterogeneous or
asymmetric agents. Right: Homogeneous or symmetric agents.

(a) Comparison on Humanoid 9|8 over both environment steps and
training time.

(b) Comparison on GRF 11-vs-11
scenario.

Figure 19: Wall time Analysis.

B.3 WALL TIME ANALYSIS

Multiple updates in a stage may increase training time, and the need for more training time may im-
pact the scalability of A2PO, which is a common concern regarding the sequential update scheme.
Nevertheless, a sequential update scheme will increase training time less than might be expected.
Before proceeding, we note that the majority of experiments in our work are synchronously imple-
mented, and the training time consists of the time spent updating policies and collecting samples.

We have proposed a simple yet effective method for controlling training time in order to reduce
training time. As a trade-off between performance and training time, we divide the agents into blocks
to reduce the number of update iterations. For example, the tasks with 10 agents can be divided into
3 blocks, with sizes 3, 3, 4, respectively, and only 3 updates will be performed in a policy update
iteration. From the implementation perspective, since the number of samples used in a single update
decreases, the sequential update scheme requires less memory and less updating time when update
policies. Therefore, it is possible to control the training time as less than 1.5 times the training time
of the simultaneous update methods. In addition, assuming a good implementation, fewer update
iterations will be performed if mini-batches are used in a single policy update, as the size of a
mini-batch can be greater in sequential update methods under limited memory resources. In such
a case, fewer mini-batches will be used, further decreasing the training time. Moreover, sampling
consumes the majority of the training time, and the increased updating time appears less signiï¬cant
when analysing the wall time for on-policy algorithms with synchronized implementations.

The training time is depicted in Tab. 6. A2PO achieves signiï¬cantly greater performance with only
marginally more training time. In addition, we illustrate the Humanoid 9
8 comparisons regarding
|
environment steps and training time in Fig. 19a, and the comparisons on the GRF 11-vs-11 scenario
in Fig. 19b. A2PO maintains an advantage in terms of training time.

B.4 HYPER-PARAMETERS

We tune several hyper-parameters in all the benchmarks, other hyper-parameters refer to the settings
used in MAPPO. c(cid:15) are selected to be 0.5 in all the tasks.

32

0.000.250.500.751.00StepsÃ1070200040006000EpisodeReturnHumanoid-v29|80.000.250.500.751.00StepsÃ1070.00.20.40.60.81.0WinRateMMM20.000.250.500.751.00StepsÃ107100020003000400050006000EpisodeReturnAnt-v24x2024StepsÃ1060.00.20.40.60.81.0WinRate8mvs9mMAPPOMAPPOw/AdaptCoPPOCoPPOw/AdaptHAPPOHAPPOw/AdaptA2POw/oAdaptA2PO0.000.250.500.751.00StepsÃ1070100020003000400050006000EpisodeReturnHumanoid-v29|8050100150Minutes0100020003000400050006000EpisodeReturnHumanoid-v29|8MAPPOw/oPSCoPPOw/oPSHAPPOw/oPSA2POw/oPS0.02.55.07.510.0Hours0.20.40.60.8WinRateFootball11vs11MAPPOCoPPOHAPPOA2POPublished as a conference paper at ICLR 2023

Table 6: The comparison of training duration. The format of the ï¬rst line in a cell is: Training
time(Sampling time+Updating Time). The second line of a cell represents the time normalized.

Task

3s5z

MAPPO

CoPPO

HAPPO

A2PO

3h29m(3h3m+0h26m)
1.00(0.87 + 0.13)

3h33m(3h6m+0h27m)
1.02(0.89 + 0.13)

3h49m(3h7m+0h42m)
1.10(0.89 + 0.20)

4h32m(3h41m+0h51m)
1.30(1.06 + 0.25)

27m vs 30m

13h23m(8h31m + 4h52m)
1.00(0.64 + 0.36)

13h19m(8h24m + 4h55m)
1.00(0.63 + 0.37)

16h2m(8h20m + 7h42m)
1.20(0.62 + 0.58)

15h53m(8h7m + 7h46m)
1.19(0.61 + 0.58)

8
Humanoid 9
|

Ant 4x2

2h0m(1h45m + 0h15m)
1.00(0.87 + 0.13)

1h58m(1h43m + 0h15m)
0.99(0.86 + 0.13)

2h15m(1h45m + 0h30m)
1.12(0.87 + 0.25)

2h31m(2h0m + 0h31m)
1.26(1.00 + 0.26)

6h42m(6h16m + 0h26m)
1.00(0.93 + 0.07)

6h45m(6h19m + 0h26m)
1.01(0.94 + 0.07)

7h29m(6h5m + 1h24m)
1.12(0.91 + 0.21)

7h2m(5h34m + 1h28m)
1.05(0.83 + 0.22)

Humanoid 17x1

12h9m(10h6m + 2h3m)
1.00(0.83 + 0.17)

17h7m(15h5m + 2h2m)
1.41(1.24 + 0.17)

16h55m(11h2m + 5h53m)
1.39(0.91 + 0.48)

19h25m(11h59m + 7h26m)
1.60(0.99 + 0.61)

Football 5vs5

34h46m(32h47m + 1h59m) 32h46m(30h49m + 1h57m) 39h26m(31h54m + 7h32m) 37h26m(30h2m + 7h24m)
1.00(0.94 + 0.06)

1.08(0.86 + 0.21)

0.94(0.89 + 0.06)

1.13(0.92 + 0.22)

B.4.1 STARCRAFTII MULTI-AGENT CHALLENGE

We list the hyper-parameters used for each task of SMAC in Tab. 7.

Table 7: Hyper-parameters in SMAC.

Hyperparameters agent block ppo epoch Î»

(cid:15)

MMM
3s vs 5z
2c vs 64zg
3s5z
5m vs 6m
8m vs 9m
10m vs 11m
6h vs 8z
3s5z vs 3s6z
MMM2
27m vs 30m
corridor

3
3
2
3
2
5
2
2
2
2
3
2

12
15
5
8
10
15
10
8
5
5
5
5

0.95 0.2
0.95 0.05
0.95 0.2
0.95 0.2
0.93 0.05
0.95 0.05
0.97 0.2
0.99 0.2
0.90 0.2
0.95 0.2
0.95 0.2
0.95 0.2

B.4.2 MULTI-AGENT MUJOCO

For the model structure in MA MuJoCo, the output from the last layer is processed by a Tanh layer
and the action distribution is modeled as a Gaussian distribution initialized with mean as 0 and log
std as -0.5. The probability output of different actions are averaged when computing the policy ratio.
The common hyper-parameters used in MA MuJoCo are listed in Tab. 8.

Table 8: Common hypermeters in MA MuJoCo.

Hyperparameters Values

entropy
gain
batch size

0
0.01
4000

B.4.3 MULTI-AGENT PARTICLE ENVIRONMENT

We list the hyper-parameters used in MPE in Tab. 10.

B.4.4 GOOGLE RESEARCH FOOTBALL

We list the hyper-parameters used in the GRF 5-vs-5 scenario in Tab. 11.

33

Published as a conference paper at ICLR 2023

Table 9: Hypermeters for the scenarios in MA MuJoCo.

Hyperparameters Ant

HalfCheetah Hopper Humanoid HumanoidStandup Walker2d

agent block
ppo epoch
actor lr
critic lr
Î»
(cid:15)

8x1:4 /
5
8
3e-4
3e-4
3e-4
3e-4
0.93
0.93
0.2
0.2

/
8
1e-4
1e-4
0.95
0.1

17x1:5
5
3e-4
3e-4
0.9
0.2

17x1:4
5
3e-4
3e-4
0.93
0.2

/
5
3e-4
3e-4
0.93
0.2

Table 10: Hypermeters for the scenarios in MPE.

Hyperparameters Values

ppo epoch
chunk length
entropy
actor lr
critic lr
Î»
(cid:15)

8
5
0.05
2e-4
2e-4
0.97
0.2

C THE RELATED WORK OF OTHER MARL METHODS

Value decomposition methods. The value decomposition methods such as VDN (Sunehag et al.,
2017) and Qmix (Rashid et al., 2018), factorize the joint value function and adopt the central-
ized training and decentralized execution paradigm. The Individual-Global-MAX (IGM) prin-
ciple is proposed to ensure consistency between the joint and local greedy action selections in
the joint Q-value function Qtot(Ï , a) and the individual Q-value function

â
, arg maxaâA Qtot(Ï , a) = (arg maxa1âA1 Q1(Ï 1, a1), . . . , arg maxanâAn Qn(Ï n, an)). Two
T
sufï¬cient conditions, the additivity and the monotonicity, to satisfy IGM are proposed in Sunehag
et al. (2017) and Rashid et al. (2018) respectively. In addition to the V function and Q function
decomposition, QPLEX (Wang et al., 2021) considers implementing IGM in the dueling structure
where Q = V + A. QPLEX only constrains the advantage functions to satisfy the IGM prin-
ciple. The global advantage function is decomposed as Atot(Ï , a) = (cid:80)n
i=1 Î»i(Ï , a)Ai(Ï , ai),
where Î»i(Ï , a) > 0. We evaluate the performance of Qmix in Tab. 2 and Tab. 5.
Integrat-
ing the IGM principle into A2PO without compromising the monotonic improvement guarantee
is a desirable extension. Speciï¬cally, the advantage-based IGM establishes a connection between
the global advantage function and the local advantage functions, and the advantage decomposition
Atot(Ï , a) = (cid:80)n
i=1 Î»i(Ï , a)Ai(Ï , ai) will not jeopardize the derivation of the monotonic improve-
ment guarantee.

Qi(Ï i, ai
{

n
i=1:

â

Ï

}

Convergence and optimality of MARL. T-PPO (Ye et al., 2022) ï¬rstly introduce a framework
called Generalized Multi-Agent Actor-Critic with Policy Factorization (GPF-MAC), which consists
of methods with factorized local policies and may become stuck in sub-optimality. To address this
problems, T-PPO transforms a multi-agent MDP into a special âsingle-agentâ MDP with a sequen-
tial structure. T-PPO transforms a multi-agent MDP into a âsingle-agentâ MDP with a sequential
structure to address this issue. T-PPO has been shown to produce an optimal policy if implemented
properly. Theoretically, sequential update methods, such as A2PO and HAPPO, are also instances
of GPF-MAC and may be stuck into sub-optimal policies. The main differences between A2PO and
T-PPO include that A2PO updates the factorized policies sequentially and makes decisions simul-
taneously, while T-PPO makes decisions sequentially, and that A2PO does not introduce the virtual
state and the sequential transformation framework network. And theoretically, T-PPO may compro-
mise the monotonic improvement guarantee. In Tab. 12, we compare A2PO, MAPPO and T-PPO
on SMAC tasks empirically. A2PO is superior to T-PPO in the majority of tasks.

34

Published as a conference paper at ICLR 2023

Table 11: Hypermeters for the scenarios in MPE.

Hyperparameters Values

ppo epoch
chunk length
entropy
actor lr
critic lr
Î»
(cid:15)
Î³

10
10
0.001
5e-4
5e-4
0.95
0.25
0.995

Table 12: Comparisons of A2PO, MAPPO and T-PPO.

Map

Difï¬culty MAPPO w/ PS T-PPO w/ PS A2PO w/ PS

100(0.0)
Easy
Super Hard 90.6(8.9)

1c3s5z
MMM2
3s5z vs 3s6z Super Hard 82.8(19.2)
6h vs 8z
corridor

Super Hard 87.5(1.5)
Super Hard 99.1(0.3)

99.8((0.0)
81.6(7.7)
85.5(5.2)
91.8(1.1)
96.9(0.0)

100(0.0)
98.4(1.3)
93.8(19.8)
90.6(1.3)
100(0.0)

D THE RELATED WORK OF COORDINATE DESCENT

Realizing the similarity between the sequential policy update scheme and the block coordinate de-
scent algorithms, we borrow the optimization techniques in the coordinate descent algorithms to
accelerate the optimization and amplify the convergence advantage over the simultaneous update
scheme (Gordon & Tibshirani, 2015; Shi et al., 2017). One of the critical questions in the coordi-
nate descent algorithms is selecting the coordinate for the next-step optimization. Glasmachers &
Dogan (2013); Lu et al. (2018) provided analyses of the convergence rate advantage of the Gauss-
Southwell rule, i.e., greedily selecting the coordinate with the maximal gradient, over the random
selection rule. We recognize the optimization of our surrogate objective (Schulman et al., 2017)
agent-by-agent as a block coordinate descent problem. Therefore the agent selection rule plays a
crucial role in accelerating the optimization. Inspired by the coordinate selection rules, we propose
greedy and semi-greedy agent selection rules and empirically show that the underperforming agents
beneï¬t from the greedily selecting agents.

35

