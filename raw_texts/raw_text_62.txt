0
2
0
2

r
p
A
3
1

]

G
L
.
s
c
[

3
v
0
2
1
0
0
.
0
1
9
1
:
v
i
X
r
a

September 2019 (Revised April 2020)

Multiagent Rollout Algorithms and

Reinforcement Learning

Dimitri Bertsekasâ 

Abstract

We consider ï¬nite and inï¬nite horizon dynamic programming problems, where the control at each stage

consists of several distinct decisions, each one made by one of several agents. We introduce an approach,

whereby at every stage, each agentâs decision is made by executing a local rollout algorithm that uses a base
policy, together with some coordinating information from the other agents. The amount of local computation

required at every stage by each agent is independent of the number of agents, while the amount of total
computation (over all agents) grows linearly with the number of agents. By contrast, with the standard

rollout algorithm, the amount of total computation grows exponentially with the number of agents. De-
spite the drastic reduction in required computation, we show that our algorithm has the fundamental cost

improvement property of rollout: an improved performance relative to the base policy. We also discuss pos-
sibilities to improve further the methodâs computational eï¬ciency through limited agent coordination and

parallelization of the agentsâ computations. Finally, we explore related approximate policy iteration algo-
rithms for inï¬nite horizon problems, and we prove that the cost improvement property steers the algorithm

towards convergence to an agent-by-agent optimal policy.

1. MULTIAGENT PROBLEM FORMULATION - FINITE HORIZON PROBLEMS

We consider a standard form of an N -stage dynamic programming (DP) problem (see [Ber17], [Ber19]),

which involves the discrete-time dynamic system

xk+1 = fk(xk, uk, wk),

k = 0, 1, . . . , N â 1,

(1.1)

where xk is an element of some (possibly inï¬nite) state space, the control uk is an element of some ï¬nite

control space, and wk is a random disturbance, which is characterized by a probability distribution Pk(Â· |

xk, uk) that may depend explicitly on xk and uk, but not on values of prior disturbances wkâ1, . . . , w0. The

control uk is constrained to take values in a given subset Uk(xk), which depends on the current state xk.

The cost of the kth stage is denoted by gk(xk, uk, wk); see Fig. 1.1.

We consider policies of the form

Ï = {Âµ0, . . . , ÂµN â1},

â  McAfee Professor of Engineering, MIT, Cambridge, MA, and Fulton Professor of Computational Decision

Making, ASU, Tempe, AZ.

1

 
 
 
 
 
 
Control uk
Random Transition

Future Stages Terminal Cost

Future Stages Terminal Cost gN (xN )

...

+1 xN

) x0

...

Random Transition xk+1 = fk(xk, uk, wk) Random cost

) xk

) Random Cost
) Random Cost gk(xk, uk, wk)

xk+1

Stage k

k Future Stages

Figure 1.1 Illustration of the N -stage stochastic optimal control problem. Starting from state

xk, the next state under control uk is generated according to a system equation

xk+1 = fk(xk, uk, wk),

where wk is the random disturbance, and a random stage cost gk(xk, uk, wk) is incurred.

where Âµk maps states xk into controls uk = Âµk(xk), and satisï¬es a control constraint of the form Âµk(xk) â

Uk(xk) for all xk. Given an initial state x0 and a policy Ï = {Âµ0, . . . , ÂµN â1}, the expected cost of Ï starting

at x0 is

JÏ(x0) = E

(

N â1

gN (xN ) +

gk

xk, Âµk(xk), wk

k=0
X

(cid:0)

,

)
(cid:1)

where the expected value operation E{Â·} is over all the random variables wk and xk. The optimal cost is

the function J * of the initial state x0, deï¬ned by

J *(x0) = min
ÏâÎ 

JÏ(x0),

where Î  is the set of all policies, while an optimal policy Ïâ is one that attains the minimal cost for every

initial state; i.e.,

JÏâ(x0) = min
ÏâÎ 

JÏ(x0).

Since J * and Ïâ are typically hard to obtain by exact DP, we consider reinforcement learning (RL) algorithms

for suboptimal solution, and focus on rollout, which we describe next.

1.1. The Standard Rollout Algorithm

The aim of rollout is policy improvement.

In particular, given a policy Ï = {Âµ0, . . . , ÂµN â1}, called base

policy, with cost-to-go from state xk at stage k denoted by Jk,Ï(xk), k = 0, . . . , N , we wish to use rollout

to obtain an improved policy, i.e., one that achieves cost that is at most Jk,Ï(xk) starting from each xk.

The standard rollout algorithm provides on-line control of the system as follows (see the textbooks [BeT96],

[Ber17], [Ber19]):

2

Standard One-Step Lookahead Rollout Algorithm:

Start with the initial state x0, and proceed forward generating a trajectory

{x0, Ëu0, x1, Ëu1, . . . , xN â1, ËuN â1, xN }

according to the system equation (1.1), by applying at each state xk a control Ëuk selected by the

one-step lookahead minimization

Ëuk â arg min

ukâUk(xk)

E

gk(xk, uk, wk) + Jk+1,Ï

fk(xk, uk, wk)

(1.2)

n

(cid:0)

.
(cid:1)o

The one-step minimization (1.2), which uses Jk+1,Ï in place of the optimal cost-to-go function, deï¬nes

a policy ËÏ = {ËÂµ0, . . . , ËÂµN â1}, where for all xk and k, ËÂµk(xk) is equal to the control Ëuk obtained from Eq.

(1.2). This policy is referred to as the rollout policy. The fundamental cost improvement result here is that

the rollout policy improves over the base policy in the sense that

Jk,ËÏ(xk) â¤ Jk,Ï(xk),

â xk, k,

(1.3)

where Jk,ËÏ(xk), k = 0, . . . , N , is the cost-to-go of the rollout policy starting from state xk ([Ber17], Section

6.4, or [Ber19], Section 2.4.2).

The expected value in Eq. (1.2) is the Q-factor of the pair (xk, uk) corresponding to the base policy:

Qk,Ï(xk, uk) = E

gk(xk, uk, wk) + Jk+1,Ï

fk(xk, uk, wk)

.

In the âstandardâ implementation of rollout, at each encountered state xk, the Q-factor Qk,Ï(xk, uk) is

n

(cid:0)

(cid:1)o

computed by some algorithm separately for each control uk â Uk(xk) (often by Monte Carlo simulation).

Unfortunately, in the multiagent context to be discussed shortly, the number of controls in Uk(xk), and the

attendant computation of Q-factors, grow rapidly with the number of agents, and can become very large.

The purpose of this paper is to introduce a modiï¬ed rollout algorithm for the multiagent case, which requires

much less computation while maintaining the cost improvement property (1.3).

1.2. The Multiagent Case

Let us assume a special structure of the control space, corresponding to a multiagent version of the problem.â 

In particular, we assume that the control uk consists of m components u1

k, . . . , um
k ,

â  While we focus on multiagent problems, our methodology applies to any problem where the control uk consists

uk = (u1

k, . . . , um

k ),

of m components, uk = (u1

k, . . . , um

k ).

3

with the component uâ

k, â = 1, . . . , m, chosen by agent â at stage k, from within a given set U â

k(xk). Thus

the control constraint set is the Cartesian productâ 

Uk(xk) = U 1

k (xk) Ã Â· Â· Â· Ã U m

k (xk).

(1.4)

Then the minimization (1.2) involves as many as sm Q-factors, where s is the maximum number of elements

of the sets U i

k(xk) [so that sm is an upper bound to the number of controls in Uk(xk), in view of its Cartesian
product structure (1.4)]. Thus the computation required by the rollout algorithm is of order O(sm) per stage.

In this paper we propose an alternative rollout algorithm that achieves the cost improvement property

(1.3) at much smaller computational cost, namely of order O(sm) per stage. A key idea here is that the

computational requirements of the rollout one-step minimization (1.2) are proportional to the number of

controls in the set Uk(xk) and are independent of the size of the state space. This motivates a reformulation

of the problem, ï¬rst suggested in the neuro-dynamic programming book [BeT96], Section 6.1.4, whereby

control space complexity is traded oï¬ with state space complexity by âunfoldingâ the control uk into its m

components, which are applied one agent-at-a-time rather than all-agents-at-once. We discuss this idea next

within the multiagent context.

1.3. Trading oï¬ Control Space Complexity with State Space Complexity

We noted that a major issue in rollout is the minimization over uk â Uk(xk) in Eq. (1.2), which may be very

time-consuming when the size of the control constraint set is large. In particular, in the multiagent case

where uk = (u1

k ), the time to perform this minimization is typically exponential in m. In this case,
we can reformulate the problem by breaking down the collective decision uk into m individual component

k, . . . , um

decisions, thereby reducing the complexity of the control space while increasing the complexity of the state

space. The potential advantage is that the extra state space complexity does not aï¬ect the computational

requirements of some RL algorithms, including rollout.

To this end, we introduce a modiï¬ed but equivalent problem, involving one-agent-at-a-time control

selection. At the generic state xk, we break down the control uk into the sequence of the m controls
k, u2
u1
mediate âstatesâ (xk, u1

k , and between xk and the next state xk+1 = fk(xk, uk, wk), we introduce artiï¬cial inter-
), and corresponding transitions. The choice

k, . . . , um

k, . . . , umâ1

k

k), (xk, u1

of the last control component um

) marks the transition to the next state

k, u2
k), . . . , (xk, u1
k at âstateâ (xk, u1

k, . . . , umâ1

k

xk+1 = fk(xk, uk, wk) according to the system equation, while incurring cost gk(xk, uk, wk); see Fig. 1.2.

â  The Cartesian product structure of the constraint set is adopted here for simplicity of exposition, particularly

when arguing about computational complexity. The idea of trading oï¬ control space complexity and state space

complexity (cf. Section 1.3), on which this paper rests, does not depend on a Cartesian product constraint structure.

Of course when this structure is present, it simpliï¬es the computations of the methods of this paper.

4

1 Control um
k

Random Transition

, u1
k

) xk

, u2
k

xk, u1
k

m
k u3
k

k xk, u1
1

k, u2
k

Random Transition xk+1 = fk(xk, uk, wk) Random cost
k umâ1
3
...
k2
k xk, u1

k, . . . , umâ1

xk+1

k

) Random Cost
) Random Cost gk(xk, uk, wk)

Stage k

Figure 1.2 Equivalent formulation of the N -stage stochastic optimal control problem for the
case where the control uk consists of m components u1

k, u2

k, . . . , um
k :

uk = (u1

k, . . . , um

k ) â U 1

k (xk) Ã Â· Â· Â· Ã U m

k (xk).

The ï¬gure depicts the kth stage transitions. Starting from state xk, we generate the intermediate
k, . . . , umâ1
), using the respective controls u1
k), . . . , (xk, u1
states (xk, u1
.
The ï¬nal control um leads from (xk, u1
) to xk+1 = fk(xk, uk, wk), and a random stage
cost gk(xk, uk, wk) is incurred.

k, . . . , umâ1
k, . . . , umâ1

k), (xk, u1

k, u2

k

k

k

It is evident that this reformulated problem is equivalent to the original, since any control choice that

is possible in one problem is also possible in the other problem, while the cost structure of the two problems

is the same. In particular, every policy

(Âµ1

k, . . . , Âµm

k ) | k = 0, . . . , N â 1

Ï =

(cid:8)

(cid:9)

of the original problem, including a base policy in the context of rollout, is admissible for the reformulated

problem, and has the same cost function for the original as well as the reformulated problem.â 

â  It would superï¬cially appear that the reformulated problem contains more policies than the original prob-

lem, because its form is more general:

in the reformulated problem the policy at the kth stage applies the control

components

k(xk), Âµ2
Âµ1

k(xk, u1

k), . . . , Âµm

k (xk, u1

k, . . . , umâ1

k

),

where u1

k = Âµ1

k(xk) and for â = 2, . . . , m, uâ

k is deï¬ned sequentially as

uâ
k = Âµâ

k(xk, u1

k, . . . , uââ1

k

).

Still, however, this policy is equivalent to the policy of the original problem that applies the control components

where for â = 2, . . . , m, ËÂµâ

k(xk) is deï¬ned sequentially as

k(xk), ËÂµ2
Âµ1

k(xk), . . . , ËÂµm

k (xk),

ËÂµâ
k(xk) = Âµâ
k

xk, Âµ1

k(xk), ËÂµ2

k(xk), . . . , ËÂµââ1

k

(cid:0)

5

(xk)

.

(cid:1)

The motivation for the reformulated problem is that the control space is simpliï¬ed at the expense

of introducing m â 1 additional layers of states, and corresponding m â 1 cost-to-go functions J 1

k (xk, u1
J 2
of the state space does not adversely aï¬ect the operation of rollout, since the Q-factor minimization (1.2)

(xk, u1

k),
), in addition to Jk(xk). On the other hand, the increase in size

k), . . . , J mâ1

k, . . . , umâ1

k (xk, u1

k, u2

k

k

is performed for just one state at each stage. Moreover, in a diï¬erent context, the increase in size of the

state space can be dealt with by using function approximation, i.e., with the introduction of cost-to-go

approximations

ËJ 1
k (xk, u1

k, r1

k), ËJ 2

k (xk, u1

k, u2

k, r2

k), . . . , ËJ mâ1

k

(xk, u1

k, . . . , umâ1

k

, rmâ1
k

),

in addition to ËJk(xk, rk), where rk, r1

k, . . . , rmâ1

k

are parameters of corresponding approximation architectures

(such as feature-based architectures and neural networks).

2. MULTIAGENT ROLLOUT

Consider now the standard rollout algorithm applied to the reformulated problem shown in Fig. 1.2, with

k, . . . , Âµm

k ), with each Âµâ

a given base policy Ï = {Âµ0, . . . , ÂµN â1}, which is also a policy of the original problem [so that Âµk =
(Âµ1
k, â = 1, . . . , m, being a function of just xk]. The algorithm generates a rollout
policy ËÏ = {ËÂµ0, . . . , ËÂµN â1}, where for each stage k, ËÂµk consists of m components ËÂµâ
k ),
and is obtained for all xk according to

k, i.e., ËÂµk = (ËÂµ1

k, . . . , ËÂµm

ËÂµ1
k(xk) â arg min
âU 1
k

u1
k

(xk )

ËÂµ2
k(xk) â arg min
âU 2
k

u2
k

(xk )

E

gk

n

E

gk

n

(cid:0)

(cid:0)

xk, ËÂµ1

k(xk), u2

k, . . . , Âµm

k (xk), wk

Â· Â· Â·

Â· Â· Â·

(cid:1)

(cid:1)

xk, u1

k, Âµ2

k(xk), . . . , Âµm

k (xk), wk

+ Jk+1,Ï

fk

xk, u1

k, Âµ2

k(xk), . . . , Âµm

k (xk), wk

(cid:16)

(cid:0)

+ Jk+1,Ï

fk

xk, ËÂµ1

k(xk), u2

k, . . . , Âµm

k (xk), wk

ËÂµm
k (xk) â arg

min
âUm
k

(xk)

um
k

E

gk

xk, ËÂµ1

k(xk), ËÂµ2

k(xk), . . . , um

k , wk

xk, ËÂµ1

k(xk), ËÂµ2

k(xk), . . . , um

k , wk

n

(cid:0)

(cid:1)

(cid:16)

(cid:0)

(cid:16)

(cid:0)

Â· Â· Â·

+ Jk+1,Ï

fk

,

(cid:1)(cid:17)o

,

(cid:1)(cid:17)o

.

(cid:1)(cid:17)o

(2.1)

via a sequence of m minimizations, once over each of the agent controls u1

Thus, when applied on-line, at xk, the algorithm generates the control ËÂµk(xk) =
k, . . . , um

k(xk), . . . , ËÂµm
ËÂµ1
k (xk)
k , with the past controls
(cid:1)
determined by the rollout policy, and the future controls determined by the base policy; cf. Eq. (2.1). Assuming

(cid:0)

a maximum of s elements in the constraint sets U i

k(xk), the computation required at each stage k is of order

O(n) for each of the âstatesâ xk, (xk, u1

k), . . . , (xk, u1

k, . . . , umâ1

k

), for a total of order O(sm) computation.

In the âstandardâ implementation of the algorithm, at each (xk, u1

k, . . . , uââ1

k

) with â â¤ m, and for

each of the controls uâ

k, we generate by simulation a number of system trajectories up to stage N , with all
future controls determined by the base policy. We average the costs of these trajectories, thereby obtaining

6

the Q-factor corresponding to (xk, u1

minimal Q-factor, with the controls u1

k, . . . , uââ1
k, . . . , uââ1

k

k

, uâ

k). We then select the control uâ
held ï¬xed at the values computed earlier.

k that corresponds to the

Prerequisite assumptions for the preceding algorithm to work in an on-line multiagent setting are:

(a) All agents have access to the current state xk.

(b) There is an order in which agents compute and apply their local controls.

(c) There is intercommunication between agents, so agent â knows the local controls u1

k, . . . , uââ1

k

computed

by the predecessor agents 1, . . . , â â 1 in the given order.

Note that the rollout policy (2.1), obtained from the reformulated problem is diï¬erent from the rollout

policy obtained from the original problem [cf. Eq. (1.2)]. Generally, it is unclear how the two rollout policies

perform relative to each other in terms of attained cost. On the other hand, both rollout policies perform no

worse than the base policy, since the performance of the base policy is identical for both the reformulated

problem and for the original problem. This is shown formally in the following proposition.

Proposition 2.1: Let Ï be a base policy and let ËÏ be a corresponding rollout policy generated by

the multiagent rollout algorithm (2.1). We have

Jk,ËÏ(xk) â¤ Jk,Ï(xk),

for all xk and k.

(2.2)

Proof: We will show Eq. (2.2) by induction, and for simplicity, we will give the proof for the case of just

two agents, i.e., m = 2. Clearly Eq. (2.2) holds for k = N , since JN,ËÏ = JN,Ï = gN . Assuming that it holds

for index k + 1, i.e., Jk+1,ËÏ â¤ Jk+1,Ï, we have for all xk,

Jk,ËÏ(xk) = E

gk

â¤ E

n

gk

xk, ËÂµ1

k(xk), ËÂµ2

k(xk), wk

(cid:0)
xk, ËÂµ1

k(xk), ËÂµ2

k(xk), wk

+ Jk+1,ËÏ

fk

(cid:1)

+ Jk+1,Ï

(cid:16)

fk

xk, ËÂµ1

k(xk), ËÂµ2

k(xk), wk

(cid:0)
xk, ËÂµ1

k(xk), ËÂµ2

k(xk), wk

(cid:1)(cid:17)o

E

gk(xk, ËÂµ1

(cid:0)
(cid:1)
k(xk), u2
k, wk) + Jk+1,Ï

(cid:16)

fk

xk, ËÂµ1

k(xk), u2

k, wk

(cid:1)(cid:17)o

n
k(xk), Âµ2
xk, ËÂµ1

k(xk), wk

+ Jk+1,Ï

fk

(cid:16)
(cid:0)
k(xk), Âµ2
xk, ËÂµ1

k(xk), wk

(cid:1)(cid:17)o

(2.3)

E

gk(xk, u1

k, Âµ2

(cid:0)
k(xk), wk) + Jk+1,Ï

(cid:1)

(cid:16)

fk

xk, u1

k, Âµ2

k(xk), wk

(cid:1)(cid:17)o

n
(cid:0)
= min
âU 2
u2
k (xk)
k

â¤ E

gk

n
(cid:0)
= min
âU 1
u1
k (xk)
k

â¤ E

gk

n
k(xk), Âµ2
xk, Âµ1

k(xk), wk

+ Jk+1,Ï

fk

(cid:16)
(cid:0)
k(xk), Âµ2
xk, Âµ1

k(xk), wk

(cid:1)(cid:17)o

(cid:16)

(cid:0)

(cid:1)(cid:17)o

(cid:1)

7

n
= Jk,Ï(xk),

(cid:0)

where in the preceding relation:

(a) The ï¬rst equality is the DP equation for the rollout policy ËÏ.

(b) The ï¬rst inequality holds by the induction hypothesis.

(c) The second equality holds by the deï¬nition of the rollout algorithm as it pertains to agent 2.

(d) The third equality holds by the deï¬nition of the rollout algorithm as it pertains to agent 1.

(e) The last equality is the DP equation for the base policy Ï.

The induction proof of the cost improvement property (2.2) is thus complete for the case m = 2. The proof

for an arbitrary number of agents m is entirely similar. Q.E.D.

Note the diï¬erence in the proof argument between the all-agents-at-once and one-agent-at-a-time rollout

algorithms. In the former algorithm, the second equality in Eq. (2.3) would be over both u1

k (xk) and
k â U 2
u2
k (xk), and the second inequality and third equality would be eliminated. Still the proof of the cost
improvement property (2.2) goes through in both cases. Note also that if the base policy were optimal, Eq.

k â U 1

(2.3) would hold as an equality throughout for both rollout algorithms, while the rollout policy ËÏ would also

be optimal.

On the other hand, there is an important situation where the all-agents-at-once rollout algorithm can

improve the base policy but the one-agent-at-a-time algorithm will not. This possibility may arise when

the base policy is âagent-by-agent-optimal,â i.e., each agentâs control component is optimal, assuming that

the control components of all other agents are kept ï¬xed at some known values.â  Such a policy may not

be optimal, except under special conditions. Thus if the base policy is agent-by-agent-optimal, multiagent

rollout will be unable to improve strictly the cost function, even if this base policy is strictly suboptimal.

However, we speculate that a situation where a base policy is agent-by-agent-optimal is unlikely to occur

in rollout practice, since ordinarily a base policy must be reasonably simple, readily available, and easily

simulated.

Let us also note that the qualitative diï¬erence between all-agents-at-once versus one-agent-at-a-time

rollout is reminiscent of the context of value iteration (VI) algorithms, which involve minimization of a

Bellman equation-like expression over the control constraint. In such algorithms one may choose between

â  This is a concept that has received much attention in the theory of team optimization, where it is known

as person-by-person optimality. It has been studied in the context of somewhat diï¬erent problems, which involve

imperfect state information that may not be shared by all the agents; see Marschak [Mar55], Radner [Rad62],

Witsenhausen [Wit71], Ho [Ho80]. For more recent works, see Nayyar, Mahajan, and Teneketzis [NMT13], Nayyar

and Teneketzis [NaT19], Li et al. [LTZ19], the book by Zoppoli, Parisini, Baglietto, and Sanguineti [ZPB19], and the

references quoted there.

8

Gauss-Seidel methods, where the cost of a single state (and the control at that state) is updated at a time,

while taking into account the results of earlier state cost computations, and Jacobi methods, where the cost

of all states is updated at once. The tradeoï¬ between Gauss-Seidel and Jacobi methods is well-known in

the VI context: generally, Gauss-Seidel methods are faster, while Jacobi methods are also valid, as well as

better suited for distributed asynchronous implementation; see [BeT89], [Ber12]. Our context in this paper is

quite diï¬erent, however, since we are considering updates of agent controls, and not cost updates at diï¬erent

states.

Let us provide an example that illustrates how the size of the control space may become intractable

for even moderate values of the number of agents m.

Example 2.1 (Spiders and Fly)

Here there are m spiders and one ï¬y moving on a 2-dimensional grid. During each time period the ï¬y moves to

a some other position according to a given state-dependent probability distribution. The spiders, working as

a team, aim to catch the ï¬y at minimum cost. Each spider learns the current state (the vector of spiders and

ï¬y locations) at the beginning of each time period, and either moves to a neighboring location or stays where

it is. Thus each spider i has as many as ï¬ve choices at each time period (with each move possibly incurring
a diï¬erent location-dependent cost). The control vector is u = (u1, . . . , um), where ui is the choice of the ith
spider, so there are about 5m possible values of u. However, if we view this as a multiagent problem, as per

the reformulation of Fig. 1.2, the size of the control space is reduced to â¤ 5 moves per spider.

To apply multiagent rollout, we need a base policy. A simple possibility is to use the policy that directs

each spider to move on the path of minimum distance to the current ï¬y position. According to the multiagent

rollout formalism, the spiders choose their moves in a given order, taking into account the current state, and

assuming that future moves will be chosen according to the base policy. This is a tractable computation,

particularly if the rollout with the base policy is truncated after some stage, and the cost of the remaining

stages is approximated using a certainty equivalence approximation in order to reduce the cost of the Monte

Carlo simulation. The problem can be made more complicated by introducing terrain obstacles, travel costs,

or multiple ï¬ies.

Sample computations with this example indicate that the multiagent rollout algorithm of this section

performs about as well as the standard rollout algorithm. Both algorithms perform much better than the

base policy, and exhibit some âintelligenceâ that the base policy does not possess. In particular, in the rollout

algorithms the spiders attempt to âencircleâ the ï¬y for faster capture, rather that moving straight towards the

ï¬y along a shortest path.

The following example is similar to the preceding one, but involves two ï¬ies and two spiders moving

along a line, and admits an exact analytical solution. It illustrates how the multiagent rollout policy may

exhibit intelligence and agent coordination that is totally lacking from the base policy. The behavior described

in the example has been supported by computational experiments with larger two-dimensional problems of

9

Figure 2.1 Illustration of the 2-dimensional spiders-and-ï¬y problem. The state is the
pair of distances between spiders and ï¬y. At each time period, each spider moves to a
neighboring location or stays where it is. The spiders make moves with perfect knowledge

of the locations or each other and of the ï¬y. The ï¬y moves randomly, regardless of the

position of the spiders.

the type described in the preceding example.

Example 2.2 (Spiders and Flies)

This is a spiders-and-ï¬ies problem that admits an analytical solution. There are two spiders and two ï¬ies

moving along integer locations on a straight line. For simplicity we will assume that the ï¬iesâ positions are

ï¬xed at some integer locations, although the problem is qualitatively similar when the ï¬ies move randomly. The

spiders have the option of moving either left or right by one unit; see Fig. 2.2. The objective is to minimize the

time to capture both ï¬ies. The problem has essentially a ï¬nite horizon since the spiders can force the capture

of the ï¬ies within a known number of steps.

Here the optimal policy is to move the two spiders towards diï¬erent ï¬ies, the ones that are initially closest

to them (with ties broken arbitrarily). The minimal time to capture is the maximum of the two initial distances

of the two optimal spider-ï¬y pairings.

Let us apply multiagent rollout with the base policy that directs each spider to move one unit towards

the closest ï¬y position (and in case of a tie, move towards the ï¬y that lies to the right). The base policy is

poor because it may unnecessarily move both spiders in the same direction, when in fact only one is needed

to capture the ï¬y. This limitation is due to the lack of coordination between the spiders: each acts selï¬shly,

ignoring the presence of the other. We will see that rollout restores a signiï¬cant degree of coordination between

the spiders through an optimization that takes into account the long-term consequences of the spider moves.

According to the multiagent rollout mechanism, the spiders choose their moves one-at-a-time, optimizing

10

Spider 1 Spider 2 Fly 1 Fly 2

Spider 1 Spider 2 Fly 1 Fly 2

Spider 1 Spider 2 Fly 1 Fly 2

Spider 1 Spider 2 Fly 1 Fly 2

+ 1, . . . , k

+ 1, . . . , k

+ 1, . . . , k

Spider 1 Spider 2 Fly 1 Fly 2 n â 1

1 n n

n n + 1

Figure 2.2 Illustration of the two-spiders and two-ï¬ies problem. The spiders move along

integer points of a line. The two ï¬ies stay still at some integer locations. The optimal policy

is to move the two spiders towards diï¬erent ï¬ies, the ones that are initially closest to them.

The base policy directs each spider to move one unit towards the nearest ï¬y position.

Multiagent rollout with the given base policy starts with spider 1 at location n,
and calculates the two Q-factors that correspond to moving to locations n â 1 and n + 1,
assuming that the remaining moves of the two spiders will be made using the go-towards-
the-nearest-ï¬y base policy. The Q-factor of going to n â 1 is smallest because it saves
in unnecessary moves of spider 1 towards ï¬y 2, so spider 1 will move towards ï¬y 1. The

trajectory generated by multiagent rollout is to move spiders 1 and 2 towards ï¬ies 1 and

2, respectively, then spider 2 ï¬rst captures ï¬y 2, and then spider 1 captures ï¬y 1. Thus
multiagent rollout generates the optimal policy.

over the two Q-factors corresponding to the right and left moves, while assuming that future moves will be

chosen according to the base policy. Let us consider a stage, where the two ï¬ies are alive while the spiders

are at diï¬erent locations as in Fig. 2.2. Then the rollout algorithm will start with spider 1 and calculate two

Q-factors corresponding to the right and left moves, while using the base heuristic to obtain the next move of

spider 2, and the remaining moves of the two spiders. Depending on the values of the two Q-factors, spider 1

will move to the right or to the left, and it can be seen that it will choose to move away from spider 2 even if

doing so increases its distance to its closest ï¬y contrary to what the base heuristic will do; see Fig. 2.2. Then

spider 2 will act similarly and the process will continue. Intuitively, spider 1 moves away from spider 2 and ï¬y

2, because it recognizes that spider 2 will capture earlier ï¬y 2, so it might as well move towards the other ï¬y.

Thus the multiagent rollout algorithm induces implicit move coordination, i.e., each spider moves in a

way that takes into account future moves of the other spider. In fact it can be veriï¬ed that the algorithm

will produce an optimal sequence of moves starting from any initial state. It can also be seen that ordinary

rollout (both ï¬ies move at once) will also produce an optimal move sequence. Moreover, the example admits a

two-dimensional generalization, whereby the two spiders, starting from the same position, will separate under

the rollout policy, with each moving towards a diï¬erent spider, while they will move in unison in the base policy

whereby they move along the shortest path to the closest surviving ï¬y. Again this will typically happen for

both standard and multiagent rollout.

The preceding example illustrates how a poor base policy can produce a much better rollout policy,

something that can be observed in many other problems. Intuitively, the key fact is that rollout is âfarsightedâ

in the sense in can beneï¬t from control calculations that reach far into future stages.

11

3. ROLLOUT VARIANTS FOR FINITE HORIZON PROBLEMS

It is worth noting a few variants of the rollout algorithm for the reformulated ï¬nite horizon problem of Fig.

1.2.

(a) Instead of selecting the agent controls in a ï¬xed order, it is possible to change the order at each stage

k (the preceding cost improvement proof goes through again by induction). In fact it is possible to

optimize over multiple orders at the same stage, or to base the order selection on various features of

the state x (for instance in the case of Example 2.1, with multiple spiders and ï¬ies, giving priority to

the spiders âclosestâ to some ï¬y may make sense).

(b) We can use at each stage k, a base policy {Âµ1

state xk, but also on the preceding controls, i.e., consists of functions Âµâ

k } that selects controls that depend not just on the
k, . . . , uââ1
k of the form Âµâ
).
This can exploit intuition into the problemâs structure, but from a theoretical viewpoint, it is not more

k(xk, u1

k, . . . , Âµm

k

general. The reason is that policies where control component selections uâ

k depend on the previously
in addition to the current state xk, can be equivalently rep-

selected control components u1

k, . . . , uââ1

k

resented by policies where the selection of uâ

k depends on just xk.

(c) The algorithm can be applied to a partial state information problem (POMDP), after it has been trans-

formed to a perfect state information problem, using a belief state formulation, where the conditional

probability distribution of the state given the available information plays the role of xk (note here

that we have allowed the state space to be inï¬nite, thereby making our methodology applicable to the

POMDP/belief state formulation).

(d) We may use rollout variants involving multistep lookahead, truncated rollout, and terminal cost function

approximation, in the manner described in the RL book [Ber19]. Of course, in such variants the cost

improvement property need not hold strictly, but it holds within error bounds, some of which are given

in [Ber19], Section 5.1, for the inï¬nite horizon discounted problem case.

We may also consider multiagent rollout algorithms that are asynchronous in the sense that the agents

may compute their rollout controls in parallel or in some irregular order rather than in sequence, and they

may also communicate these controls asynchronously with some delays. Algorithms of this type are discussed

in generality in the book [BeT89], and also in the papers [BeY10], [BeY12], [YuB13], within the present DP

context [see also the books [Ber12] (Section 2.6), and [Ber18] (Section 2.6)]. An example of such an algorithm

is obtained when at a given stage, agent â computes the rollout control Ëuâ
k(xk), . . . , Âµââ1

of some of the agents 1, . . . , â â 1, and uses the controls Âµ1

k

k before knowing the rollout controls
(xk) of the base policy in their place.

While such an algorithm is likely to work well for many problems, it may not possess the cost improvement

property. In fact we can construct a simple example involving a single state, two agents, and two controls

12

per agent, where the 2nd agent does not take into account the control applied by the 1st agent, and as a

result the rollout policy performs worse than the base policy.

Example 3.1 (Cost Deterioration in the Absence of Adequate Agent Coordination)

Consider a problem with two agents (m = 2) and a single state. Thus the state does not change and the costs

of diï¬erent stages are decoupled (the problem is essentially static). Each of the two agents has two controls:
k â {0, 1} and u2
u1
equal to 2 if u1

k â {0, 1}. The cost per stage gk is equal to 0 if u1
k = 1. Suppose that the base policy applies u1

k = 0. Then it can be seen that when

k, is equal to 1 if u1

k 6= u2
k = u2

k = 0, and is

k = u2

k = u2

executing rollout, the ï¬rst agent applies u1

k = 1, and in the absence of knowledge of this choice, the second

agent also applies u2

k = 1 (thinking that the ï¬rst agent will use the base policy control u1

k = 0). Thus the cost

of the rollout policy is 2 per stage, while the cost of the base policy is 1 per stage. By contrast the rollout

algorithm that takes into account the ï¬rst agentâs control when selecting the second agentâs control applies
k = 1 and u2
u1

k = 0, thus resulting in a rollout policy with the optimal cost of 0 per stage.

The diï¬culty here is inadequate coordination between the two agents.

In particular, each agent uses

rollout to compute the local control, each thinking that the other will use the base policy control. If instead

the two agents were to coordinate their control choices, they would have applied an optimal policy.

The simplicity of the preceding example also raises serious questions as to whether the cost improvement

property (2.2) can be easily maintained by a distributed rollout algorithm where the agents do not know

the controls applied by the preceding agents in the given order of local control selection, and use instead

the controls of the base policy. Still, however, such an algorithm is computationally attractive in view of its

potential for eï¬cient distributed implementation, and may be worth considering in a practical setting. A

noteworthy property of this algorithm is that if the base policy is optimal, the same is true of the rollout

policy. This suggests that if the base policy is nearly optimal, the same is true of the rollout policy.

One may also speculate that if agents are naturally âweakly coupledâ in the sense that their choice of

control has little impact in the desirability of various controls of other agents, then a more ï¬exible inter-agent

communication pattern may be suï¬cient for cost improvement.â  A computational comparison of various

multiagent rollout algorithms with ï¬exible communication patterns may shed some light on this question.

The key question is whether and under what circumstances agent coordination is essential, i.e., there is a

signiï¬cant performance loss when the computations of diï¬erent agents are done to some extent concurrently

â  In particular, one may divide the agents in âcoupledâ groups, and require coordination of control selection only

within each group, while the computation of diï¬erent groups may proceed in parallel. For example, in applications

where the agentsâ locations are distributed within some geographical area, it may make sense to form agent groups

on the basis of geographic proximity, i.e., one may require that agents that are geographically near each other (and

hence are more coupled) coordinate their control selections, while agents that are geographically far apart (and hence

are less coupled) forego any coordination.

13

rather than sequentially with intermediate information exchange.

4. MULTIAGENT PROBLEM FORMULATION - INFINITE HORIZON DISCOUNTED

PROBLEMS

The multiagent rollout ideas that we have discussed so far can be modiï¬ed and generalized to apply to

inï¬nite horizon problems. In this context, we may consider multiagent versions of value iteration (VI for

short) and policy iteration (PI for short) algorithms. We will focus on discounted problems with ï¬nite

number of states and controls, so that the Bellman operator is a contraction mapping, and the strongest

version of the available theory applies (the solution of Bellmanâs equation is unique, and strong convergence

results hold for VI and PI methods); see [Ber12], Chapters 1 and 2, and [Ber18], Chapter 2. However, a

qualitatively similar methodology can be applied to undiscounted problems involving a termination state

(e.g., stochastic shortest path problems, see [BeT96], Chapter 2, [Ber12], Chapter 3, and [Ber18], Chapters

3 and 4).

In particular, we consider a standard Markovian decision problem (MDP for short) inï¬nite horizon

discounted version of the ï¬nite horizon m-agent problem of Section 1.2, where m > 1. The control u consists

of m components uâ, â = 1, . . . , m,

u = (u1, . . . , um),

(for the MDP notation adopted for this section, we switch for convenience to subscript indexing for control

components, and reserve superscript indexing for policy iterates). At state x and stage k, a control u is

applied, and the system transitions to a next state y with transition probabilities pxy(y) and cost g(x, u, y).

When at stage k the transition cost is discounted by Î±k, where Î± â (0, 1) is the discount factor. Each control

component uâ is separately constrained to lie in a given ï¬nite set Uâ(x) when the system is at state x. Thus

the control constraint is u â U (x), where U (x) is the ï¬nite Cartesian product set

U (x) = U1(x) Ã Â· Â· Â· Ã Um(x).

The cost function of a stationary policy Âµ that applies control Âµ(x) â U (x) at state x is denoted by JÂµ(x),

and the optimal cost [the minimum over Âµ of JÂµ(x)] is denoted J *(x).

An equivalent version of the problem, involving a reformulated/expanded state space is depicted in

Fig. 4.1 for the case m = 3. The state space of the reformulated problem consists of

x, (x, u1), . . . , (x, u1, . . . , umâ1),

where x ranges over the original state space, and each uâ, â = 1, . . . , m, ranges over the corresponding

constraint set Uâ(x). At each stage, the agents choose their controls sequentially in a ï¬xed order: from state

x agent 1 applies u1 â U1(x) to go to state (x, u1), then agent 2 applies u2 â U2(x) to go to state (x, u1, u2),

14

Agent 1 Agent 2 Agent 3

1 x, u1, u2

2 u3

1 x, u1, u2

1 x, u1, u2

1 x, u1, u2

Cost 0 Cost g(x, u, w)
Control constraint:

1 u2

3 x, u1
Agent 1 Agent 2 Agent 3

3 Cost 0 Cost

3 x, u1

3 x, u1

3 x, u1

u1

Agent 1 Agent 2 Agent 3

3 Cost 0 Cost

x x, u

x x, u

x x, u

x x, u

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Base Heuristic Minimization Possible Path Reformulated State Space

Figure 4.1 Illustration of how to transform an m-agent inï¬nite horizon problem into a stationary
inï¬nite horizon problem with fewer control choices available at each state (in this ï¬gure m = 3).
At the typical stage and state x, the ï¬rst agent chooses u1 at no cost leading to state (x, u1).
Then the second agent applies u2 at no cost leading to state (x, u1, u2). Finally, the third agent
applies u3 leading to some state y at cost g(x, u, y), where u is the combined control of the three
agents, u = (u1, u2, u3). The ï¬gure shows the ï¬rst three transitions of the trajectories that start
from the states x, (x, u1), and (x, u1, u2), respectively.

and so on, until ï¬nally at state (x, u1, . . . , umâ1), agent m applies um â Um(x), completing the choice of

control u = (u1, . . . , um), and eï¬ecting the transition to state y at a cost g(x, u, y), appropriately discounted.

Note that this reformulation involves the type of tradeoï¬ between control space complexity and state

space complexity that we discussed in Section 1.3. The reformulated problem involves m cost-to-go functions

J 0(x), J 1(x, u1), . . . , J mâ1(x, u1, . . . , umâ1),

(4.1)

with corresponding sets of Bellman equations, but a much smaller control space. Moreover, the existing

analysis of rollout algorithms, including implementations, variations, and error bounds, applies to the refor-

mulated problem; see Section 5.1 of the authorâs RL textbook [Ber19]. Similar to the ï¬nite horizon case, the

implementation of the rollout algorithm involves one-agent-at-a-time policy improvement, and is much more

economical for the reformulated problem, while maintaining the basic cost improvement and error bound

properties of rollout, as they apply to the reformulated problem.

4.1. Agent-by-Agent Policy Iteration for Inï¬nite Horizon Discounted Problems

A PI algorithm generates a sequence of policies {Ïk}, and can be viewed as repeated or perpetual rollout,

i.e., Ïk+1 is the rollout policy obtained when Ïk is the base policy. For the reformulated problem just

described, the policy evaluation step of the PI algorithm requires the calculation of m cost-to-go functions

of the form (4.1), so the policy evaluation must be done over a much larger space than the original state

space. Moreover, the policies generated by this algorithm are also deï¬ned over a larger space and have the

15

form

Âµ0(x), Âµ1(x, u1), . . . , Âµmâ1(x, u1, . . . , umâ1).

(4.2)

Motivated by this fact, we may consider a multiagent PI algorithm that operates over the simpler class of

policies of the form

Âµ(x) =

Âµ0(x), Âµ1(x), . . . , Âµmâ1(x)

,

(4.3)

i.e., the policies for the original inï¬nite horizon problem.

(cid:0)

(cid:1)

We assume n states, denoted by 1, . . . , n, and we introduce the Bellman operator T , which maps a

vector J =

J(1), . . . , J(n)

to the vector T J =

(T J)(1), . . . , (T J)(n)

according to

(cid:0)

(cid:1)

(T J)(x) = min
uâU(x)

n

(cid:0)
pxy(u)

g(x, u, y) + Î±J(y)

,

x = 1, . . . , n,

(4.4)

(cid:1)

y=1
X

(cid:0)

(cid:1)

and for each policy Âµ, the corresponding Bellman operator TÂµ deï¬ned by

n

(TÂµJ)(x) =

pxy

Âµ(x)

y=1
X

(cid:0)

g
(cid:1)(cid:16)

x, Âµ(x), y

+ Î±J(y)

,

x = 1, . . . , n.

(4.5)

(cid:0)

(cid:1)

(cid:17)

It is well known that T and TÂµ are contraction mappings of modulus Î± with respect to the sup norm, and

their unique ï¬xed points are J * and JÂµ, respectively, i.e., J * = T J * and JÂµ = TÂµJÂµ.

In the preceding expressions, we will often expand u to write it in terms of its components. In particular,

we may write pxy(u1, . . . , um) and g(x, u1, . . . , um, y) in place of pxy(u) and g(x, u, y), respectively. Similarly,

we will denote the components of a policy Âµ as Âµ1, . . . , Âµm.

The Standard Policy Iteration Algorithm

For each policy Âµ, we introduce the subset of policies

M(Âµ) = {ËÂµ | TËÂµJÂµ = T JÂµ}.

(4.6)

Equivalently, we have ËÂµ â M(Âµ) if ËÂµ is obtained from Âµ by using the standard policy improvement operation,

ËÂµ(x) â arg min
uâU(x)

n

y=1
X

pxy(u)

g(x, u, y) + Î±JÂµ(y)

,

x = 1, . . . , n,

(4.7)

(cid:0)

(cid:1)

or

where QÂµ(x, u) is the Q-factor of the state-control pair (x, u) corresponding to Âµ, given by

ËÂµ(x) â arg min
uâU(x)

QÂµ(x, u),

x = 1, . . . , n,

n

QÂµ(x, u) =

pxy(u)

g(x, u, y) + Î±JÂµ(y)

.

y=1
X

(cid:0)

16

(cid:1)

(4.8)

(4.9)

The standard form of PI generates a sequence {Âµk} of policies, starting from a given policy Âµ0 (see e.g.,

[Ber12]). Given the current policy Âµk, it generates a new policy from the set of âimprovedâ policies M(Âµk)

of Eq. (4.6):

Âµk+1 â M(Âµk).

(4.10)

Thus the kth iteration of the standard PI algorithm can be separated into two phases:

(a) Policy evaluation, which computes JÂµk .

(b) Policy improvement , which computes a new policy Âµk+1 â M(Âµk) by the minimization over u â U (x)

of the Q-factor QÂµk (x, u); cf. Eq. (4.7)-(4.8).

The Multiagent Policy Iteration Algorithm

Our proposed one-agent-at-a-time PI algorithm uses a modiï¬ed form of policy improvement, whereby the

control u = (u1, . . . , um) is optimized one-component-at-a-time, similar to Section 2. In particular, given the

current policy Âµk, the next policy is obtained as

where for a given Âµ, we denote by

M(Âµ) the set of policies ËÂµ = (ËÂµ1, . . . , ËÂµm) satisfying for all x = 1, . . . , n,

f

Âµk+1 â

M(Âµk),

(4.11)

ËÂµ1(x) â arg min

u1âU1(x)

n

n

f

pxy

u1, Âµ2(x), . . . , Âµm(x)

g

x, u1, Âµ2(x), . . . , Âµm(x), y

+ Î±JÂµ(y)

,

y=1
X

(cid:0)

(cid:1)(cid:16)

(cid:0)

(cid:1)

(cid:17)

ËÂµ2(x) â arg min

u2âU2(x)

pxy

ËÂµ1(x), u2, Âµ3(x), . . . , Âµm(x)

g

x, ËÂµ1(x), u2, Âµ3(x), . . . , Âµm(x), y

+ Î±JÂµ(y)

,

y=1
X

(cid:0)

n

Â· Â· Â·

Â· Â· Â·

(cid:1)(cid:16)

(cid:0)

Â· Â· Â·

(cid:1)

(cid:17)

ËÂµm(x) â arg min

pxy

ËÂµ1(x), . . . , ËÂµmâ1(x), um

g

x, ËÂµ1(x), . . . , ËÂµmâ1(x), um, y

+ Î±JÂµ(y)

. (4.12)

umâUm(x)

y=1
X

(cid:1)(cid:16)
Note that each of the m minimizations (4.12) can be performed for each state x independently, i.e.,

(cid:17)

(cid:0)

(cid:1)

(cid:0)

the computations for state x do not depend on the computations for other states, thus allowing the use

of parallel computation over the diï¬erent states. On the other hand, the computations corresponding to

individual components must be performed in sequence (in the absence of special structure related to coupling

of the control components through the transition probabilities and the cost per stage). It will also be clear

from the subsequent analysis that the ordering of the components may change from one policy improvement

operation to the next.

Similar to the ï¬nite horizon case of Sections 2 and 3, the salient feature of the one-agent-at-a-time policy

improvement operation (4.12) is that it is far more economical than the standard policy improvement:

it

17

requires a sequence of m minimizations, once over each of the control components u1, ..., um. In particular, for

the minimization over the typical component uâ, the preceding components u1, . . . , uââ1 have been computed

earlier by the minimization that yielded the policy components ËÂµ1, . . . , ËÂµââ1, while the following controls

uâ+1, . . . , um are determined by the current policy components Âµâ+1, . . . , Âµm. Thus, if the number of controls

within each component constraint set Uâ(x) is bounded by a number s, the modiï¬ed policy improvement

phase requires at most smn calculations of a Q-factor of the generic form (4.9). By contrast, since the number

of elements in the constraint set U (x) is bounded by sm, the corresponding number of Q-factor calculations

in the standard policy improvement is bounded by smn. Thus the one-agent-at-a-time policy improvement

where the number of Q-factors grows linearly with m, as opposed to the standard policy improvement, where

the number of Q-factor calculations grows exponentially with m.

We say that a policy Âµ = {Âµ1, . . . , Âµm} is agent-by-agent optimal if it satisï¬es Âµ â

M(Âµ), or equivalently,

for all x = 1, . . . , n, and â = 1, . . . , m, we have

n

pxy

Âµ1(x), . . . , Âµm(x)

g

x, Âµ1(x), . . . , Âµm(x), y

+ Î±JÂµ(y)

f

y=1
X

(cid:0)

(cid:1)(cid:16)

(cid:0)

= min

uââUâ(x)

n

y=1
X

(cid:1)

(cid:17)

pxy

Âµ1(x), . . . , Âµââ1(x), uâ, Âµâ+1(y), . . . , Âµm(y)

(cid:0)

Â·

g

Âµ1(y), . . . , Âµââ1(y), uâ, Âµâ+1(y), . . . , Âµm(y)

+ Î±JÂµ(y)

.

(cid:1)

(cid:16)

(cid:0)

(cid:1)

(4.13)
(cid:17)

To interpret this deï¬nition, let a policy Âµ = {Âµ1, . . . , Âµm} be given, and consider for every â = 1, . . . , m the
single agent DP problem where for all ââ² 6= â the ââ²th policy components are ï¬xed at Âµââ², while the âth policy

component is subject to optimization. We view the agent-by-agent optimality deï¬nition as the optimality

condition for all the single agent problems [Eq. (4.13) can be written as TÂµ,âJÂµ = TâJÂµ, where Tâ and TÂµ,â

are the Bellman operators (4.4) and (4.5) that correspond to the single agent problem of agent â]. We can

then conclude that Âµ = {Âµ1, . . . , Âµm} is agent-by-agent optimal if each component Âµâ is optimal for the

âth single agent problem, where it is assumed that the remaining policy components remain ï¬xed; in other

words by using Âµâ, each agent â acts optimally, assuming all other agents ââ² 6= â use the corresponding policy

components Âµââ².

In the terminology of team theory such a policy may also be called âperson-by-person

optimal.â

Note that an (overall) optimal policy is agent-by-agent optimal, but the reverse is not true as the

following example shows. This is well-known from the aforementioned research on team theory; see [Mar55],

[Rad62], [Wit71], [Ho80], [NMT13], [NaT19], [LTZ19], [ZPB19].

Example 4.1 (Counterexample for Agent-by-Agent Optimality)

Consider an inï¬nite horizon problem, which involves two agents (m = 2) and a single state x. Thus the state

does not change and the costs of diï¬erent stages are decoupled (the problem is essentially static). Each of the

18

two agents has two controls: u1 â {0, 1} and u2 â {0, 1}. The cost per stage g is equal to 2 if u1 6= u2, is

equal to 1 if u1 = u2 = 0, and is equal to 0 if u1 = u2 = 1. The unique optimal policy is to apply Âµ1(x) = 1

and Âµ2(x) = 1. However, it can be seen that the suboptimal policy that applies Âµ1(x) = 0 and Âµ2(x) = 0 is

agent-by-agent optimal.

The preceding example is representative of an entire class of DP problems where an agent-by-agent

optimal policy is not overall optimal. Any static multivariable optimization problem where there are nonop-

timal solutions that cannot be improved upon by coordinate descent can be turned into an inï¬nite horizon

DP example where these nonoptimal solutions deï¬ne agent-by-agent optimal policies that are not overall op-

timal. Conversely, one may search for problem classes where an agent-by-agent optimal policy is guaranteed

to be (overall) optimal among the type of multivariable optimization problems where coordinate descent is

guaranteed to converge to an optimal solution; for example positive deï¬nite quadratic problems or prob-

lems involving diï¬erentiable strictly convex functions (see [Ber16], Section 3.7). Generally, agent-by-agent

optimality may be viewed as an acceptable form of optimality for many types of problems.

Our main result is that the agent-by-agent PI algorithm just described converges to an agent-by-agent

optimal policy in a ï¬nite number of iterations. For the proof, we use a special rule for breaking ties in the

policy improvement operation in favor of the current policy component. This rule is easy to enforce, and

guarantees that the algorithm cannot cycle between policies. Without this tie-breaking rule, the following

proof can be modiï¬ed to show that while the generated policies may cycle, the corresponding cost function

values converge to the cost function value of some agent-by-agent optimal policy.

Proposition 4.1: (PI Convergence to an Agent-by-Agent Optimal Policy) Let {Âµk} be

a sequence generated by the agent-by-agent PI algorithm (4.11) assuming that ties in the policy

improvement operation of Eq. (4.12) are broken as follows: If for any â = 1, . . . , m and x = 1, . . . , n,

the control component Âµâ(x) attains the minimum in Eq. (4.12), we choose ËÂµâ(x) = Âµâ(x) [even if

there are other control components within Uâ(x) that attain the minimum in addition to Âµâ(x)]. Then

for all x and k, we have

JÂµk+1 (x) â¤ JÂµk (x),

and after a ï¬nite number of iterations, we have Âµk+1 = Âµk, in which case the policies Âµk+1 and Âµk are

agent-by-agent optimal.

Proof:

In the following proof and later all vector inequalities are meant to be componentwise, i.e., for any

two vectors J and J â², we write J â¤ J â² if J(x) = J â²(x) for all x. The critical step of the proof is the following

19

monotone decrease inequality:

TËÂµJ â¤ TÂµJ â¤ J,

for all ËÂµ â

M(Âµ) and J with TÂµJ â¤ J,

(4.14)

which yields as a special case TËÂµJÂµ â¤ JÂµ, since TÂµJÂµ = JÂµ. This parallels a key inequality for standard PI,

f

namely that TËÂµJÂµ â¤ JÂµ, for all ËÂµ â M(Âµ), which lies at the heart of its convergence proof. Once Eq. (4.14)

is shown, the monotonicity of the operator TËÂµ implies the cost improvement property JËÂµ â¤ JÂµ, and by using

the ï¬niteness of the set of policies, the ï¬nite convergence of the algorithm will follow.

We will give the proof of the monotone decrease inequality (4.14) for the case m = 2. The proof for an

arbitrary number of components m > 2 is entirely similar. Indeed, if TÂµJ â¤ J, we have for all x,

n

(TËÂµJ)(x) =

pxy

ËÂµ1(x), ËÂµ2(x)

g

x, ËÂµ1(x), ËÂµ2(x), y

+ Î±J(y)

y=1
X

(cid:0)

n

= min

u2âU2(x)

n

pxy

y=1
X

(cid:0)

(cid:1)(cid:16)
(cid:0)
ËÂµ1(x), u2

â¤

pxy

ËÂµ1(x), Âµ2(x)

g

y=1
X

(cid:0)

n

= min

u1âU1(x)

n

pxy

y=1
X

(cid:0)

(cid:1)(cid:16)
(cid:0)
u1, Âµ2(x)

â¤

pxy

Âµ1(x), Âµ2(x)

g

y=1
X

(cid:0)
= (TÂµJ)(x)

â¤ J(x),

(cid:1)(cid:16)

(cid:0)

(cid:1)
x, ËÂµ1(x), u2, y

g
(cid:1)(cid:16)
x, ËÂµ1(x), Âµ2(x), y

(cid:0)

(cid:1)
x, u1, Âµ2(x), y

g
(cid:1)(cid:16)
x, Âµ1(x), Âµ2(x), y

(cid:0)

(cid:17)

+ Î±J(y)

(cid:17)

(cid:1)

+ Î±J(y)

(cid:17)

(cid:1)

+ Î±J(y)

(cid:1)

(cid:17)

(cid:17)

+ Î±J(y)

(4.15)

where:

(1) The ï¬rst and fourth equalities use the deï¬nition of the Bellman operator TËÂµ.

(2) The second and third equalities hold by the deï¬nition of policies ËÂµ â

M(Âµ).

(3) The ï¬rst and second inequalities are evident.

f

(4) The last inequality is the assumption TÂµJ â¤ J.

By letting J = JÂµk in the monotone decrease inequality (4.14), we have TÂµk+1JÂµk â¤ JÂµk . In view of

the monotonicity of TÂµk+1, we also have T t+1

Âµk+1JÂµk â¤ T t

Âµk+1JÂµk for all t â¥ 1, so that

JÂµk+1 = lim
tââ

T t
Âµk+1 JÂµk â¤ TÂµk+1JÂµk â¤ JÂµk .

It follows that either JÂµk+1 = JÂµk , or else we have strict policy improvement, i.e., JÂµk+1 (x) < JÂµk (x) for
at least one state x. As long as strict improvement occurs, no generated policy can be repeated by the

20

algorithm. Since there are only ï¬nitely many policies, it follows that within a ï¬nite number of iterations,

we will have JÂµk+1 = JÂµk . Once this happens, equality will hold throughout in Eq. (4.15) when Âµ = Âµk,
ËÂµ = Âµk+1, and J = JÂµk . This implies that

n

pxy

Âµk+1
1

(x), Âµk+1

2

(x)

g

x, Âµk+1
1

(x), Âµk+1

2

(x), y

y=1
X

(cid:0)

(cid:1)(cid:16)

(cid:0)

= min

u2âU2(x)

n

=

pxy

and

n

y=1
X

(cid:0)

+ Î±JÂµk (y)
(cid:17)

n

pxy

Âµk+1
1

(cid:1)
(x), u2

g

x, Âµk+1
1

(x), u2, y

y=1
X
Âµk+1
1

(cid:0)
(x), Âµk

2 (x)

g
(cid:1)(cid:16)

(cid:1)(cid:16)
(cid:0)
x, Âµk+1
1

(cid:0)

(x), Âµk

2 (x), y

(cid:1)

+ Î±JÂµk (y)
(cid:17)

(cid:1)
+ Î±JÂµk (y)
(cid:17)

,

pxy

Âµk+1
1

(x), Âµk

2 (x)

g

x, Âµk+1
1

(x), Âµk

2 (x), y

y=1
X

(cid:0)

(cid:1)(cid:16)

(cid:0)

= min

u1âU1(x)

n

=

pxy

n

pxy

(cid:1)
u1, Âµk

2(x)

y=1
X
(cid:0)
1(x), Âµk
Âµk
2 (x)

+ Î±JÂµk (y)
(cid:17)

2(x), y

x, u1, Âµk

g
(cid:1)(cid:16)
(cid:0)
1(x), Âµk
x, Âµk

2 (x), y

+ Î±JÂµk (y)
(cid:17)

(cid:1)
+ Î±JÂµk (y)
(cid:17)

.

g
(cid:1)(cid:16)

(cid:0)
In view of our tie breaking rule, Eq. (4.17) implies that Âµk+1
Âµk+1
2 = Âµk
optimal. Q.E.D.

1, and then Eq. (4.16) implies that
2. Thus we have Âµk+1 = Âµk, and from Eqs. (4.16) and (4.17), Âµk+1 and Âµk are agent-by-agent

= Âµk

(cid:0)

(cid:1)

1

y=1
X

(4.16)

(4.17)

As Example 4.1 shows, there may be multiple agent-by-agent optimal policies, with diï¬erent cost

functions. This illustrates that the policy obtained by the multiagent PI algorithm may depend on the

starting policy. It turns out that the same example can be used to show that the policy obtained by the

algorithm depends also on the order in which the agents select their controls.

Example 4.2 (Dependence of the Final Policy on the Agent Iteration Order)

Consider the problem of Example 4.1.
optimal policy Âµâ where Âµâ
Let the starting policy be Âµ0 where Âµ0

1(x) = 1 and Âµâ

In this problem there are two agent-by-agent optimal policies: the

2(x) = 1, and the suboptimal policy ËÂµ where ËÂµ1(x) = 0 and ËÂµ2(x) = 0.

2(x) = 0. Then if agent 1 iterates ï¬rst, the algorithm will
terminate with the suboptimal policy, Âµ1 = ËÂµ, while if agent 2 iterates ï¬rst, the algorithm will terminate with
the optimal policy, Âµ1 = Âµâ.

1(x) = 1 and Âµ0

Generally, it may not be easy to escape from a suboptimal agent-by-agent optimal policy. This is

similar to trying to escape from a local minimum in multivariable optimization. Of course, one may try

minimization over suitable subsets of control components, selected by some heuristic, possibly randomized,

mechanism. However, such a approach is likely to be problem-dependent, and may not oï¬er meaningful

guarantees of success.

21

We note that the line of proof based on the monotone decrease inequality (4.14) given above can be

used to establish the validity of some variants of agent-by-agent PI. One such variant, which we will not

pursue further, enlarges the set

M(Âµ) to allow approximate minimization over the control components in

Eq. (4.12). In particular, we require that in place of Eq. (4.15), each control ËÂµâ(x), â = 1, . . . , m, satisï¬es

f

n

pxy

ËÂµ1(x), . . . , ËÂµââ1(x), ËÂµâ(x), Âµâ+1(x), . . . , Âµm(x)

y=1
X

(cid:0)

(cid:1)

g

x, ËÂµ1(x), . . . , ËÂµââ1(x), ËÂµâ(x), Âµâ+1(x), . . . , Âµm(x), y

+ Î±JÂµ(y)

n

<

pxy

(cid:0)

(cid:16)
ËÂµ1(x), . . . , ËÂµââ1(x), Âµâ(x), Âµâ+1(x), . . . , Âµm(x)

(cid:1)

(cid:17)

y=1
X

(cid:0)

g

x, ËÂµ1(x), . . . , ËÂµââ1(x), Âµâ(x), Âµâ+1(x), . . . , Âµm(x), y

+ Î±JÂµ(y)

,

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(cid:1)

whenever there exists uâ â Uâ(x) that can strictly reduce the corresponding minimized expression in Eq.

(4.15). It can be seen that even with this approximate type of minimization over control components, the

convergence proof of Prop. 4.1 still goes through.

Another important variant of agent-by-agent PI is an optimistic version, whereby policy evaluation is

performed by using a ï¬nite number of agent-by-agent value iterations. Moreover, there are many possibilities

for approximate agent-by-agent PI versions, including the use of value and policy neural networks.

In

particular, the multiagent policy improvement operation (4.12) may be performed at a sample set of states

xs, s = 1, . . . , q, thus yielding a training set of state-rollout control pairs

xs, ËÂµ(xs)

, s = 1, . . . , q, which can

be used to train a (policy) neural network to generate an approximation ËÂµ to the policy ËÂµ. The policy ËÂµ can

(cid:0)

(cid:1)

be used in turn to train a feature-based architecture or a neural network that approximates its cost function

JËÂµ, and the approximate multiagent PI cycle can be continued. Thus in this scheme, the diï¬culty with a

large control space is mitigated by agent-by-agent policy improvement, while the diï¬culty with a large state

space is overcome by training value and policy networks. A further discussion of this type of approximate

schemes is beyond the scope of the present paper.

Finally, we note that the issues relating to parallelization of the policy improvement (or rollout) step

that we discussed at the end of Section 3 for ï¬nite horizon problems, also apply to inï¬nite horizon problems.

Moreover, the natural partition of the state space illustrated in Fig. 4.1 suggests a distributed implementation

(which may be independent of any parallelization in the policy improvement step). In particular, distributed

asynchronous PI algorithms based on state space partitions are proposed and analyzed in the work of

Bertsekas and Yu [BeY10] [see also [BeY12], [YuB13], and the books [Ber12] (Section 2.6), and [Ber18]

(Section 2.6)]. These algorithms are relevant for distributed implementation of the multiagent PI ideas of

the present paper.

22

5. CONCLUDING REMARKS

We have shown that in the context of multiagent problems, an agent-by-agent version of the rollout algorithm

has greatly reduced computational requirements, while still maintaining the fundamental cost improvement

property of the standard rollout algorithm. There are many variations of rollout algorithms for multiagent

problems, which deserve attention, despite the potential lack of strict cost improvement in the case of a

suboptimal base policy that is agent-by-agent optimal. Computational tests in some practical multiagent

settings will be helpful in comparatively evaluating some of these variations.

We have primarily focused on the cost improvement property, and the practically important fact that

it can be achieved at a much reduced computational cost. However, it is useful to keep in mind that the

agent-by-agent rollout algorithm is simply the standard all-agents-at-once rollout algorithm applied to the

(equivalent) reformulated problem of Fig. 1.2 (or Fig. 4.1 in the inï¬nite horizon case). As a result, all

known insights, results, error bounds, and approximation techniques for standard rollout apply in suitably

reformulated form.

In this paper, we have assumed that the control constraint set is ï¬nite in order to argue about the

computational eï¬ciency of the agent-by-agent rollout algorithm. The rollout algorithm itself and its cost

improvement property are valid even in the case where the control constraint set is inï¬nite, including the

model predictive control context (cf. Section 2.5 of the RL book [Ber19]), and linear-quadratic problems.

However, it may be unclear that agent-by-agent rollout oï¬ers an advantage in the inï¬nite control space case.

We have also discussed an agent-by-agent version of PI for inï¬nite horizon problems, which uses one-

component-at-a-time policy improvement. While this algorithm may terminate with a suboptimal policy

that is agent-by-agent optimal, it may produce comparable performance to the standard PI algorithm,

which however may be computationally intractable even for a moderate number of agents. Moreover, our

multiagent PI convergence result of Prop. 4.1 can be extended beyond the ï¬nite-state discounted context to

more general inï¬nite horizon DP contexts, where the PI algorithm is well-suited for algorithmic solution.

Other extensions include agent-by-agent variants of VI, optimistic PI, and other related methods. The

analysis of such extensions is reported separately; see [Ber20a].

We ï¬nally mention that the idea of agent-by-agent rollout also applies within the context of challenging

deterministic discrete/combinatorial optimization problems, which involve constraints that couple the con-

trols of diï¬erent stages. We discuss the corresponding constrained multiagent rollout algorithms separately

in the paper [Ber20b].

6. REFERENCES

[BeT89] Bertsekas, D. P., and Tsitsiklis, J. N., 1989. Parallel and Distributed Computation: Numerical

Methods, Prentice-Hall, Englewood Cliï¬s, NJ; republished in 1996 by Athena Scientiï¬c, Belmont, MA.

23

[BeT96] Bertsekas, D. P., and Tsitsiklis, J. N., 1996. Neuro-Dynamic Programming, Athena Scientiï¬c, Bel-

mont, MA.

[BeY10] Bertsekas, D. P., and Yu, H., 2010. âAsynchronous Distributed Policy Iteration in Dynamic Pro-

gramming,â Proc. of Allerton Conf. on Communication, Control and Computing, Allerton Park, Ill, pp.

1368-1374.

[BeY12] Bertsekas, D. P., and Yu, H., 2012. âQ-Learning and Enhanced Policy Iteration in Discounted

Dynamic Programming,â Math. of OR, Vol. 37, pp. 66-94.

[Ber12] Bertsekas, D. P., 2012. Dynamic Programming and Optimal Control, Vol. II, 4th edition, Athena

Scientiï¬c, Belmont, MA.

[Ber16] Bertsekas, D. P., 2016. Nonlinear Programming, 3rd edition, Athena Scientiï¬c, Belmont, MA.

[Ber17] Bertsekas, D. P., 2017. Dynamic Programming and Optimal Control, Vol. I, 4th edition, Athena

Scientiï¬c, Belmont, MA.

[Ber18] Bertsekas, D. P., 2018. Abstract Dynamic Programming, Athena Scientiï¬c, Belmont, MA.

[Ber19] Bertsekas, D. P., 2019. Reinforcement Learning and Optimal Control, Athena Scientiï¬c, Belmont,

MA.

[Ber20a] Bertsekas, D. P., 2020. âMultiagent Value Iteration Algorithms in Dynamic Programming and

Reinforcement Learning,â in preparation.

[Ber20b] Bertsekas, D. P., 2020. âConstrained Multiagent Rollout and Multidimensional Assignment with

the Auction Algorithm,â arXiv preprint, arXiv:2002.07407.

[Ho80] Ho, Y. C., 1980. âTeam Decision Theory and Information Structures,â Proceedings of the IEEE, Vol.

68, pp. 644-654.

[LTZ19] Li, Y., Tang, Y., Zhang, R., and Li, N., 2019. âDistributed Reinforcement Learning for De-

centralized Linear Quadratic Control: A Derivative-Free Policy Optimization Approach,â arXiv preprint

arXiv:1912.09135.

[Mar55] Marschak, J., 1975. âElements for a Theory of Teams,â Management Science, Vol. 1, pp. 127-137.

[NMT13] Nayyar, A., Mahajan, A. and Teneketzis, D., 2013. âDecentralized Stochastic Control with Partial

History Sharing: A Common Information Approach,â IEEE Transactions on Automatic Control, Vol. 58,

pp. 1644-1658.

[NaT19] Nayyar, A. and Teneketzis, D., 2019. âCommon Knowledge and Sequential Team Problems,â IEEE

Transactions on Automatic Control, Vol. 64, pp. 5108-5115.

[Rad62] Radner, R., 1962. âTeam Decision Problems,â Ann. Math. Statist., Vol. 33, pp. 857-881.

24

[Wit71] Witsenhausen, H., 1971. âSeparation of Estimation and Control for Discrete Time Systems,â Pro-

ceedings of the IEEE, Vol. 59, pp. 1557-1566.

[YuB13] Yu, H., and Bertsekas, D. P., 2013. âQ-Learning and Policy Iteration Algorithms for Stochastic

Shortest Path Problems,â Annals of Operations Research, Vol. 208, pp. 95-132.

[ZPB19] Zoppoli, R., Parisini, T., Baglietto, M., and Sanguineti, M., 2019. Neural Approximations for

Optimal Control and Decision, Springer.

25

