âI Donât Think Soâ: Summarizing Policy Disagreements for Agent Comparison

Yotam Amitai, Ofra Amir

Faculty of Industrial Engineering, Technion I.I.T.

1
2
0
2
c
e
D
2

]
I

A
.
s
c
[

2
v
4
6
0
3
0
.
2
0
1
2
:
v
i
X
r
a

Abstract

With Artiï¬cial Intelligence on the rise, human interaction
with autonomous agents becomes more frequent. Effective
human-agent collaboration requires users to understand the
agentâs behavior, as failing to do so may cause reduced pro-
ductivity, misuse or frustration. Agent strategy summariza-
tion methods are used to describe the strategy of an agent to
its destined user through demonstration. A summaryâs objec-
tive is to maximize the userâs understanding of the agentâs ap-
titude by showcasing its behaviour in a selected set of world
states. While shown to be useful, we show that current meth-
ods are limited when tasked with comparing between agents,
as each summary is independently generated for a speciï¬c
agent. In this paper, we propose a novel method for gener-
ating dependent and contrastive summaries that emphasize
the differences between agent policies by identifying states
in which the agents disagree on the best course of action. We
conduct user studies to assess the usefulness of disagreement-
based summaries for identifying superior agents and convey-
ing agent differences. Results show disagreement-based sum-
maries lead to improved user performance compared to sum-
maries generated using HIGHLIGHTS, a strategy summa-
rization algorithm which generates summaries for each agent
independently.

Introduction
With the maturing of reinforcement learning (RL) methods,
RL-based agents are being trained to perform complex tasks
in various domains, including robotics, healthcare and trans-
portation. Importantly, these agents do not operate in a vac-
cum â people interact with agents in a wide range of settings.
Effective interaction between an agent and a user requires
from the latter the ability to anticipate and understand its be-
havior. For example, clinicians should understand treatment
regimes recommended by agents to determine their viability.
To facilitate improved user understanding of agentsâ be-
havior, a range of explainable RL methods have been de-
veloped (Puiutta and Veith 2020; Heuillet, Couthouis, and
DÂ´Ä±az-RodrÂ´Ä±guez 2021). These can be divided into two ex-
planation categories: (1) âlocalâ explanation approaches ex-
plaining why an agent chose a particular action in a given
state, e.g., saliency maps highlighting the information an

Copyright Â© 2022, Association for the Advancement of Artiï¬cial
Intelligence (www.aaai.org). All rights reserved.

agent attends to (Greydanus et al. 2017), and (2) âglobalâ ex-
planation methods that describe the policy of an agent more
generally, such as strategy summaries that demonstrate the
agentsâ behavior in a selected set of states (Amir, Doshi-
Velez, and Sarne 2019). While these approaches have been
shown to improve peopleâs understanding of agent behavior,
they are typically not optimized for a particular user task.

In this work, we aim to enhance usersâ ability to distin-
guish between the behavior of different agents. Such scenar-
ios arise when people are required to select an agent from a
set of alternatives. E.g., a user might need to choose from a
variety of smart home assistants or self-driving cars avail-
able on the market. Importantly, there often isnât a clear
âground-truthâ for which agent is superior, as agents may
prioritize alternative outcomes and users may occupy differ-
ent preferences. For example, some users may prefer self-
driving cars that value safety very highly, while others might
be willing to relax safety considerations to a degree to al-
low for faster driving. The ability to distinguish policies is
also important for model developers, as different conï¬gu-
rations of reward functions and algorithm parameters can
lead to different behaviors in unexpected ways, especially
in domains where the reward function is not obvious, such
as healthcare (Gottesman et al. 2019).

One possible approach for helping users distinguish be-
tween policies of agents is to use strategy summarization
methods (Amir, Doshi-Velez, and Sarne 2018, 2019). Using
these methods, a summary is generated for each agent, al-
lowing the user to compare their behavior. However, these
approaches are not optimized for the task of agent compar-
ison, as each summary is generated independently. For in-
stance, the HIGHLIGHTS algorithm (Amir and Amir 2018)
selects states based on their importance as determined by
the differences in Q-values for alternative actions in a given
state. If two high-quality agents are compared, it is possible
that they will consider the same states as most important,
and will choose the same (optimal) action in those states, re-
sulting in similar strategy summaries. In such a case, even if
the agentsâ policies differ in numerous regions of the state-
space, it will not be apparent from the summaries.

This work presents the DISAGREEMENTS algorithm
which is optimized for comparing agent policies. Our algo-
rithm compares agents by simulating them in parallel and
noting disagreements between them, i.e. situations where the

 
 
 
 
 
 
agentsâ policies differ. These disagreement states constitute
behavioral differences between agents and are used to gen-
erate visual summaries optimized towards showcasing the
most prominent conï¬icts, thus providing contrastive infor-
mation. Our approach assumes access to the agentâs strategy,
described using a Markov Decision Process (MDP) policy,
and quantiï¬es the importance of disagreements by making
use of agentsâ Q-values.

To evaluate DISAGREEMENTS, we conducted two
human-subject experiment to test the following properties of
the algorithm: Firstly, whether it improves usersâ ability to
identify a superior agent when one exists, i.e. given ground-
truth performance measures. Secondly, do DISAGREE-
MENTS summaries outperform HIGHLIGHTS summaries
in conveying differences in behavior of agents to users. Both
experiments make use of HIGHLIGHTS summaries as a
baseline for comparison. Results indicate a signiï¬cant im-
provement in user performance for the agent superiority
identiï¬cation task, while not falling short in the behaviour
conveying task, as compared to HIGHLIGHTS.

Our contributions are threefold: i) We introduce and for-
malize the problem of comparing agent policies; ii) we de-
velop DISAGREEMENTS, an algorithm for generating vi-
sual contrastive summaries of agentsâ behavioral conï¬icts,
and iii) we conduct human-subject experiments, demonstrat-
ing that summaries generated by DISAGREEMENTS lead
to improved user performance compared to HIGHLIGHTS
summaries.

Related Work

In recent years, explainable AI has regained interest, initially
focusing mainly on explaining supervised models. More re-
cently, research has begun exploring explanations of rein-
forcement learning agents (Puiutta and Veith 2020; Heuillet,
Couthouis, and DÂ´Ä±az-RodrÂ´Ä±guez 2021). In this work, we fo-
cus on global explanations that aim to describe the policy
of the agent rather than explain a particular action. Speciï¬-
cally, we develop a new method for strategy summarization.
In this section we describe in more depth strategy summary
methods (Amir, Doshi-Velez, and Sarne 2019).

Strategy summarization techniques convey agent behav-
ior by demonstrating the actions taken by the agent in a se-
lected set of world states. The key question in this approach
is then how to recognize meaningful agent situations.

One such approach called HIGHLIGHTS (Amir and Amir
2018), extracts important states from execution traces of the
agent. Intuitively, a state is considered important if the deci-
sion made in that state has a substantial impact on the agentâs
utility. To illustrate, a car reaching an intended highway exit
would be an important state, as choosing a different action
(continuing on the highway) will cause a signiï¬cant detour.
HIGHLIGHTS has been shown to support peopleâs ability
to understand the capabilities of agents and develop mental
models of their behavior (Amir and Amir 2018; Huber et al.
2020).

Sequeira and Gervasio (2020) extended HIGHLIGHTS
by suggesting additional importance criteria for the sum-
mary state selection referred to as Interestingness Elements.

Huang et al. (2017); Lage et al. (2019) proposed a differ-
ent approach for generating summaries based on machine
teaching methods. The key idea underlying this approach is
to select a set of states that is optimized to allow the recon-
struction of the agentsâ policy using imitation learning or
inverse reinforcement learning methods.

Common to all previous policy summarization ap-
proaches is that each summary is generated speciï¬cally for
a single agent policy, independent of other agents. This can
hinder usersâ ability to compare agents, as the summaries
might show regions of the state-space where the agents act
similarly, failing to capture useful information with respect
to where the agent policies diverge. For example, HIGH-
LIGHTS focuses on âimportantâ states and it is likely that
the states found to be most the important to one agent will be
considered important by another agent as well. These could
be inherently important stages of the domain such as reach-
ing the goal or evading a dangerous state. If the agents act
similarly in these important states, the HIGHLIGHTS sum-
maries of the agents might portray similar behavior, even
for agents whose global aptitude varies greatly. In contrast,
if the summaries do differ from one another and portray dif-
ferent regions of the state-space, they do not convey how the
alternative agent would have acted had it been tasked with
the same scenario.

To address these limitations, we propose a new approach
that is speciï¬cally optimized for supporting usersâ ability to
distinguish between policies.

Background
For the purpose of this work, we assume a Markov Deci-
sion Process (MDP) setting. Formally, an MDP is a tuple
(cid:104)S, A, R, T r(cid:105), where S is the set of world states; A is the set
of possible actions available to the agent; R : S Ã A â R
is a reward function mapping each state, and T r(s, a, s(cid:48)) â
[0, 1] s.t. s, s(cid:48) â S, a â A is the transition function. A
solution to an MDP is a policy denoted Ï.
Summaries A summary, denoted S, is a set of trajecto-
ries T = (cid:104)t1, . . . tn(cid:105). Each trajectory t being a sequence of
l consecutive states t = (cid:104)si, . . . , sD, . . . , si+l(cid:105) surrounding
the disagreement state sD and extracted from the agentâs ex-
ecution traces.

We formally deï¬ne a summary extraction process of an
agentâs policy given an arbitrary importance function Im,
mapping state-action pairs to numerical scores.
Deï¬nition 1 (Summary Extraction). Given an agentâs exe-
cution traces, a summary trajectory budget k, and an impor-
tance function Im.
The agentâs summary S is then the set of trajectories T =
{t1, ..., tk} that maximizes the importance function.
S = max

Im(T )

(1)

T

In this paper, our baseline is the HIGHLIGHTS algorithm,
which computes importance as a function of the Q-values
in a given state. Speciï¬cally, we implement the importance
function from Huber et al. (2020), an extension to HIGH-
LIGHTS, which suggests determining the importance of a

state based on the difference between the maximal and sec-
ond Q-values. Formally:

Im(s) = max

a

QÏ(a, s) â secondh
a

ighest QÏ(a, s)

(2)

The trajectory is then the sequence of states preceding and
succeeding the important state.

Disagreement-Based Summaries
We propose a new summary method which supports the
comparison of alternative agents by explicitly highlighting
the disagreements between them. Thus, constructing con-
trastive summaries that convey policy divergence between
agents exposed to the same world states. This approach is
in line with the literature on explanations from the social
sciences, which shows that people prefer contrastive expla-
nations (Miller 2018). We note that while typically con-
trastive explanations refer to âwhy not?â questions and con-
sider counterfactuals, in our case the contrast is between the
decisions made by two different policies.

We next describe our proposed disagreement-based sum-
mary method. Speciï¬cally, we formalize the notion of
agent disagreement, describe the âDISAGREEMENTSâ al-
gorithm for generating a joint summary of the behavior of
two agents, and describe how to measure the importance of
a disagreement state and disagreement trajectory.

Agent Disagreement The two main dimensions on which
agents can differ are their valuations of states and their poli-
cies, i.e. their choice of action. These dimensions are of
course related, as different state valuations will naturally
lead to different policies. Our deï¬nition of a disagreement
focuses on the policy. We then utilize the value function for
ranking purposes.

In other words, any state s for which different agents
choose different actions is considered a disagreement state.
We use these states to analyze and portray how the agents
differ from one another in their behavior. Formally:
Deï¬nition 2 (Disagreement State). Given two agents Ag1
and Ag2 with policies Ï1, Ï2, respectively. Deï¬ne a state s
as a disagreement state sD iff:

Ï1(s) (cid:54)= Ï2(s)

The set of all disagreement states D would then be:

âs â S | Ï1(s) (cid:54)= Ï2(s) : s â D

(3)

(4)

For a compact MDP where every state may be computed,
this deï¬nition would sufï¬ce. Alas, for more complex set-
tings containing a continuous or vast state space, it is not
feasible to compare all states. The proposed method must be
able to overcome this difï¬culty.

Identifying Agent Disagreements Through Parallel On-
line Execution Given two alternative agents to compare,
we initiate an online execution of both agents simultane-
ously such that we follow the ï¬rst (denoted as the Leader or
L for short, with policy ÏL), while querying the second (de-
noted as the Disagreer or D for short, with policy ÏD) for
the action it would have chosen in each state. Both agents

are constrained to act greedily and deterministically with re-
spect to their Q-values. Upon reaching a disagreement state,
we allow the Disagreer to âsplit-offâ from following the
Leader and continue independently for a limited number of
steps while recording the states it reaches for later analysis.
Once the limit is reached, we store the disagreement trajec-
tory, and revert the Disagreer back to the disagreement state,
from which it continues to follow the Leader until the next
disagreement state is reached and the process is repeated.

The DISAGREEMENTS Algorithm The algorithm
pseudo-code is supplied in Algorithm 1. Itâs parameters are
summarized in Table 1.

Parameter
k

l
h

numSim

Description
Summary budget, i.e. number
of trajectories
Length of each trajectory
Number of states following s to
include in the trajectory
The number of
(episodes)
AGREEMENTS algorithm

simulations
run by the DIS-

overlapLim Maximal number of

shared
states allowed between two tra-
jectories in the summary
Importance method used for
evaluating disagreements

impM eth

F
5

10
5

10

H
5

20
10

10

3

5

Last
State

Last
State

Table 1: Parameters for Frogger & Highway domains.

The Algorithm. First, three lists are initialized for the
Leader traces, disagreement states and Disagreer trajecto-
ries (lines 4â6). Then, simulation of the agents are run (lines
7â27). Each simulation collects all states seen by the Leader
during the execution (line 24), disagreement states (line 13),
and the Disagreer trajectories (lines 14â19). Each step of the
simulation, both agents are queried for their choice of action
(lines 10â11), if they disagree on the action â a disagree-
ment state is added (line 13) and a disagreement trajectory is
created (lines 14â19), after which the simulation is reverted
to the last disagreement state (line 21). Upon simulations
completion, all disagreement trajectories (coupled pairs of
Leader and Disagreer trajectories) are obtained (line 28) and
the most important ones are passed as output (line 29).

Since the Leader agent controls which areas of the do-
main state-space are reached, we repeat the process again,
reversing the agentâs roles. This is important because the
Leader determines which regions of the state-space are
reached and as such also conveys information about the
agentâs preferences. This process results in two sets of dis-
agreements states (one for each agent as the Leader). Typi-
cally, these sets can be very large, and it is infeasible for a
user to explore all of them. Therefore, a ranking procedure
is necessary for the disagreements found in order to gener-
ate a compact summary. We next describe approaches for
quantifying the importance of a disagreement.

Disagreement Importance Various methods can be used
for determining disagreement importance. We ï¬rst deï¬ne
the notion of a stateâs value based on multiple agents.

State Value We assume the agents are Q-based, possess-
ing a Q function (Q(s, a) â R) which quantiï¬es their val-

#Disagreer trajectories

#Leader traces
#Disagreement states

sim, s = InitializeSimulation()
while (!simÏL .ended()) do

aÏL â sim.getAction(ÏL(s))
aÏD â sim.getAction(ÏD(s))
if aÏL ! = aÏD then

Algorithm 1: The DISAGREEMENTS algorithm.
1: Input: ÏL, ÏD, k, l, h,
2: overlapLim, numSim, impM eth
3: Output: S
4: LT r â empty list
5: D â empty list
6: DT â empty list
7: for i = 1 to numSim do
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27: end for
28: DAT â disagreementT rajP airs(D, LT r, DT , l, h)

end for
DT .add(dt)
sim, s = reloadSimulation(Ds[â1])

sÏD â sim.advanceState(ÏD)
aÏD â sim.getAction(ÏD(s))
dt.add(s)

end if
s â sim.advanceState(ÏL)
Ltraces.add(s)

D.add(s)
dt â empty list
for i = 1 to h do

end while
runs = runs + 1

#Disagreer trajectory

29: S â topImpT raj(DAT , k, overlapLim, impM eth)

uation of state-action pairs, denoted as Q-values. Q-values
are calculated and adjusted during the training phase of the
agent and depend on the algorithm used as well as on the
speciï¬cation of the reward function. Therefore, Q-values of
different agents may vary greatly. This renders each indi-
vidual agentâs assessment of a state-action pair as its own
estimate rather than representing a ground truth. The values
themselves may not even be on the same scale. To allow for
comparison between values, we normalize each agentâs Q-
values by dividing by the maximum Q-value appearing in
each, thus rendering each value an indication of how good a
state-action pair is compared to the best one observed by the
agent. Formally:

(cid:0)Q(s, a)(cid:1)

Q(cid:48) =

maxs,a

Q â mins,a
(cid:0)Q(s, a)(cid:1) â mins,a
Since our agentsâ action selection is greedy and determin-
istic with respect to their Q-values, we denote the value of
state s as the highest Q-value associated with it.
Deï¬nition 3 (Agent State Value). Given an agent Ag, its Q
function QAg and a state s, we deï¬ne the value of s as:

(cid:0)Q(s, a)(cid:1)

(5)

V Ag(s) = max

a

Q(cid:48)Ag(s, a)

(6)

Alas, measuring only the importance of a single state has
its limitations. Without ground truth, we are left only with

the agentsâ estimations which may be ï¬awed. Suppose our
agents reach a disagreement state where both are convinced
the otherâs action is a poor choice. They each go their sepa-
rate ways only to reunite at a shared state after several steps
with minimal to no impact on the succeeding execution, e.g.
overtaking a vehicle from the left or from the right. This re-
alization led us to formulate a trajectory-based approach for
determining the importance of a disagreement state.

Disagreement Trajectory Importance To determine the
importance of a disagreement state, we compare the trajec-
tories that branch out of it by following each agent. We for-
mulate trajectory value metrics to evaluate these, while con-
straining ourselves to observing only trajectories of similar
length.

A trajectory tÏ

h(s) = (cid:104)s+1, ..., s+h(cid:105) denotes the sequence
of states encountered when following state s for h steps ac-
cording to a policy Ï. Since each agent evaluates states dif-
ferently, we consider the value of a state as the sum of both
agentsâ valuations, i.e. V (s) = V L(s) + V D(s). There are
numerous ways to quantify the importance of a trajectory. A
description of several methods tested in our work is provided
in the Appendix. The summaries generated for the user stud-
ies made use of the last-state importance method.

Last State Importance: We deï¬ne the importance of a
disagreement trajectory as the difference between the values
of the last states reached by each of the agents. This reï¬ects
how âfar offâ from each other the disagreement has led the
agents. Formally:

Im(tÏL

h (s), tÏD

h (s)) = |V (sÏL
This metric achieves a high importance score when one
agent arrives at a state which both agents agree is valuable,
while the other agent arrives at a state whose value both
agents agree is poor.

+h) â V (sÏD

+h)|

(7)

Trajectory Diversity Using these methods we are able to
acquire the set of disagreement states D ordered by impor-
tance, and for each, their corresponding trajectories. These
shall be woven together to create a visual summary for dis-
playing to the user. To increase the coverage of the summary
and avoid showing redundant trajectories (in both methods),
we restrict the summary generated to not contain i) multiple
trajectories that end or begin at the same state, ii) trajectories
where the Leader and Disagreer share the same state before-
last and iii) overlapping trajectories which share more than
a predeï¬ned number of states.

Empirical Methodology
To evaluate our method we conducted two user studies. The
ï¬rst was designed to assess whether DISAGREEMENTS
summaries help users identify the superior between two
alternative agents, while the second user study examines
whether such summaries are useful for conveying agent be-
havior differences. In both studies we use the HIGHLIGHTS
algorithm as a baseline for comparison. We note here that
there is a signiï¬cant difference between the output sum-
maries of both methods rooted in the fact that DISAGREE-
MENTS was designed for presenting two policies in a con-
trastive manner. This is achieved by portraying both agents

simultaneously on the screen. To our knowledge no other
global explanation methods exist that directly compare poli-
cies and visualizes their differences to lay users.

Empirical Domains. To evaluate our algorithm we gener-
ated summaries of agents playing the game of Frogger (Se-
queira and Gubert 2020) and controlling a vehicle in a high-
way environment (Leurent 2018).

Frogger The objective of the game is to guide a frog from
the bottom of the screen to an empty lily-pad at the top of
the screen. The agent controls the frog and can initiate the
following four movement actions: up, down, left or right,
causing the frog to hop in that direction. To reach the goal
the agent must lead the frog across a road with moving cars
while avoiding being run over, then, the agent must pass the
river by jumping on passing logs. This domain allows us
to compare different agents in a setting with ground truth
information about agentsâ skill, i.e. game score.

Highway This domain consists of a busy highway with
multiple lanes and vehicles. The agent controls a vehicle
driving through trafï¬c with the intent of avoiding colli-
sions. The agent can choose to move right or left (changing
lanes), increase or decrease velocity or stay idle, i.e. make
no change. There is no deï¬ned target the agent is required
to reach, instead the road goes on continuously. This prop-
erty allows us to observe the agentâs general behavior and
preferences instead of focusing on its progression towards
reaching the goal.Screenshots from the DISAGREEMENTS
output summaries displayed in Figure 1.

Figure 1: Top: Frogger; Bottom: Highway.
Red & black rectangles represent different agents.

Frogger Agents. We made use of the framework devel-
oped by Sequeira and Gervasio (2020) to test the DIS-
AGREEMENTS algorithm on multiple conï¬gurable agents
of varying capabilities. Three different agents were trained
using standard Q-learning (?), based on the conï¬gurations

provided by the framework.
â¢ Expert (E): 2000 training episodes. Default rewards. Av-

erage game score: 110,000.

â¢ Mid-range (M): 1000 training episodes. Default re-

wards. Average game score: 50,000.

â¢ LimitedVision (LV): 2000 training episodes. Default
rewards. Lower perception of incoming cars. Average
game score: 55,000.

Agent performance was calculated by averaging the game
score of ten executions. Each agentâs unique conï¬guration
contributes to its performance, providing us a ground truth
for the assessment tasks we present to the experiment par-
ticipants. The agentsâ skill hierarchy, based on their aver-
age score, is as follows: E > LV > M . An important
requirement for the experiment was that all agents have a
decent ability to play Frogger. Prior to this study, we veri-
ï¬ed via an additional experiment that HIGHLIGHTS sum-
maries are indeed useful for comparing Frogger agents that
differ substantially in their skills (see Appendix). All HIGH-
LIGHTS and DISAGREEMENTS summaries were gener-
ated for fully trained agents, thus reï¬ecting their ï¬nal poli-
cies.

Highway Agents. Agents with varying behaviors were
trained by altering their reward functions. All highway
agents were trained for 2000 episodes using double DQN
architecture (Hasselt 2010) and rewarded for avoiding colli-
sions.
â¢ ClearLane (CL): Rewarded for high velocity while
maximizing the distance between itself and the nearest
vehicle in front of it.

â¢ SocialDistance (SD): Rewarded for maximizing the dis-

tance between itself and the closest k vehicles.

â¢ FastRight(FR): Rewarded for high velocity and driving

in the rightmost (bottom) lane.

Henceforth, we will refer to all domain agents by their ab-
breviations.

Summary Attributes All summaries were composed of ï¬ve
trajectories made up of sequential states, ten for Frogger and
twenty for Highway. These contained the important state at
the center of the trajectory, with half the states preceding and
the rest succeeding it. Video-clips of the summaries were
generated to present to the users and a fade-in and fade-out
effect was added to further call attention to the transition
between trajectories. For more details, sensitivity analysis
and the complete surveys, see Appendix.

Experiment 1 - Identifying Superiority The objectives
of the ï¬rst experiment were twofold. Firstly, to support our
claims regarding the limitations of the HIGHLIGHTS algo-
rithm for comparing agents, and secondly, to compare the
DISAGREEMENTS algorithm to HIGHLIGHTS and show
its added value.

Hypotheses. We hypothesized that summaries generated
by the HIGHLIGHTS algorithm are limited in their ability to
help users distinguish between agents and the DISAGREE-
MENTS algorithm is more suited for this task. More specif-
ically, we state the following hypotheses:
H1: Participants shown summaries generated by HIGH-
LIGHTS for agents of decent skill will struggle to identify

the better performing agent.
H2: Participants shown summaries generated by the DIS-
AGREEMENTS algorithm will exhibit a higher success rate
for identifying the better performing agent, compared to
ones shown HIGHLIGHTS summaries.

Experimental Conditions A between-subject experimen-
tal setup was designed with two experimental conditions
that varied in the summary generation method, DISAGREE-
MENTS or HIGHLIGHTS (Amir and Amir 2018). Partici-
pants were randomly assigned a condition.

Participants. 74 participants were recruited through Ama-
zon Mechanical Turk (27 female, mean age = 36.51, STD
= 9.86), each receiving $3 for their completion of the Task.
To incentivize participants to make an effort, they were pro-
vided a bonus of 10 cents for each correct answer in the su-
periority identiï¬cation task. Participants who spent less than
a threshold duration of time on experiment tasks, based on
the length of the task summary video, were ï¬ltered out.

Procedure. Participants were ï¬rst introduced to the game
of Frogger and the concept of AI agents. Each explanation
was followed by a short quiz to ensure understanding be-
fore advancing to the task. Next, participants were randomly
split into one of two conditions and were shown summary
videos of pairs of different agents generated using either
DISAGREEMENTS or HIGHLIGHTS.

Participants in both groups were ï¬rst introduced to the
summary method they would be shown and were required to
pass a quiz to ensure their understanding. Participants were
then asked to choose the better performing agent based on
the summary videos. They were able to pause, play and re-
peat the summary videos without restrictions, allowing free-
dom to fully inspect the summary before deciding which
agent they believe is more skillful. Participants were also
asked to provide a textual explanation for their selection and
to rate their decision conï¬dence on a 7-point Likert scale (0 -
not at all conï¬dent to 6 - very conï¬dent). Overall, there were
3 pairs of agent comparisons (cid:104)E, M (cid:105)(cid:104)E, LV (cid:105)(cid:104)LV, M (cid:105). The
ordering of the agent pairs was randomized to avoid learning
effects, and participants were also not told if the same agent
appeared in multiple comparisons, that is, they made each
decision independently of other decisions.

Participants in the HIGHLIGHTS condition were shown
a HIGHLIGHTS summary of each agent (i.e. two separate
videos, one for each agent.), while participants in the DIS-
AGREEMENTS group were supplied two conï¬gurations of
the DISAGREEMENTS summaries. One summary where
the ï¬rst agent is the Leader while the second is the Dis-
agreer, and the opposite summary, where the ï¬rst agent is
the Disagreer and the second is the Leader. Upon conclu-
sion, participants answered a series of explanation satisfac-
tion questions adapted from (Hoffman et al. 2018).

Evaluation Metrics and Analyses. The main evaluation
metric of interest was the success rate of identifying the su-
perior Frogger agent with each summary method. We com-
pare this metric across all the agent selection tasks given
to participants. We also compare participantsâ conï¬dence in
their decision. To compare the explanation satisfaction rat-
ings given to the summaries, we averaged the values of the
different items normalizing in such a way that higher values

always mean that the summary is more helpful. In all anal-
yses we used the non-parametric Mann-Whitney U test and
computed effect sizes using rank-biserial correlation. In all
plots the error bars depict the bootstrapped 95% conï¬dence
intervals (Efron and Tibshirani 1994).

Experiment 2 - Conveying Agent Differences This ex-
perimentâs objective was to test the usefulness of DIS-
AGREEMENTS summaries for conveying differences in
general agent behavior in comparison to HIGHLIGHTS.

Hypotheses. The DISAGREEMENTS algorithm is de-
signed to portray instances of disagreement between agents.
We hypothesized that this would provide a clear and con-
trastive distinction between the alternative agents, thus em-
phasizing behavioral differences and providing a more ap-
pealing visual experience. More speciï¬cally,we state the fol-
lowing hypotheses:
H3: Success rate of participants shown DISAGREEMENTS
summaries will surpass that of participants shown HIGH-
LIGHTS summaries.
H4: participants will prefer summaries generated by the
DISAGREEMENTS method.

Experimental Conditions A within subject setup was cho-
sen in order to allow participants to provide a direct compari-
son between the methods and state their preferences. As par-
ticipants experience both HIGHLIGHTS and DISAGREE-
MENTS summaries, to reduce cognitive overload, we chose
to display only two agent comparisons for each method. We
chose the comparisons between the most distinctly dissim-
ilar agents, dropping (cid:104)SD, CL(cid:105) due to similarity of reward
associated with distance from neighboring vehicles.

Participants. 45 participants were recruited through Ama-
zon Mechanical Turk (13 female, mean age = 37.51, STD
= 10.35), receiving similar pay and bonus incentive as in
the ï¬rst experiment. As in Exp#1, participants were ï¬ltered
out if their task completion time was below a threshold.

Procedure. Participants followed a similar procedure as
in the previous experiment diverging solely in the domain
introduction and the questions asked. Instead of superiority
between the agents, participants were queried about which
trait was more dominant in the compared agents. The ground
truth was established directly from the reward functions of
the agents. Each comparison included two such questions
along with a mandatory conï¬dence rating and textual ex-
planation of the answers. In addition, participants were ul-
timately asked which method they preferred. Since this was
a within-subject design, participants saw both methods, in a
random order. All questions are provided in the Appendix.

Evaluation Metrics and Analyses. The main evaluation
metric of interest was the success rate of correctly assign-
ing a more dominant trait to an agent. We compare this
metric across the different agent selection tasks and sum-
mary methods. Similarly to experiment 1, we compare par-
ticipantsâ conï¬dence in their answers. For evaluating sum-
mary method satisfaction of participants we used the non-
parametric Wilcoxon signed-rank test (Wilcoxon 1947) for
matched pairs.

Figure 2: Participant Success Percentage per Summary Method. DA: DISAGREEMENTS; HL: HIGHLIGHTS.

Results
We now describe the results of our comparison between
the HIGHLIGHTS strategy summarization method and our
novel DISAGREEMENTS approach. We report the main ex-
perimental results with respect to the hypotheses raised in
the previous section.

Figure 2 shows the percentage of participants who were
successful in answering the experiment tasks in each of the
experiment conditions, for each agent pair combination.

(H1) Participants in the HIGHLIGHTS condition strug-
gled to successfully identify the better performing agent in
the comparison task. When all agents are of decent perfor-
mance, we see the difï¬culty of distinguishing between them
manifest itself in a poor success rate. Based on participantsâ
textual explanations of the choice of agent, it seems they
were concerned that agent E was indecisive, e.g., â[Agent
E] seems very indecisive while ... [Agent M ] seems to have
a plan and is going with it.â. We hypothesize that these re-
sponses are a consequence of a single trajectory in agent Eâs
summary where the frog is seen leaping between logs in a
seemingly indecisive manner. These results emphasize the
limitations of independent comparisons.

(H2) Participants in the DISAGREEMENTS condition
were more successful in the agent comparison task. Partic-
ipants in the DISAGREEMENTS group showed vast im-
provement in the ability to identify the better performing
Frogger agent (see Figure 2). The differences in success rate
between conditions were statistically signiï¬cant and sub-
stantial for all agent comparisons (E vs. LV : p = 1.6â5; E
vs. M : p = 1.7â5; LV vs. M : p = 0.018). Textual explana-
tions provide insights regarding how the contrastive nature
of the DISAGREEMENTS summaries helped participants
decide which agent to choose, e.g. â I preferred the path that
... [Agent E] was takingâ; âI felt that ... [Agent E] was mak-
ing slightly stronger moves, and pushing ahead furtherâ.

(H3) A signiï¬cant difference was found between success
rates of participants in the conveying differences task. Par-
ticipants achieved a signiï¬cantly higher success rate with the
DISAGREEMENTS method in the F R vs. SD comparison
(p = 0.014). Albeit, this was mostly a result of the low per-
formance of HIGHLIGHTS in Q1 due to the SD summary
containing, coincidentally, only trajectories of the agent at
the bottom lane. The inferior performance of DISAGREE-
MENTS in Q1 of CL vs. F R (p = 0.187) can be explained

by summary trajectories where no vehicles were present in
CLâs lane allowing it to drive faster than F R and appear
less considerate of keeping distance. While not necessarily
outperforming it, the DISAGREEMENTS is at least equiva-
lently useful as HIGHLIGHTS, which was shown to be bet-
ter than random (Huber et al. 2020).

(H4) No clear participant preference towards one sum-
mary method was observed. Most participants answered
that both methods were equally beneï¬cial. However, more
participants found DISAGREEMENTS more helpful and
containing less irrelevant information than HIGHLIGHTS,
while ï¬nding the latter more pleasing.

Conï¬dence and satisfaction In both experiments no statis-
tically signiï¬cant differences were found between the con-
ï¬dence or satisfaction of participants in different conditions
(See Appendix).

Discussion and Future Work
With the maturing of AI, circumstances which require peo-
ple to choose between alternative market-available solu-
tions, are likely to arise. The necessity of distinguishing be-
tween alternative agents becomes ever more clear. More-
over, distinguishing between policies is key for developers
when analyzing different algorithms and conï¬gurations.

This paper presented a new approach for comparing RL
agents by generating policy disagreement summaries. Ex-
perimental results show summaries help convey agent be-
haviour differences and improve usersâ ability to identify su-
perior agents, when one exists.

As for future work, we note the following possible direc-
tions: i) expanding DISAGREEMENTS to enable compari-
son of more than two agents; ii) testing additional state and
trajectory importance methods; iii) further enhancing the di-
versity between trajectories in the summary, and iv) formu-
lating and deï¬ning disagreement âtypesâ for generating fur-
ther user-speciï¬c summaries.

Acknowledgements
The research was partially funded by the Israeli Science
Foundation grant #2185/20

References
Amir, D.; and Amir, O. 2018. HIGHLIGHTS: Summarizing
Agent Behavior to People. In Proc. of the 17th International

learning in healthcare.

conference on Autonomous Agents and Multi-Agent Systems
(AAMAS).
Amir, O.; Doshi-Velez, F.; and Sarne, D. 2018. Agent
In Proceedings of the 17th In-
Strategy Summarization.
ternational Conference on Autonomous Agents and MultiA-
gent Systems, 1203â1207. International Foundation for Au-
tonomous Agents and Multiagent Systems.
Amir, O.; Doshi-Velez, F.; and Sarne, D. 2019. Summariz-
ing agent strategies. Autonomous Agents and Multi-Agent
Systems, 33(5): 628â644.
Efron, B.; and Tibshirani, R. J. 1994. An introduction to the
bootstrap. CRC press.
Gottesman, O.; Johansson, F.; Komorowski, M.; Faisal, A.;
Sontag, D.; Doshi-Velez, F.; and Celi, L. A. 2019. Guide-
Nature
lines for reinforcement
medicine, 25(1): 16â18.
Greydanus, S.; Koul, A.; Dodge, J.; and Fern, A. 2017. Vi-
sualizing and Understanding Atari Agents. arXiv preprint
arXiv:1711.00138.
Hasselt, H. 2010. Double Q-learning. Advances in neural
information processing systems, 23: 2613â2621.
Heuillet, A.; Couthouis, F.; and DÂ´Ä±az-RodrÂ´Ä±guez, N. 2021.
Explainability in deep reinforcement learning. Knowledge-
Based Systems, 214: 106685.
Hoffman, R. R.; Mueller, S. T.; Klein, G.; and Litman, J.
2018. Metrics for explainable AI: Challenges and prospects.
arXiv preprint arXiv:1812.04608.
Huang, S. H.; Held, D.; Abbeel, P.; and Dragan, A. D.
2017. Enabling robots to communicate their objectives. Au-
tonomous Robots, 1â18.
Huber, T.; Weitz, K.; AndrÂ´e, E.; and Amir, O. 2020. Lo-
cal and Global Explanations of Agent Behavior: Integrat-
CoRR,
ing Strategy Summaries with Saliency Maps.
abs/2005.08874.
Lage, I.; Lifschitz, D.; Doshi-Velez, F.; and Amir, O. 2019.
Exploring Computational User Models for Agent Policy
Summarization. In Proceedings of the Twenty-Eighth Inter-
national Joint Conference on Artiï¬cial Intelligence, IJCAI-
19, 1401â1407. International Joint Conferences on Artiï¬cial
Intelligence Organization.
Leurent, E. 2018. An Environment for Autonomous Driv-
ing Decision-Making. https://github.com/eleurent/highway-
env.
Miller, T. 2018. Explanation in artiï¬cial intelligence: In-
sights from the social sciences. Artiï¬cial Intelligence.
Puiutta, E.; and Veith, E. M. S. P. 2020. Explainable Rein-
forcement Learning: A Survey. CoRR, abs/2005.06247.
Sequeira, P.; and Gervasio, M. 2020.
Interestingness ele-
ments for explainable reinforcement learning: Understand-
ing agentsâ capabilities and limitations. Artiï¬cial Intelli-
gence, 288: 103367.
Sequeira, P.; and Gubert, J. 2020. Frogger python imple-
mentation.
Wilcoxon, F. 1947. Probability tables for individual com-
parisons by ranking methods. Biometrics, 3(3): 119â122.

