4
2
0
2

r
p
A
4
1

]
L
C
.
s
c
[

3
v
4
4
4
1
0
.
0
1
3
2
:
v
i
X
r
a

Adapting LLM Agents with Universal Feedback in Communi-
cation

Kuan Wang1, Yadong Lu 2, Michael Santacroce2, Yeyun Gong3,
Chao Zhang1, Yelong Shen2
1Georgia Institute of Technology 2Microsoft Azure AI 3Microsoft Research
{kuanwang, chaozhang}@gatech.edu
{yadonglu, misantac, yegong, yeshe}@microsoft.com

Abstract

Recent advances in large language models (LLMs) have demonstrated
potential for LLM agents. To facilitate the training for these agents with
both linguistic feedback and non-linguistic reward signals, we introduce
Learning through Communication (LTC). We design a universal buffer to
store all the feedback, and an iterative pipeline to enable an LLM agent to
explore and update its policy in an given environment. To utilize our uni-
versal buffer for capturing agent interactions in various tasks, we introduce
diverse communication patterns tailored for both single-agent and multi-
agent environments. We evaluate the effectiveness of our LTC approach on
four diverse datasets: ALFWorld (single-agent), HotpotQA (multi-agent
collaboration), Chameleon (multi-agent competition), and GSM8k (multi-
agent teacher-student). On these datasets, LTC outperforms supervised
instruction fine-tuning baselines by 3.6% to 12%. These results demonstrate
the versatility and effectiveness of LTC in facilitating online adaptation for
LLM agents.

1

Introduction

Recent advances in large language models (LLMs) Ouyang et al. (2022); Bubeck et al.
(2023); Wei et al. (2022a) have shed light on human-like LLM agents. In addition to design
prompting methods Wei et al. (2022b); Yao et al. (2023); Wu et al. (2023a), recent works also
focus on how to train the LLMs agent use linguistic feedback and non-linguistic reward
signals. The linguistic feedback is usually processed as instruction data to do Instruction
Fine-tuning (IFT) Chung et al. (2022); Lee et al. (2023); Honovich et al. (2022); Wang et al.
(2022e), while the non-linguistic reward signals are generally used to do alignment with
human preference Ouyang et al. (2022); Bai et al. (2022a); Stiennon et al. (2020); Leike et al.
(2018).

While some scenarios provide agents with heterogeneous feedback, existing methods can
only utilize the feedback partially. For instance, in multiplayer board role-playing games,
players generate a wealth of linguistic data, and the game concludes with definitive reward
signals indicating victory or defeat. Current approaches employ the linguistic data for IFT Li
et al. (2023); Micheli & Fleuret (2021), while the reward signals serve solely as a filtering
criterion to select the ILF data instead of the objective of reinforcement learning.

To address this gap, we propose a universal framework, named Learning through Commu-
nication (LTC), to train LLM agents with both linguistic feedback and non-linguistic reward
signals. We design a universal buffer to store all the feedback, and an iterative pipeline
to enable an LLM agent to explore and update its policy in an given environment. Each
iteration of LTC comprises two distinct phases: (1) Exploration: During this phase, the agent
interacts with the environments and other agents to gather diverse trajectories (linguistic)
and reward signals (non-linguistic) into the universal buffer. (2) Updating: In this phase, the
agentâs model is updated based on the collected data in the universal buffer. For updating,
LTC combines the language modeling loss and the PPO loss to strike a balance between

1

 
 
 
 
 
 
Figure 1: The LTC framework is adept for both single-agent and multi-agent environ-
ments. Within these environments, agents have the capability to persistently engage in
exploration and interaction to collect trajectories through various communication patterns.
Concurrently, LTC facilitates the training of these agents utilizing the data acquired from
their exploratory activities. This process enables the agents to autonomously adapt to their
respective environments, negating the necessity for human supervision.

language consistency and reward signals As the pivot of the iterative pipeline, the replay
buffer is updated after each exploration phase, and a subset of the buffer is sampled for the
updating phase.

To universally supports linguistic feedback and non-linguistic reward signals during com-
munication, we design the replay buffer structure as a trajectory of tokens sequences
(Figure 2). Such a replay buffer structure is applicable to diverse tasks, including single-
agent and multi-agent environments. To facilitate collecting trajectories with linguistic data
and reward signals, we devised three communication patterns: (1) Single-agent Monologue:
This pattern allows a single agent to collect trajectories contain linguistic data and receive
reward signals from the environments. (2) Multi-agent Dialogue: This pattern enables
multiple agents to interact with each other and external tools to collect linguistic data, and
utilize reward signals provided by the environments. (3) Teacher-student Dialogue: This
variant of multi-agent dialogue that collect the linguistic feedback and non-linguistic reward
signals provided by a teacher agent instead of the environment.

We evaluate LTC method on several representative datasets: ALFWorld for decision-making,
HotpotQA for knowledge-intensive reasoning, and GSM8k for numerical reasoning. Through-
out these experiments, LTC consistently outperforms the baselines. In ALFWorld, LTC
outperforms the strong instruction tuning baseline by 12% on success rate, even in the
challenging Pick 2 task. This shows that our communication mechanism enables the agent
to learn from its experiences for task solving. On HotpotQA, LTC outperforms the instruction
tuning baseline by 5% on EM score, and our Llama-7B based agent even obtains slightly
better (0.6%) performance than the ReAct-Tuning baseline which uses 9Ã larger PaLM-62B
model. On GSM8k, LTC also beats the CoT-Tuning baseline by 3.6% on accuracy. These
results highlight the adaptability and effectiveness of LTC approach across varied domains.

Our key contributions are:

1. Learning through Communication (LTC): We propose a universal framework,
named Learning through Communication (LTC), to train LLM agents with both
linguistic feedback and non-linguistic reward signals. We design a universal
buffer to store all the feedback, and an iterative pipeline to enable an LLM agent to
explore and update its policy in an given environment.

2. Task-specific Communication Patterns: The LTC paradigm allows for flexible
design of communication patterns tailored to different tasks. We introduce three
specific patterns: Single-agent Monologue, Multi-agent Dialogue, and Teacher-
student Dialogue. These patterns can be combined to generate diverse structured
interactions and feedback signals for agent training, catering to various task types.

3. Empirical Study and Findings: We conduct rigorous experiments on public bench-
mark tasks to demonstrate the effectiveness of LTC. Our results indicate that LTC
can be a superior approach compared to instruction-tuning or prompting baselines.

2

Few-shotsAgentPromptTraining-freeInefficient InferenceExamples dependentPrompting-freeEfficient InferenceOnline learningTrajectoriesReply buffersAgentTrain LTCICLEnv: You are in the middle of a room. Looking quickly around you, you see a countertop, a fridge â¦ Your task is to: put an egg in the bowl.Go to the countertop. Put the egg in the bowl.Success, reward +1An egg is more likely to appear in the fridge â¦You need to find and take an egg first â¦Open the fridge, I get an egg.A bowl is more likely to appear on the counterâ¦Now you need to find the bowl and put the â¦Env: You are in the middle of a room. Looking quickly around you, you see a countertop, a fridge â¦ Your task is to: put an egg in the bowl.Go to the countertop. Put the egg in the bowl.Success, reward +1Open the fridge, I get an egg.Env: Letâs play the Chameleon. The topic is: Animals. Secret word: Elephant (visible to chameleon players)[P1, P3] reward +1My clue is: Itâs a smart mammal.They can run fast.Itâs native to Africa and Asia.I think it is P2. Their clue doesnât fit as well.I vote P1.I vote P2. His clue doesnât seem to fit â¦Now vote who is the chameleonP2P1P3Accusation is correct. P2 can guess the secret word.Accusation is correct. P2 can guess the secret word.I guess the secret word is cheetah.(chameleon)[P2] reward -1TrajectoriesAgentTrain Prompting-freeSingle-agent Multi-agent An egg is more likely to appear in the fridge â¦I need to find and take an egg first â¦A bowl is more likely to appear on the counterâ¦Now I need to find the bowl and put the â¦2Figure 2: The buffer data is a serial of integer/float sequences. We treat each token id as
the action in our reinforcement learning formula. We also save its corresponding mask
indicating the source of the token, the value from the critic model, the log-prob indicating
the log-likelihood when sampling the action and the reward from the environment/other
agents.
2 Related Work

2.1

Instruction Tuning

Instruction tuning (IT) is an important technique for improving the capabilities and con-
trollability of LLMs Radford et al. (2019); Brown et al. (2020); Wei et al. (2022a); Qin et al.
(2023); OpenAI (2023); Chowdhery et al. (2022); Touvron et al. (2023). Many studies have
been dedicated to instruction data generation and selection Chung et al. (2022); Wang et al.
(2022e); Lee et al. (2023). For instance, Unnatural Instructions Honovich et al. (2022) is cre-
ated by using the Super-Natural Instructions dataset Wang et al. (2022f) as a seed to prompt
InstructGPT Ouyang et al. (2022). Self-Instruct Wang et al. (2022e) employs a recursive
pipeline that generates instruction data from hand-crafted seed tasks using ChatGPT Ope-
nAI (2022). Other studies focus on fine-tuning pre-trained LLMs with instruction data.
BLOOMZ Muennighoff et al. (2022) is initialized with BLOOM Scao et al. (2022) and then
fine-tuned using the xP3 instruction dataset Muennighoff et al. (2022). Flan-T5 is initialized
with T5 Raffel et al. (2020) and fine-tuned with the FLAN dataset Longpre et al. (2023).
Additionally, after the release of LLaMA Touvron et al. (2023), many works have utilized it
as the base model for instruction tuning, such as Alpaca Taori et al. (2023), Vicuna Chiang
et al. (2023), and GPT-4-LLM Peng et al. (2023). Some papers explore alignment fine-tuning
using RLHF Ouyang et al. (2022); Bai et al. (2022a); Stiennon et al. (2020); Leike et al. (2018).
InstructGPT Ouyang et al. (2022) employs GPT-3 for supervised fine-tuning on a human-
filtered instruction dataset, followed by training a reward model and using PPO Schulman
et al. (2017) for RLHF. Claude investigates RLHF Bai et al. (2022a) and constitutional ap-
proaches Bai et al. (2022b) for making LLMs both harmless and helpful. DPO Rafailov
et al. (2023) fine-tunes the LLMs to align with human preferences by directly optimizing a
classification problem on preference data instead of RLHF. While these prominent research
works focus on aligning LLMs for general instruction-following, our objective is to adapt
LLM agents for specific tasks or environments.

2.2 LLM Agents

LLMs have demonstrated the potential to act as advanced agents Ouyang et al. (2022);
Bubeck et al. (2023); Wei et al. (2022a), and significant progress has been made in developing
versatile LLM agents Weng (2023); Sumers et al. (2023); Park et al. (2023); Liu et al. (2023a);
Lin et al. (2023); Xu et al. (2023) and benchmarks Wang et al. (2022a); Deng et al. (2023);
Liu et al. (2023b). For planning, Chain-of-Thought (CoTWei et al. (2022b)) prompts the
model to think step by step, by decomposing complex tasks into smaller and simpler steps.
Self Consistency (Wang et al., 2022c;d) extends CoT by using ensembles of predictions to
improve consistency of the LLM. Inner Monologue Huang et al. (2022b) leverages envi-
ronment feedback to enhance LLMsâ planning and processing capabilities in embodied
robotics tasks without extra training. ReAct Yao et al. (2023) integrates reasoning and action
taking, expanding the action space to include both task-specific discrete actions and lan-
guage. Reflexion Shinn et al. (2023) equips agents with dynamic memory and self-reflection
capabilities to improve reasoning by using continuous trials in the same environment as
feedback. Recent research has also shown that LLMs can be augmented as an autonomous
agent to use external tools to solve problems in interactive environments. These techniques
include retrieval augmentation Shi et al. (2023); Yao et al. (2023); Izacard et al. (2022), math
tools Schick et al. (2023); Yao et al. (2023); Lu et al. (2023), and code interpreters Gao et al.
(2022); Wang et al. (2022b). Prior works also have explored using multiple LLMs in a
collaborative setting to solve complex tasks Hong et al. (2023); Qian et al. (2023); Li et al.

3

Figure 3: LTC has an iterative two-phase framework. During the exploration phase, the
agent proactively explores new environments and communicates with other agents, gather-
ing the trajectories to update the replay buffer. Then the agent is trained for updating the
policy in the updating phase.

(2023); Wang et al. (2023); Talebirad & Nadiri (2023); Akata et al. (2023). Open-source
projects like AutoGPT Significant-Gravitas (2023), GPT-Engineer AntonOsika (2023), and
BabyAGI yoheinakajima (2023) also showcase the potential of LLM not just in generating
content but also as a general problem solver. Most of the above methods are based on either
human-designed few-shot prompting examples, or finetuning with pre-collected instruction
datasets. Our LTC is not a few-shot prompting method and we focus on adapting the agent
by collecting training data automatically by exploration.

3 Learning Through Communication

We design Learning Through Communication (LTC), an iterative training method for LLM
agents to continuously adapt to new environments. As shown in Figure 3, LTC iterates
between two phases: (1) An exploration phase where agents can interact with new envi-
ronments and other agents to collect trial data with feedback, and (2) a updating phrase to
fine-tune the agent to update the policy.

3.1 Exploration Phase

At the start of each iteration, the agent explores the environments to get the trajectories
and the reward signal data. We denote these data as a tuple: S = (T , M, R), where
T = {t1, t2, . . . , tn} represents the text data generated by the communication process during
agent exploration, M = {m1, m2, . . . , mn} with mi â {0, 1, 2} indicates the source of the
text data (system or agents), R = {r1, r2, . . . , rn} with ri â {â1, 0, 1} represents the reward
signals provided by either the system or the agents. We demonstrate the the details of this
data structure in Figure 2, M is the mask list, and R is the reward list. In PPO training,
both the value list and the log-prob list correspond directly to the action list. For brevity, we
denote these three lists together as T here. Please see Appendix A.2 for more detaills.

To collect the trajectories data S = (T , M, R) from different types of tasks, we design the
communication patterns for these tasks. Here we provide three communication patterns:

â¢ Single-agent Monologue: Single-agent Monologue is a single-agent soliloquy
style communication pattern, designed for general instruction following tasks
(Algorithm 1). It split the tasks into step by step like ReAct and CoT, and their own
trajectories with system rewards are collected to train themselves at the same time
with their exploration. Figure 1 left is a toy example of ALFWorld to demonstrate
the Monologue pattern with a single agent. This agent soliloquizes to think the
situation and take the actions to explore the environment and finally get the reward
provided by the environment. This pattern is based on the think and act steps in the
ReAct formulation Yao et al. (2023), we design the training buffer collection process
to make it aligh with our reinforcement learning formulation.

â¢ Multi-agent Dialogue: Multi-agent Dialogue is a multi-agent discussion style
pattern (Algorithm 2). It is designed for multi-agent collaborating and competing
tasks, where multiple agents will play their role by speaking or taking actions in a
certain order and a final reward will be given by the environment based on the

4

Replay BuffersCollectGPT-3GPT-4Llama-2GPT-3GPT-4Llama-2GPT-3EnvironmentsGPT-4Replay BuffersExploration Phaseâ¦Llama-2GPT-3AgentEnvironmentsGPT-4InteractInteractGPU3Llama-2GPT-3AgentEnvironmentsGPT-4InteractInteractGPU2Llama-2GPT-3AgentEnvironmentsGPT-4InteractInteractGPU1LLMPre-trainedReinforcement learningCollect TrajectoriesTrainAgentPolicy Update PolicyExploration PhaseUpdating PhaseInitialize2Figure 4: The toy examples to demonstrate communication patterns: 1) the left figure is
the Multi-agent Dialogue pattern, where two agent play different roles to collaborate on
the task. The thinker agent is responsible for analyzing the situation and give suggestion
to the actor agent who is responsible for making decisions. We can just assign the LTC
agent to play the thinker agent when testing without GPT-4 agent. 2) the right figure is the
Teacher-student Dialogue pattern, where the student agent starts with an initial answer to
the current question, and then the teacher directly corrects the answer with a reward. To
help the student improve ability instead of just memorizing the solution, the teacher will
generate another analogous question to ask the student. Eventually, the student gives a new
answer for this analogous question and gets a new reward signal from the teacher.

performance of the agents. The left figure of Figure 4 is a toy example of HotpotQA
to illustrate this pattern for collaborating, where the GPT-4 agent play as a thinker
to analyze the situation and give suggestions to the actor agent who is responsible
for making decisions. The reward in HotpotQA is the correctness of the answer
obtained by two agents. And we can use their communication data to train the LTC
agents do both thinker and actor so that they can learn how to cooperate with each
other to solve the task. The right figure of Figure 1 is a toy example of Multi-agent
Dialogue for a competing game task Chameleon, where three agents play different
roles. The reward is the win or loss of the game, so they need do with deduction
and bluffing in the communication process to win the game. And their games
trajectories will be use in LTC iterations to boost the agents.

â¢ Teacher-student Dialogue: Teacher-student Dialogue is a teacher-student style
pattern for powerful agents to teach the novice agents (Algorithm 3). We design
this pattern for complex analytical tasks such as numerical reasoning, which require
extensive analytical examples for agents to improve the specific reasoning ability
lacking in the pretrained models. Teacher-student Dialogue pattern has two roles
(student and teacher) played by two agents, however, in addition to the linguistic
feedback, the teacher roles can directly provide the non-linguistic reward signals,
which are all provided by the system (environments) in the previous pattern. The
right figure of Figure 4 is a toy example with GSM8k to demonstrate how the
student agent communicates with the teacher agent in a homework-correcting style.
In the math question environment, the student agent starts with an initial answer to
the current question, then the teacher directly corrects the answer with a reward.
To help the student improve ability instead of just memorizing the solution, the
teacher will generate another individual question and provide a new reward to the
student.

3.2 Updating phase

In the updating phase, the LLM agent model could be optimized through the conversation
sessions collected in the exploration stage. Given a example session S = (T , M, R), we
mainly utilize two training objects for model training.

â¢ Language model Objective: LLM encourages the model to learn from the trajectory
T , serving as an unsupervised learning schema to help model for behavior cloning
from other agentsâ response or predicting system feedbacks.

5

CollectReinforce learningPromptEnv: You are in the middle of a room. Looking quickly around you, you see a countertop, a fridge â¦ Your task is to: put an egg in the bowl.I need to find and take an egg first, then â¦An egg is more likely to appear in the fridge â¦Open the fridge, I get an egg. Now I need to find the bowl and put the egg into it â¦A bowl is more likely to appear on the counterâ¦Go to the countertop. Put the egg in the bowl.Success, reward +1Few-shotsReply buffersPromptTrainICLCollected trailsAgentAgentTraining-freePrompting-freeEfficient InferenceInefficient InferenceExamples dependentContinual learningLTCLoRAYour task is to: provide a detailed answer so user don't need to search outside to understand the answer â¦ Question: Shawn has five toys. For Christmas, he got two new toys each from his mom and dad. How many toys does he have now?Shawn started with 5 toys. If he got 2 new toys then that is 5+2=7. The answer is 7.Sarah started with 9 pens. If she gives 3 pens to Bob and 3 pens to Tom, then that is 9-3-3=3. The answer is 3.Itâs incorrect. If he got 2 toys from his mom and 2 toys from his dad, then that is 5+2+2=9.Now take this homework problem: Sarah has 9 pens, and she gives 3 pens to each of Bob and Tom. How many pens does Sarah have left?Correct! You have learnt this type of questions.Success, reward +1Fail, reward -1Signal from GPTYour task is to: solve a question answering task with interleaving Thought, Action, Observation steps â¦ Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?Search[Pavel Urysohn]I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find â¦Pavel Samuilovich Urysohn was a Soviet â¦Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.Search[Leonid Levin]Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type â¦Finish[yes]Leonid Levin is a mathematician and computer scientist. So they have the same type of work.Success, reward +1Can replace when testingEnv: You are in the middle of a room. Looking quickly around you, you see a countertop, a fridge â¦ Your task is to: put an egg in the bowl.I need to find and take an egg first, then â¦An egg is more likely to appear in the fridge â¦Open the fridge, I get an egg. Now I need to find the bowl and put the egg into it â¦A bowl is more likely to appear on the counterâ¦Go to the countertop. Put the egg in the bowl.Success, reward +11â¢ Reinforcement Objective: Lreinforce optimizes the model by maximizing the expecta-
tion reward provided by environment or a teacher agent (i.e., GPT-4 OpenAI (2023)).
It is an goal-oriented objective, and allows the model to learn through both positive
and negative signals in the communication session.

Thus, the overall training objective for LTC combines the above two terms:

LLTC(S) = Î²LLM(T ) + Lreinforce(S),
(1)
where Î² is a balancing hyper-parameter. The off-policy PPO algorithm Schulman et al.
(2017) is utilized for optimizing Lreinforce(S), and it can be further breakdown into policy
loss, value loss and policy entropy regularization terms in implementation. The vanilla PPO
algorithm takes the triplet (state, action, rewards) for training. In this case, we sample from
the trajectories (T<i, ti) for simulating the state-action pairs, specifically, we only keep the
tokens generated by agent model itself as actions for policy updating.

4 Experiments

4.1 Datasets

We conducted experiments on four datasets: ALFWorld (Shridhar et al., 2020b), Hot-
potQA (Yang et al., 2018), Chameleon Wu et al. (2023b) and GSM8k (Cobbe et al., 2021). Each
of these datasets represents a different environment type, namely single-agent, multi-agent
collaborating, multi-agent competing, and teacher-student, respectively. And different com-
munication patterns are used: Single-agent Monologue for ALFWorld, Multi-agent Dialogue
for HotpotQA and Chameleon Wu et al. (2023b), and Teacher-student Dialogue for GSM8k.

ALFWorld ALFWorld (Figure 1) is a text-based game that follows the ALFRED bench-
mark (Shridhar et al., 2020a). In this game, agents are presented with six types of tasks that
involve navigating a simulated household environment using textual actions. With over
50 locations to explore, these tasks demand strategic planning and thorough exploration.
Following (Shridhar et al., 2020b), we utilize the train set that consists of 3553 environments
for training our model and the baselines; and we use the unseen test set that comprises 134
environments for evaluatation.

HotpotQA HotpotQA is a question-answering dataset that focuses on multi-hop reasoning
based supporting facts, with the goal of improving the explainability of QA systems. In
this dataset, agents are required to reason across two or more Wikipedia passages to derive
answers. We initialize the environments using only the text of the questions, meaning
that agents are provided with the question and task description but do not have access to
supporting paragraphs. To support their reasoning, agents must either rely on their internal
knowledge or interact with an external Wikipedia tool to retrieve the necessary information.
For training, we sample the environments from the training set, which consists of 90,447
QA-pairs. For evaluation, we run 500 random examples from the test set, following (Yao
et al., 2023).

Chameleon Chameleon is a multi-player social deduction game environment implemented
by the ChatArena Wu et al. (2023b). There are two roles in the game, chameleon and non-
chameleon. The topic of the secret word will be first revealed to all the players. Then
the secret word will be revealed to non-chameleons. Non-chameleons try to identify the
chameleon without giving away the secret word, while the chameleon tries to blend in and
guess the word. The game involves giving clues, voting on who the chameleon might be,
and a final guess from the accused chameleon. We use [3, 4, 5] players setting to train and
test the agentsâ performance.

GSM8k The GSM8k dataset is a collection of 8.5K math problems for grade school students.
These problems have been crafted by human experts to ensure linguistic diversity. The
dataset is divided into two sets: 7.5K problems for training and 1K problems for testing.
Each problem in the dataset requires 2 to 8 steps of reasoning to arrive at the solution. The
problems primarily focus on fundamental arithmetic operations like addition, subtraction,
multiplication, and division.

6

4.2 Settings
Model Architecture We use a modified version of Llama Touvron et al. (2023) as the
base model. To generate state values corresponding to the action tokens, we introduce an
additional linear layer to serve ast the value head. This value head acts as an auxiliary
output module, and the output values are processed using the tanh() function to ensure
they fall within the range of (-1, 1). This adaptation for RL has also been discussed in prior
studies Santacroce et al. (2023).

Agent Pre-training We use the Llama-7B model (Touvron et al., 2023) for our LLM agent.
To enhance the agentâs ability to follow task-specific instructions, we initialize it by instruc-
tion fine-tuning (IT). And this initialized agent works as the baseline for a fair compari-
son. This step is crucial because the original Llama-7B model, without prior instruction
fine-tuning, struggled to follow task instructions and generation sensible actions in the
environments. To collect data for instruction fine-tuning, we employ GPT3/4 as our agent to
explore the environments created from the training set. We then filter out negative examples
and retain positive examples to train the initial agent. For both the ALFWorld and HotpotQA
datasets, we leverage GPT3 (specifically, text-davinci-003). However, for the GSM8k dataset,
we use GPT4 due to GPT3âs inadequate performance in handling mathematical problems,
which resulted in a scarcity of positive examples.

Training details We utilize the AdamW optimizer (Loshchilov & Hutter, 2017) with a batch
size of 32. The learning rate is set to 2e-4. In each iteration, the sizes of new environments
for agents to explore are: 256 for ALFWorld, 512 for GSM8k, and 1024 for HotpotQA. For
parameter-efficient fine-tuning, we employ LoRA Hu et al. (2021) with hyperparameters
R = 16 and Î± = 16. For distributed training, we utilize 4 nodes with 8ÃA100 GPUs on
HotpotQA and GSM8k. For the experiments on ALFWorld, we use 1 node with 2ÃA100 GPUs
due to the datasetâs small scale.

Baselines We compare the agents trained by LTC with existing prompting and instruction
tuning methods, including ReAct (Yao et al., 2023), ReAct-IM (Huang et al., 2022b), CoT (Wei
et al., 2022b), CoT-SC (Wang et al., 2022c;d), BUTLER Micheli & Fleuret (2021). The detailed
of these baselines are described in Appendix A.7. Most of these methods focus on few-shot
prompting, and different pre-trained models are used. To ensure a fair comparison, we
include the additional baselines named ReAct-Tuning and CoT-Tuning by fine-tuning the
Llama-7B model using the collected trajectories as fine-tuning data. In addition, GPT-4 are
not used in the test time, and all the results reported are obtained by the trained agent itself.

65
92

83
96

55
57

60
87

55
62

59
68

39
58

24
41

55
78

57
71

76
86

Look

Pick 2 All

Method \ Task

ReAct (avg)
ReAct (best of 6)

Pick Clean Heat Cool

ReAct-IM (avg)
ReAct-IM (best of 6)

BUTLERg (best of 8)
BUTLER (best of 8)

4.3 Results
ALFWorld As shown in Table 1,
LTC outperforms the previous best
methods* on all of tasks of ALFWorld.
We can see that Instruction Fine-
tuning is already a strong baseline
outperforming others, yet our LTC
achieves a success rate of 91%, re-
markably outperforming the best In-
struction Tuning baseline (78%). No-
tably, on both Cool and Look tasks,
LTC obtains a 100% success rate.
Even on the hardest Pick Two &
Place task (e.g., âput two pencils in
the drawerâ), it achieves a decent 76% success rate. The Pick Two task requires the agent
to perform two sequences of âpick and placeâ actions in one task, while keeping track of
the desired type and the location. The combined sequences and the need to remember
the previous location make this task challenging. This may be the reason why baselines
achieve lower success rates on this task. In contrast, our LTC agent, which further trains the

Table 1: AlfWorld success rates (%) for 6 tasks.
The results of the bottom block are obtained by fine-
tuning Llama-7B model.

ReAct-Tuning (avg)
ReAct-Tuning (best of 3)
LTC (avg)
LTC (best of 3)

72
78
96
100

90
95
97
100

8
24
67
76

77
78
90
91

83
92
89
92

91
96
93
96

91
97
91
97

76
100

17
22

22
37

12
24

26
39

33
46

70
74

48
53

24
33

23
39

*For ALFWorld, ReAct and ReAct-IM results are from Table 3 of Yao et al. (2023). BUTLER and
BUTLERg results are from Table 4 of Shridhar et al. (2020b), and they are trained with DAgger Ross
et al. (2011).

7

agent with self-exploration significantly outperforms other agents. This underscores the
effectiveness of the communication mechanism in LTC.

HotpotQA As shown in Table 2, LTC outperforms
the instruction tuning baselineâ  by 5% on Exact
Match (EM) score, and it even outperforms ReAct
and CoT on their default settings. Note that Re-
Act and CoT use PaLM-540B and GPT3-175B as
the pre-trained LM model, which is 77x and 25x
larger than our the Llama-7B model we used. By
sampling 21 CoT trajectories during inference and
adopting the majority answer, CoT-SC is slightly
better (0.2%) than LTC, and their combined method
ReAct â CoT-SC surpasses LTC by 1.9%. Com-
pared to other models with tuning, our Llama-7B
based agent even obtains slightly better (0.6%) per-
formance than the ReAct-Tuning baseline with 9Ã
larger PaLM-62B model.

Model

Method

EM score

PaLM-540B

CoT (Wei et al., 2022b)
CoT-SC (Wang et al., 2022c)
ReAct (Yao et al., 2023)
ReAct â CoT-SC

GPT3-175B

ReAct

PaLM-62B

PaLM-8B

Llama-7B

Llama2-13B

ReAct-Tuning
CoT-Tuning

ReAct-Tuning
CoT-Tuning

ReAct-Tuning
LTC(single-agent monologue)
LTC(multi-agent dialogue)

ReAct-Tuning
LTC(multi-agent dialogue)

29.4
33.4
27.4
35.1

30.8

32.6
25.2

25.0
14.0

28.2
31.0
33.2

33.8
35.8

Chameleon As shown in Table 3, LTC outper-
forms the instruction tuning baselines by 3.1% on
winning rate against GPT-4 players. In the training,
all the players are played by the same Llama2-7B
model that we are training. While in the testing, to get the winning rate of our trained agent
against GPT4, only 1 player is randomly picked to use our trained agent as backend, and
other players are played by GPT4. We could see that the LTC agents winning rate improves
with the increasing of number of players, we explain this by the more players, the higher
chance that the GPT4 players carry the game.

EM scores on HotpotQA
Table 2:
with prompt and tuning methods.
Methods that use fine-tuning are
marked by â-Tuningâ.

GSM8k As shown in Table 4, LTC(teacher-student
dialogue) outperforms the instruction fine-tuning
baseline by 3.6% on accuracy, and it surpasses
the LTC(single-agent monologue) baseline, which does
not use the reward and feedback from GPT-4.

However, LTC underperforms CoT and CoT-SC
with the much larger models (PaLM-540B and
GPT3-175B). This phenomenon is because numer-
ical reasoning requires a larger model size and suf-
ficient pretraining data, as observed in OpenAI
(2023). Unfortunately, due to computational re-
source limitations, we can only train the relatively
small Llama-7B model but were unable to train
larger-scale models. Nevertheless, we believe that
exploring LTC with larger models is promising for
future research.

5 Discussion

Method \ #players

Llama-Tuning
Llama-LTC

n=3

20.8
22.9

n=4

20.3
23.4

n=5

23.8
27.5

overall

21.9
25.0

Table 3: Chameleon game winning rates
(%) of different numbers of players.
Method

Accuracy

Model

PaLM-540B

GPT3-175B

Llama-7B

Llama-7B

CoT (Wei et al., 2022b)
CoT-SC (Wang et al., 2022c)

CoT (Wei et al., 2022b)
CoT-SC (Wang et al., 2022c)

CoT (Touvron et al., 2023)
CoT-SC (Touvron et al., 2023)

CoT-Tuning
LTC(single-agent monologue)
LTC(teacher-student dialogue)

56.5
74.4

60.1
78.0

11.0
18.1

37.7
39.6
41.3

Table 4: Accuracy on GSM8k. The re-
sults of the bottom block are obtained
by fine-tuning, while the others are
prompting methods.

Shortcuts One interesting observation is that the
GPT-4 agent sometimes employs âshortcutsâ to solve problems when serving as a teacher
to generate new training data. These shortcuts rely on the internal knowledge acquired
during its pretraining process. To illustrate this, we present a case study from HotpotQA
in Figure 7. In this case, the GPT-4 agent quickly retrieves the answer by leveraging its
memorized knowledge about the second entry after receiving the Wikipedia page of the
first entry. On the other hand, the bottom of Figure 7 demonstrates a comparison with
LLaMA-7B, which was trained using our LTC method with the GPT-4 agent in the loop.

â For HotPotQA, Prompting method results without fine-tuning are from Table 1&5 of Yao et al.

(2023). PaLM-8B and PaLM-62B scores are estimates from Figure 3 of Yao et al. (2023).

8

Figure 5: The accuracy curves of training.

Figure 6: The loss curves of training.

LLaMA-7B does not employ shortcuts and instead performs a search for the second entry.
This case study demonstrates that communication mechanism in LTC provide additional
benefits during learning, compared to soley relying on data generated by GPT-4.

ICL

(CoT)
836

Method

GSM8k Hotpot-QA Alfworld
(ReAct)
1937

Efficiency As mentioned above, prompting-
based methods such as ReAct (Yao et al., 2023)
and CoT (Wei et al., 2022b) use a subset of ex-
emplary trajectories from the given task as
few-shot prompts during inference. How-
ever, these few-shot prompts are often long,
which leads to increased inference cost and
limited context length for user queries. As
shown in Table 5, we compare the number
of input tokens for each task. We compute
the CoT prompts for GSM8k, and we use Re-
Act for the other two tasks. All the few-shot
prompts are sourced from the original paper.
As shown, our LTC agents used only 12.8%, 8.6%, and 10.8% of the input tokens required by
the ICL methods on the three tasks, respectively.

Table 5: Average number of tokens of the
input prompts on test sets. LTC does not
use any few shot examples in the prompt,
hence uses only a fraction of tokens com-
pared to ICL.

(ReAct)
1744

LTC

189

107

167

Ablation We conducted ablation studies on the loss design of LTC. Figure 5 illustrates the
success rate of agents on the ALFWorld dataset under different loss settings. Without using
our communication pattern for interactions and merely sampling pre-collected instruction
data for training, the improvement was limited. However, when we incorporated our
communication pattern to gather data, the modelâs performance quickly surpassed 80%.
Furthermore, employing PPO loss to handle positive and negative samples separately
resulted in faster and more significant improvement (blue line). In Figure 6, we present the
separate curves of the three main losses during training. Initially, the LM loss showed a
decreasing trend. Interestingly, as training iterations progressed, both the value loss and
policy loss gradually decreased, which possibly causes the LM loss to increase temporarily.
After the value loss and policy loss reached a certain threshold, the LM loss continued to
decrease till convergence.

6 Conclusion

We introduced Learning-Through-Communication (LTC), a paradigm that adapts LLM
agents to new tasks and environments via communication-based iterative learning. Within
this LTC framework, we have designed three communication modes for common tasks
including decision-making, knowledge-intensive reasoning, and numeric reasoning. These
communication modes facilitate interactions between LLM agents and their environments,
as well as other agents such as GPT-4 and humans. The history of these interactions can
be autonomously organized into training data for PPO training so that the agent can adapt
to the new task. Our approach represents a closed loop where the agent self-interacts
with the environment or other agents, and learning to improve itself with minimal human
intervention. Empirically, we have demonstrated that LTC performs strongly in success rate
and efficiency across four different tasks: AlfWorld, HotpotQA, Chameleon, and GSM8k. It
consistently outperforms existing LLM agent and instruction tuning baselines, showing the
promise of the LTC paradigm in adapting LLM agents to new tasks and environments with

9

012345678N iter0.650.700.750.800.850.90AccIterative PPO Alfworld Accw/ PPO&Interactionw/o PPOw/o Interaction01000200030004000N episode0.000.050.100.150.200.250.300.35LossAlfworld LossLM lossValue lossPolicy lossminimal human effort. As for future work, we plan to explore more diverse communication
patterns for different tasks, and involve the communication with human during the iterative
learning process. We will open source our code to facilitate further research in this line.

References

Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz.
Playing repeated games with large language models. arXiv preprint arXiv:2305.16867,
2023.

AntonOsika.

gpt-engineer.

https://github.com/AntonOsika/gpt-engineer, 2023.

GitHub repository.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and
harmless assistant with reinforcement learning from human feedback. arXiv preprint
arXiv:2204.05862, 2022a.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy
Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitu-
tional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini
Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya
Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam
McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are
few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877â1901. Curran
Associates, Inc., 2020.

SÂ´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz,
Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial
general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org
(accessed 14 April 2023), 2023.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,
et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311,
2022.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,
Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned
language models. arXiv preprint arXiv:2210.11416, 2022.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168, 2021.

Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun,

and Yu Su. Mind2web: Towards a generalist agent for the web, 2023.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie
Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint
arXiv:2211.10435, 2022.

10

Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili
Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and
Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework,
2023.

Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions:
Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689,
2022.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021.

Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and

Jiawei Han. Large language models can self-improve, 2022a.

Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,
Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown,
Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter.
Inner
monologue: Embodied reasoning through planning with language models. In arXiv
preprint arXiv:2207.05608, 2022b.

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,
Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot
learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299,
2022.

Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop,
Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human
feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.

Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable

agent alignment via reward modeling: a research direction, 2018.

Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard
Ghanem. Camel: Communicative agents for âmindâ exploration of large scale language
model society, 2023.

Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu
Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent
with fast and slow thinking for complex interactive tasks. ArXiv preprint, abs/2305.17390,
2023. URL https://arxiv.org/abs/2305.17390.

Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang,
and Soroush Vosoughi. Training socially aligned language models in simulated human
society. arXiv preprint arXiv:2305.16960, 2023a.

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang
Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao
Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang,
Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:
2308.03688, 2023b.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,
Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods
for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint

arXiv:1711.05101, 2017.

Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun
Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large
language models. arXiv preprint arXiv:2304.09842, 2023.

11

Vincent Micheli and FrancÂ¸ois Fleuret. Language models are few-shot butlers. arXiv preprint

arXiv:2104.07972, 2021.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. In International conference on machine learning, pp. 1928â1937.
PMLR, 2016.

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,
Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al.
Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786,
2022.

OpenAI. Chatgpt. Online, 2022. URL https://openai.com/blog/chatgpt.

OpenAI. Gpt-4 technical report. OpenAI blog, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language
models to follow instructions with human feedback. Advances in Neural Information
Processing Systems, 35:27730â27744, 2022.

Joon Sung Park, Joseph C. OâBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and
Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In In
the 36th Annual ACM Symposium on User Interface Software and Technology (UIST â23), UIST
â23, New York, NY, USA, 2023. Association for Computing Machinery.

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction

tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.

Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu,
and Maosong Sun. Communicative agents for software development. arXiv preprint
arXiv:2307.07924, 2023.

Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi

Yang. Is chatgpt a general-purpose natural language processing task solver?, 2023.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.

Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward
model, 2023.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a
unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1â67, 2020.

StÂ´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning
and structured prediction to no-regret online learning. In Proceedings of the fourteenth
international conference on artificial intelligence and statistics, pp. 627â635. JMLR Workshop
and Conference Proceedings, 2011.

Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, and Yelong Shen. Efficient rlhf:

Reducing the memory usage of ppo, 2023.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÂ´c, Daniel Hesslow, Ro-
man CastagnÂ´e, Alexandra Sasha Luccioni, FrancÂ¸ois Yvon, Matthias GallÂ´e, et al. Bloom: A
176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,
2022.

Timo Schick, Jane Dwivedi-Yu, Roberto Dess`Ä±, Roberta Raileanu, Maria Lomeli, Luke
Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can
teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.

12

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal

policy optimization algorithms, 2017.

Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke
Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models.
arXiv preprint arXiv:2301.12652, 2023.

Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and

Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.

Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh
Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting
grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pp. 10740â10749, 2020a.

Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C ËotÂ´e, Yonatan Bisk, Adam Trischler, and
Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interac-
tive learning. arXiv preprint arXiv:2010.03768, 2020b.

Significant-Gravitas. Autogpt. https://github.com/Significant-Gravitas/AutoGPT,

2023. GitHub repository.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec
Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human
feedback. Advances in Neural Information Processing Systems, 33:3008â3021, 2020.

Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths. Cognitive

architectures for language agents. arXiv preprint arXiv:2309.02427, 2023.

Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power

of intelligent llm agents. arXiv preprint arXiv:2306.03314, 2023.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,
Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-
following model. Stanford Center for Research on Foundation Models. https://crfm. stanford.
edu/2023/03/13/alpaca. html, 3(6):7, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
TimothÂ´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,
2023.

Ruoyao Wang, Peter Jansen, Marc-Alexandre C ËotÂ´e, and Prithviraj Ammanabrolu. Science-
world: Is your agent smarter than a 5th grader?, 2022a. URL https://arxiv.org/abs/
2203.07540.

Xingyao Wang, Sha Li, and Heng Ji. Code4struct: Code generation for few-shot structured

prediction from natural language. arXiv preprint arXiv:2210.12810, 2022b.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in
language models, 2022c. URL https://arxiv.org/abs/2203.11171.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-

augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022d.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated
instructions. arXiv preprint arXiv:2212.10560, 2022e.

Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik,
David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions
on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022f.

13

Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Un-
leashing cognitive synergy in large language models: A task-solving agent through
multi-persona self-collaboration. arXiv preprint arXiv:2307.05300, 2023.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,
Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large
language models, 2022a.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv
preprint arXiv:2201.11903, 2022b.

Lilian Weng. Llm-powered autonomous agents. lilianweng.github.io, Jun 2023. URL https:

//lilianweng.github.io/posts/2023-06-23-agent/.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist rein-

forcement learning. Machine learning, 8:229â256, 1992.

Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin
Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications
via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023a.

Yuxiang Wu, Zhengyao Jiang, Akbir Khan, Yao Fu, Laura Ruis, Edward Grefenstette, and
Tim RocktÂ¨aschel. Chatarena: Multi-agent language game environments for large language
models. https://github.com/chatarena/chatarena, 2023b.

Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan
Xu. Rewoo: Decoupling reasoning from observations for efficient augmented language
models. arXiv preprint arXiv:2305.18323, 2023.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhut-
dinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable
multi-hop question answering. In Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2018.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and
Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh
International Conference on Learning Representations, 2023. URL https://openreview.net/
forum?id=WE_vluYUL-X.

yoheinakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023. GitHub

repository.

14

Algorithm 1 The Python-style algorithm to demonstrate Monologue pattern

# agent: LLaMA agent
# input: Task description
# output: S = (T, M, R)

# initialization
T, M, R = [input], [0], [0]

i = 0
while i < max_steps:

T += ["think:"]
thought = agent.api(T)
T.append(thought)
M.append(1) # agent message mask
R.append(0)

T += ["act:"]
action = agent.api(T)
T.append(action)
M.append(1) # agent message mask
R.append(0)

response = env.excute(action)
reward = parse(response)
T.append(response)
M.append(0) # system message mask
R.append(reward)

i += 1
if reward != 0:

break
S = (T, M, R)
return S

A Appendix

A.1 Communication Patterns

To collect the trajectories and the reward signal data from different types of tasks, we design
the communication patterns for these tasks and unified the data format as described in
Figure 2. Here we use three python-sytle algorithms (Algorithm 1 Algorithm 2 Algorithm 3)
to demonstrate how three types of communication patterns help the agent collect exploration
data.

A.2 Buffer Structure

The communication data will be saved as replay buffers for the updating phase, and the
buffer data format is a serial of tokens sequences demonstrated in Figure 2. We treat each
token as the action unit in our reinforcement learning formula, and each exploration trail is
processed into 5 data sequences [Sa, Sm, Sv, Sl, Sr]:

â¢ Sa: A list of integers representing the generated token ids encoded by the tokenizer.
All the valid text trajectories are recorded as a queue, including system texts like
environment descriptions, feedback, and agent texts like parsed actions, thinking
processes, and hints from other agents. While the invalid generated text of the agent
will be skipped, such as nonsense string and action text can not be parsed. These
tokens are treated equally as the input for the LLM, but they have different masks
to apply different losses.

â¢ Sm: The system mask to mask different types of input tokens to control the training
loss. We set 0 as the default mask for system texts like environment descriptions,
system feedback, and system prompts, the actions encoded from these kinds of texts
are not actions we want the agent to learn, so they will be masked out both policy
loss and value loss in the PPO algorithm. We set 1 as the mask for agents-generated
tokens like the keywords of decisions and the thinking process, which are the main
supervising objects of our reinforcement learning pipeline, so they will be assigned
full policy loss and value loss. We set 2 as the mask for hints or feedback from other
agents, which are the actions we also want our own agent to learn but without

15

Algorithm 2 The Python-style algorithm to demonstrate Dialogue pattern

# agent1: LLaMA agent
# agent2: GPT-4 agent
# input: Task description
# output: S = (T, M, R)

# initialization
T, M, R = [input], [0], [0]

i = 0
while i < max_steps:

T += ["think:"]
thought = agent2.api(T)
T.append(thought)
M.append(2) # teacher agent message mask
R.append(0)

T += ["act:"]
action = agent1.api(T)
T.append(action)
M.append(1) # student agent message mask
R.append(0)

response = env.excute(action)
reward = parse(response)
T.append(response)
M.append(0) # system message mask
R.append(reward)

i += 1
if reward != 0:

break
S = (T, M, R)
return S

Algorithm 3 The Python-style algorithm to demonstrate Analogue pattern

# agent1: LLaMA agent
# agent2: GPT-4 agent
# input: Question description
# output: S = (T, M, R)

# initialization
T, M, R = [input], [0], [0]

i = 0
while i < max_steps:

T += ["answerâ£theâ£questionâ£stepâ£byâ£step:"]
answer1 = agent1.api(T)
query = T + answer1 + ["theâ£answerâ£isâ£correct,â£yesâ£orâ£no?â£alsoâ£givesâ£aâ£betterâ£answer"]
response = agent2.api(query)
reward, answer2 = parse(response)
T.append(answer1)
T.append(answer2)
M.append(1) # student agent message mask
M.append(2) # teacher agent message mask
R.append(reward)
R.append(+1) # assume teacher is correct

query = query + response + ["pleaseâ£generateâ£aâ£similarâ£qaâ£pairâ£toâ£teachâ£theâ£student:"]
response = agent2.api(query)
new_question, teacher_answer = parse(response)
new_question += "answerâ£theâ£questionâ£stepâ£byâ£step:"
student_answer = agent1.api(new_question)
reward = parse(student_answer, teacher_answer)
T.append(new_question + student_answer)
M.append(1) # student agent message mask
R.append(reward)

i += 1

S = (T, M, R)
return S

instant state values since they are not generated by our agent. So the tokens with
mask 2 will be mask out only the value loss and supervised by the policy loss.

â¢ Sv: The state values corresponding to the actions obtained by the value head our the
agent model. The value head is an addition layer inserted to the original pre-trained
LLM architecture, we implement it by inserting a linear layer after the second-to-

16

last LlamaDecoderLayer as the auxiliary output module and the output values are
processed by a tanh() function to keep it range inside (â1, 1).

â¢ Sr: The rewards corresponding to the actions. The rewards are very sparse, most of
the actions are zero-reward, and only when the current task is finished or the token
length of the current buffer has just overflowed it will be non-zero value: +1 for
positive, -1 for negative.

A.3 Algorithm of LTC

Algorithm 4 Python-style code of LTC

# initialization
agent = instruction_finetune(agent)
replay_buffer = []
i = 0
while i < max_iteration:

# agent: Pre-trained LLM agent
# n_gpu: total number of GPUs
# env_cls: the class of environments
# n_gen: the generation size for one iteration
# n_train: the train size for one iteration

The implementation of LTC can be sum-
marized as Algorithm 4, we unveil the
structural framework that embodies the
Learning Through Communication (LTC)
paradigm, meticulously crafted to facili-
tate autonomous and progressive learning
through iterative cycles. At the outset, the
pre-trained Large Language Model (LLM)
agent undergoes a fine-tuning phase to
align its initial configuration with the in-
tended learning tasks. Following this, an ex-
ploration phase is initiated where a batch of
training environments is sampled and sub-
sequently engaged by the agent to generate
trial data asynchronously, leveraging the
power of parallel computing across multi-
ple GPUs to enhance efficiency. This newly
generated data is synchronized across all
GPUs to foster a cohesive learning base,
which is stored in a replay buffer for further
utilization. As the agent iterates through
this process, it continually samples from this replay buffer during the updating phase,
employing the Proximal Policy Optimization (PPO) algorithm in a distributed data parallel
(DDP) setting to refine its strategies and adapt dynamically. This code snippet, therefore,
encapsulates the innovative asynchronous and distributed nature of the LTC paradigm,
marking a significant stride in fostering intelligent, adaptive, and collaborative artificial
intelligence agents.

i += 1
# Exploration Phase
envs = env_cls(sample(data, n_gen//n_gpu))
# asynchronously generate
new_buffer = generate_trials(agent, envs)
# dist.gather and dist.broadcast
new_buffer = sync_all_gpus(new_buffer)
replay_buffer.append(new_buffer)

# Training Phase
rollouts = sample(replay_buffer, n_train))
# distributed training with ppo
agent = ppo_ddp_train(agent, rollouts)

A.4 Training Loss

After exploration in each iteration, we update the replay buffer by incorporating the newly
collected trajectories and then sample the most recent trajectories to train the parameters ÏÎ¸
of the LLM agent. We design our training objective to combine: 1) the standard language
modeling loss LLM, 2) the policy loss Lpolicy, 3) the value loss Lvalue, and 4) the entropy loss
Lentropy. The overall training objective is formulated as:

Ltotal = LLM + Î²(Lpolicy + Î»Lvalue + Lentropy)

where Î² and Î» are weighting hyperparameters.

The different losses in the above are described as follows:

â¢ The LM loss LLM is defined as the cross entropy between the agent and its genera-
tions which have a positive reward, akin to self-improving model schemes Huang
et al. (2022a); Rafailov et al. (2023). By training on these generations, the agent is
further encouraged to give generations which yield positive rewards.

â¢ The policy loss Lpolicy is introduced to supervise the agentâs actions. The policy loss
Lpolicy is calculated using a masked version of the surrogate objective defined in
Schulman et al. (2017) with advantage estimates ËA,

Lpolicy(Î¸) = âE[mpolicy â min(r(Î¸) ËA, clip(r(Î¸), 1 â Ïµ, 1 + Ïµ) ËA],

17

Figure 7: GPT-4 can use shortcuts to solve the problem, while the LLaMA-7B agent cannot
mimic it.

where r(Î¸) is the output probability ratio r(Î¸) = ÏÎ¸ (a | s)
Ïold(a | s) of the agent with its
previous version Ïold. We define binary mask mpolicy to mask out the encoded
system message in PPO loss (marked by Sm = 0 in buffers A.2). For example, let
{x1, y1, x2, y2, . . . xn, yn} be a token buffer consisting of system messages xn â X
and agentsâ messages (include the target trained agent and the other teacher agents)
ÏÎ¸ output yn â Y, then the binary mask mpolicy = {0, 1, 0, 1, . . . 0, 1}.

â¢ The value loss is defined in Schulman et al. (2017) as the mean squared error between
calculated value and estimated advantages masked by another binary mask mvalue
(marked by Sm = 1 in buffers A.2). For example, let {z1, y1, z2, y2, . . . zn, yn} be a
token buffer consisting of all other messages (except the agent-generated messages)
zn â X and trained agent-generated messages ÏÎ¸ output yn â Y, then the binary
mask mpolicy = {0, 1, 0, 1, . . . 0, 1}.

â¢ Lentropy is an entropy bonus to ensure sufficient exploration, as suggested in past
work (Williams, 1992; Mnih et al., 2016). This entropy is computed as a small
negative factor times the entropy of the policy distribution : Lentropy = 0.01 Ã
âa ÏÎ¸(a|s) log ÏÎ¸(a|s).

18

Your task is to: solve a question answering task with interleaving Thought, Action, Observation steps â¦ Question: The Atik Valide Mosque and Valens Aqueduct are found in what country?I need to search Atik Valide Mosque and Valens Aqueduct, find the country they are found in.Search[Atik Valide Mosque]The Atik Valide Mosque is located in Istanbul, Turkey. I need to search Valens Aqueduct and find the country it is found in.The Atik Valide Mosque (Turkish: Atik Valide Camii, Eski Valide Camii) is a 16th century Ottoman imperial mosque located on a hill above the large and densely populated district of ÃskÃ¼dar, in Istanbul, Turkey â¦Finish[Turkey]The Aqueduct of Valens (Turkish: Valens Su Kemeri, Ancient Greek: á¼Î³ÏÎ³á½¸Ï ÏÎ¿á¿¦ á½Î´Î±ÏÎ¿Ï, romanized: AgÅgÃ³s tou hÃ½datos, lit.â'aqueduct') was a Roman aqueduct system built in the late 4th century AD, to supply Constantinople â the capital of the eastern Roman empire. Construction of the aqueduct began during â¦Search[Valens Aqueduct]The Valens Aqueduct is located in Constantinople, Turkey. So the answer is Turkey.Replace GPT-4 with our LLaMA-7B based agent.Your task is to: solve a question answering task with interleaving Thought, Action, Observation steps â¦ Question: The Atik Valide Mosque and Valens Aqueduct are found in what country?The question refers to two specific landmarks: the Atik Valide Mosque and the Valens Aqueduct. To identify the country where these landmarks are found, I'll start by searching for information on the Atik Valide Mosque, which may be unique enough to easily pinpoint the country.Search[Atik Valide Mosque]The observation confirms that the Atik Valide Mosque is located in Istanbul, Turkey. Although I haven't checked the Valens Aqueduct specifically, it's safe to assume that the question is referring to the well-known Valens Aqueduct in Istanbul. I can confidently answer that both landmarks are found in Turkey.Finish[Turkey]The Atik Valide Mosque (Turkish: Atik Valide Camii, Eski Valide Camii) is a 16th century Ottoman imperial mosque located on a hill above the large and densely populated district of ÃskÃ¼dar, in Istanbul, Turkey â¦GPT-4 uses some shortcuts based on its internal knowledge.Your task is to: solve a question answering task with interleaving Thought, Action, Observation steps â¦ Question: The Atik Valide Mosque and Valens Aqueduct are found in what country?The question refers to two specific landmarks: the Atik Valide Mosque and the Valens Aqueduct. To identify the country where these landmarks are found, I'll start by searching for information on the Atik Valide Mosque, which may be unique enough to easily pinpoint the country.Search[Atik Valide Mosque]The observation confirms that the Atik Valide Mosque is located in Istanbul, Turkey. Although I haven't checked the Valens Aqueduct specifically, it's safe to assume that the question is referring to the well-known Valens Aqueduct in Istanbul. I can confidently answer that both landmarks are found in Turkey.Finish[Turkey]The Atik Valide Mosque (Turkish: Atik Valide Camii, Eski Valide Camii) is a 16th century Ottoman imperial mosque located on a hill above the large and densely populated district of ÃskÃ¼dar, in Istanbul, Turkey â¦GPT-4 uses some shortcuts based on its internal knowledge.3A.5

Implementation detail

A.6 Asynchronously Distributed Generating

The exploration data is generated in an asynchronous style, so that the agent can handle the
environments with open-end exploration space. The training data are pre-processed into
interactive environments which are capable for agents to observe the states, take actions,
and get immediate feedback. According to the number of GPU threads, these environments
are divided into corresponding portions and then distributed to each GPU. Subsequently,
these GPUs begin to explore these environments asynchronously in parallel with the same
agent trained by the latest data. Since the lengths of the generated contents are varied
and the interactions inside the environments are generally open-ended, the time cost for
the agent to explore each environment is also varied, some GPU threads may process the
data faster than others. A barrier is set for all the GPU threads so that the early finished
GPU threads can wait for the others until the total accumulated buffers generated by the
environments achieve a preset number Sg, which is the quantity of the new training buffers
we want to add to the replay buffers in one iteration. After all the GPU threads reach the
barrier, we get enough buffers then gather the buffers from each GPU thread and merge
them together, and broadcast the new buffers to each GPU thread to update their local
replay buffers. The updated replay buffers will be used in the updating phase for training
the agents of the next iteration.

A.7 Baselines

ReAct (Yao et al., 2023) uses a subset of training cases as prompts for different tasks, in the
format of thought-action-observation sequences. For knowledge-intensive reasoning tasks
like HotpotQA, ReAct designs an action space that includes search, lookup, and finish actions,
enabling the agent to interact with Wikipedia to retrieve necessary information. On the
other hand, ReAct-IM adopts Inner Monologue (IM) (Huang et al., 2022b) style prompting.
Chain-of-thought prompting (CoT) (Wei et al., 2022b), enhances the reasoning capabilities
of Language and Vision models (LLMs) by generating a sequence of intermediate reasoning
steps. This can be considered as a reasoning-only baseline of ReAct, excluding actions
and observations. Additionally, CoT-SC (Wang et al., 2022c;d) is a follow-up work of CoT,
serving as a self-consistency baseline. It is worth noting that most of these methods employ
greedy decoding, except for BUTLER Micheli & Fleuret (2021), which utilizes beam search.
Most of these methods focus on few-shot prompting, and different pre-trained models are
used. To ensure a fair comparison, we include the additional baselines named ReAct-Tuning
and CoT-Tuning by fine-tuning the LLaMA-7B model using the collected trajectories as
fine-tuning data mentioned in 4.2. In addition, GPT-4 are not used in the test time, and all
the results reported are obtained by the trained agent itself.

A.8 Losses

We conducted ablation studies on the loss design of LTC. Figure Figure 5 illustrates the
success rate of agents on the ALFWorld dataset under different loss settings. Without using
our communication pattern for interactions and merely sampling pre-collected instruction
data for training, the improvement was limited. However, when we incorporated our
communication pattern to gather data, the modelâs performance quickly surpassed 80%.
Furthermore, employing PPO loss to handle positive and negative samples separately
resulted in faster and more significant improvement (blue line). In Figure Figure 6, we
present the separate curves of the three main losses during training. Initially, the LM loss
showed a decreasing trend. Interestingly, as training iterations progressed, both the value
loss and policy loss gradually decreased, which possibly causes the LM loss to increase
temporarily. After the value loss and policy loss reached a certain threshold, the LM loss
continued to decrease till convergence.

19

Figure 8: The text version of the toy example in Figure 1 for Monologue pattern of ALF-
World.
A.9 Cases study

One interesting observations is that the GPT-4 agent sometimes employs âshortcutsâ to
solve problems when serving as a teacher to generate new training data. These shortcuts
rely on the internal knowledge acquired during its pretraining process. To illustrate this, we
present a case study from HotpotQA in Figure Figure 7. In this case, the GPT-4 agent quickly
retrieves the answer by leveraging its memorized knowledge about the second entry after
receiving the Wikipedia page of the first entry. On the other hand, the bottom of Figure
Figure 7 demonstrates a comparison with LLaMA-7B, which was trained using our LTC
method with the GPT-4 agent in the loop. LLaMA-7B does not employ shortcuts and instead
performs a search for the second entry. This case study demonstrates that communication
mechanism in LTC provide additional benefits during learning, compared to soley relying
on data generated by GPT-4.

20

