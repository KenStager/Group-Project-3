1
2
0
2

v
o
N
9

]

G
L
.
s
c
[

4
v
7
4
4
9
0
.
6
0
0
2
:
v
i
X
r
a

Agent Modelling under Partial Observability for
Deep Reinforcement Learning

Georgios Papoudakis

Filippos Christianos

Stefano V. Albrecht

School of Informatics
University of Edinburgh
{g.papoudakis, f.christianos, s.albrecht}@ed.ac.uk

Abstract

Modelling the behaviours of other agents is essential for understanding how agents
interact and making effective decisions. Existing methods for agent modelling
commonly assume knowledge of the local observations and chosen actions of
the modelled agents during execution. To eliminate this assumption, we extract
representations from the local information of the controlled agent using encoder-
decoder architectures. Using the observations and actions of the modelled agents
during training, our models learn to extract representations about the modelled
agents conditioned only on the local observations of the controlled agent. The
representations are used to augment the controlled agentâs decision policy which is
trained via deep reinforcement learning; thus, during execution, the policy does
not require access to other agentsâ information. We provide a comprehensive
evaluation and ablations studies in cooperative, competitive and mixed multi-agent
environments, showing that our method achieves higher returns than baseline
methods which do not use the learned representations.

1

Introduction

An important aspect of autonomous decision-making agents is the ability to reason about the un-
known intentions and behaviours of other agents. Much research has been devoted to this agent
modelling problem [Albrecht and Stone, 2018], with recent works focused on learning informative
representations about another agentâs policy using deep learning architectures for agent modelling and
reinforcement learning (RL) [He et al., 2016, Raileanu et al., 2018, Grover et al., 2018, Rabinowitz
et al., 2018, Zintgraf et al., 2021].

A common assumption in existing methods is that the modelling agent has access to the local
trajectory of the modelled agents during execution [Albrecht and Stone, 2018], which may include
their local observations of the environment state and their past actions. While it is certainly desirable
to be able to observe the agentsâ local contexts in order to reason about their past and future decisions,
in practice such an assumption may be too restrictive. Agents may only have a limited view of their
surroundings, communication with other agents may be infeasible or unreliable [Stone et al., 2010],
and knowledge of the perception system of other agents may be unavailable [Gmytrasiewicz and
Doshi, 2005]. In such cases, an agent must reason with only locally available information.

We consider the following question: Can effective agent modelling be achieved using only the
locally available information of the controlled agent during execution? A strength of deep learning
techniques is their ability to identify informative features in data. Here, we use deep learning
techniques to extract informative features about the trajectory of the modelled agent from a stream of
local observations for the purpose of agent modelling. Speciï¬cally, we consider a multi-agent setting
in which we control one agent which must learn to interact with a set of other agents. We assume a
set of possible policies for the non-learning agents and that these policies are ï¬xed.

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
We propose Local Information Agent Modelling (LIAM)1, which can be seen as the general idea
of learning the relationship between the trajectory of the controlled agent and the trajectory of the
modelled agent. In this work we propose one instantiation of this idea: an encoder-decoder agent
modelling method that can extract a compact yet informative representation of the modelled agents
given only the local information of the controlled agent (its local state observations, and past actions).
The model is trained to replicate the observations and actions of the modelled agents from the local
information only. During training, the modelled agentâs observations and actions are utilised as
reconstruction targets for the decoder; after training, only the encoding component is retained which
generates representations using local observations of the controlled agent. The learned representation
conditions the policy of the controlled agent in addition to its local observation, and the policy and
model are optimised concurrently during the RL learning process.

We evaluate LIAM in three different multi-agent environments: double speaker-listener [Mordatch and
Abbeel, 2017, Lowe et al., 2017], level-based foraging (LBF) [Albrecht and Stone, 2017, Papoudakis
et al., 2021], and a modiï¬ed version of predator-prey proposed by [BÃ¶hmer et al., 2020]. Our
results support the idea that effective agent modelling can be achieved using only local information
during execution: the same RL algorithm generally achieved higher average returns when combined
with representations generated by LIAM than without, and in some cases the average returns are
comparable to those achieved by an ideal baseline which has access to the modelled agentâs trajectory
during execution. We also provide detailed evaluations of the learned encoder and decoder of LIAM
as well as comparison with different instantiations of LIAM.

2 Related Work

Learning Agent Models: We are interested in agent modelling methods that use neural networks
to learn representations of the other agents. He et al. [2016] proposed a method which learns a
modelling network to reconstruct the modelled agentâs actions given its observations. Raileanu et al.
[2018] developed an algorithm for learning to infer an agentâs intentions using the policy of the
controlled agent. Grover et al. [2018] proposed an encoder-decoder method for modelling the agentâs
policy. The encoder learns a point-based representation of different agent trajectories, and the decoder
learns to reconstruct the modelled agentâs policy. Rabinowitz et al. [2018] proposed the Theory of
mind Network (TomNet), which learns embedding-based representations of modelled agents for
meta-learning. Tacchetti et al. [2019] proposed relational forward models to model agents using
graph neural networks. [Zintgraf et al., 2021] uses a VAE for agent modelling for fully-observable
tasks. All these aforementioned works either assume that the controlled agent has direct access
to the trajectories of the modelled agent during execution or that the evaluation environments are
fully-observable. Xie et al. [2020] learned latent representations from local information to inï¬uence
the modelled agents, however, in contrast to our work, they did not utilise the modelled agentâs
trajectories. Finally, agent modelling from local information has been researched under the I-POMDP
model [Gmytrasiewicz and Doshi, 2005] and in the Poker domain research. In contrast to our work,
I-POMDPs utilise recursive reasoning [Albrecht and Stone, 2018] which assumes knowledge of the
observation models of the modelled agents (which is unavailable in our setting). In the Poker domain,
Johanson et al. [2008], Bard et al. [2013] created a mixture-of-expert counter-strategies during
training against several type of opponent policies, while during execution they cast the selection of
the best counter-strategy against a speciï¬c opponent type as a bandit problem. The main difference
between LIAM and these works is that the latter adapt (to select the best strategy) over a number of
hands (we consider that a hand is equivalent to an episode) against each opponent. In contrast, in our
work we use a single episode for adaptation.

Representation Learning in Reinforcement Learning: Another related topic which has received
signiï¬cant attention is representation learning in RL. Using unsupervised learning techniques to learn
low-dimensional representations of the environment state has led to signiï¬cant improvements both in
the returns as well as the sample efï¬ciency of the RL. Recent works on representation learning focus
on learning world models and use them to train the RL algorithm [Ha and Schmidhuber, 2018, Hafner
et al., 2020] or learning state representations for improving the sample efï¬ciency of RL algorithms
using reconstruction-based [Corneil et al., 2018, Igl et al., 2018, Kurutach et al., 2018, Gregor et al.,
2019, Gelada et al., 2019] or non-reconstruction-based methods [Laskin et al., 2020, Zhang et al.,
2020], using contrastive learning or bi-simulation metrics. Works on meta-RL and transfer learning

1We provide an implementation of LIAM in https://github.com/uoe-agents/LIAM

2

focus on learning representations over tasks and use them to solve new and previously unseen tasks
[Doshi-Velez and Konidaris, 2016, Hausman et al., 2018, Zhang et al., 2018, Rakelly et al., 2019,
Zintgraf et al., 2019]. In contrast to these work, LIAM focuses on learning representations about the
relationship between the trajectories of the controlled agent and of the modelled agent.

Multi-agent Reinforcement Learning (MARL): MARL algorithms are designed to train multiple
agents to solve tasks in a shared environment [Hernandez-Leal et al., 2017, Papoudakis et al.,
2019]. At the beginning of the training, a number of agents are untrained and typically initialised
with random policies. A common paradigm in MARL is Centralised Training with Decentralised
Execution (CTDE), in which the trajectories of all agents are utilised during training, while during
execution each agent conditions its individual policy only on its local trajectory. The information of
all agents can be utilised during training for various reasons, such as computing a joint value function
[Lowe et al., 2017, Foerster et al., 2018, Sunehag et al., 2018, Rashid et al., 2018], or generating
intrinsic rewards [Jaques et al., 2019]. During execution, only the local information of each agent is
used for selecting actions in the environment. Our work relates to CTDE in that we train LIAM in
a centralised fashion using the trajectories of all the existing agents in the environment. However,
during execution, the learned model uses only the local trajectory of the controlled agent.

3 Approach

3.1 Problem Formulation

We control a single agent which must learn to interact with other agents that use one of a ï¬xed number
of policies. We model the problem as a Partially-Observable Stochastic Game (POSG) [Shapley,
1953, Hansen et al., 2004] which consists of N agents I = {1, 2, ..., N }, a state space S, the joint
action space A = A1 Ã ... Ã AN , a transition function P : S Ã A Ã S â [0, 1] specifying the
transition probabilities between states given a joint action, and for each agent i â I a reward function
ri : S Ã A Ã S â R. We consider that each agent i has access to its observation oi â Oi, where
Oi is the observation set of agent i. The observation function â¦i : S Ã A Ã Oi â [0, 1] deï¬nes a
probability distribution over the possible next observations of agent i given the previous state and the
joint action of all agents.

We denote the agent under our control by 1, and the modelled agents by â1 where for notational
convenience we will treat the modelled agents as a single âcombinedâ agent with joint observations
oâ1 and actions aâ1. We assume a set of ï¬xed policies, Î  = {Ïâ1,k|k = 1, ..., K}, which may
be deï¬ned manually (heuristic) or pretrained using RL. Each ï¬xed policy determines the modelled
agentâs actions as a mapping Ïâ1,k(oâ1) from the modelled agentâs local observation oâ1 to a
distribution over actions aâ1. Our goal is to ï¬nd a policy ÏÎ¸ parameterised by Î¸ for agent 1 which
maximises the average return against the ï¬xed policies from the training set Î , assuming that each
ï¬xed policy is initially equally probable and ï¬xed during an episode:

arg max

Î¸

EÏÎ¸,Ïâ1,kâ¼U (Î )

(cid:35)

Î³tr1

t+1

(cid:34)Hâ1
(cid:88)

t=0

(1)

where r1
t+1 is the reward received by agent 1 at time t + 1 after performing the action a1
t , H is the
episode length (horizon), and Î³ â (0, 1) is a discount factor. It is also important to note that neither
during training nor during execution the controlled agent has access to the identity k of the policy
that is used by the modelled agent.

3.2 Local Information Agent Modelling

t

and aâ1

We aim to learn the relationship between the trajectory of the controlled agent and the trajectory of
the modelled agent. We denote by Ï â1 = {oâ1
, aâ1
t=0 the trajectory of the modelled agent where
oâ1
are the modelled agentâs observation and action at time step t in the trajectory, up to
t
horizon H. These trajectories are generated from the ï¬xed policies in Î . We assume the existence
of some latent variables (or embeddings) in the space Z, and at each time step t the latent variables
zt contain information both about the ï¬xed policy that is used by the modelled agent as well as the
dynamics of the environment as perceived by the modelled agent. To learn the relationship between
the modelled agentâs trajectory and the latent variables we can use a parametric decoder. The decoder

t }t=H

t

3

is denoted as fu : Z â Ï â1 and is the model that decodes the latent variables to the trajectory of the
modelled agent.

1:t, a1

The last step to learn the relationship between the local and the modelled agentâs trajectories is to
use a recurrent encoding model, that we denote as fw : Ï 1 â Z, with parameters w, to learn the
relationship between the local trajectory of the controlled agent and the latent variables. Speciï¬cally,
we learn the function that relates the modelled agentâs trajectory to the latent variables with an encoder
that only depends on local information of the controlled agent. Since during execution only the
encoder is required to generate the latent variables of the modelled agent, this approach removes the
assumption that access to the modelled agentâs observations and actions is available during execution.
At each time step t,
the recurrent encoder net-
work generates an embedding zt, which is condi-
tioned on the information of the agent under control
(o1
1:tâ1), until time step t. At each time step t the
parametric decoder learns to reconstruct the modelled
agentâs observation and action (oâ1
) condi-
tioned on the embedding zt. Therefore, the decoder
consists of a fully-connected feed-forward network
with two output heads; the observation reconstruction
u, and the policy reconstruction head f Ï
head f o
u (see
Figure 1). In each time step t, the decoder receives as
input embedding zt and the observation reconstruc-
tion head reconstructs the modelled agentâs observa-
tion oâ1
, while the action reconstruction head outputs
a categorical distribution over the modelled agentâs
action aâ1
. We observe that the output dimensions of
the two decoderâs reconstruction heads grow linearly
with respect to the number of the agents in the environment. We refer to this method as LIAM (Local
Information Agent Modelling). LIAM uses the information of both the controlled agent and the
modelled agent during training, but during execution only the information of the controlled agent is
used. The encoder-decoder loss is deï¬ned as:

Figure 1: Diagram of LIAM architecture. The
two solid-line rectangles show the compo-
nents of LIAM that are used during training
and during execution respectively.

and aâ1

t

t

t

t

LED =

1
H

H
(cid:88)

t=1

[(f o

u(zt) â oâ1

t )2 â log f Ï

u(aâ1
t

|zt)] where zt = fw(o1

:t, a1

:tâ1)

(2)

3.3 Reinforcement Learning Training

The latent variables z augmented with the controlled agentâs observation can be used to condition
the RL optimised policy. Consider the augmented space O1
aug = O1 Ã Z, where O1 is the original
observation space of the controlled agent in the POSG, and Z is the representation space about the
agentâs models. The advantage of learning the policy on O1
aug compared to O1 is that the policy
can specialise for different z â Z. In our experiments we optimised the policy of the controlled
agent using A2C, however, other RL algorithms could be used in its place. The input to the actor
and critic are the local observation and the generated representation. We do not back-propagate the
gradient from the actor-critic loss to the parameters of the encoder. We use different learning rates for
optimising the parameters of the networks of RL and the encoder-decoder. We empirically observed
that LIAM exhibits high stability during learning, allowing us to use larger learning rate compared
to RL. Additionally, we subtract the policy entropy from the policy gradient loss to encourage
exploration [Mnih et al., 2016]. Given a batch B of trajectories, the objective of A2C is:

LA2C = E(ot,at,rt+1,ot+1)â¼B[

(cid:0)r1

1
2
â ËA log ÏÎ¸(a1

t+1 + Î³VÏ(o1
t |o1

t+1, zt+1) â VÏ(o1
t |o1
t , zt) â Î²H(ÏÎ¸(a1

t , zt))2

t , zt))]

(3)

The pseudocode of LIAM is given in Appendix A and the implementation details in Appendix D.
Intuitively, at the beginning of each episode, LIAM starts with uninformative embeddings "average"
over the possible agent trajectories. At each time step, the controlled agent interacts with the
environment and the modelled agent, and updates the embeddings based on the local trajectory that it
perceives.

4

EncoderTrajectoryReconstructionDecoderDownstreamTask (RL)ExecutionTrainingFigure 2: The three evaluation environments. Double speaker-listener (left), level-based foraging
(middle), predator prey (right).

4 Experiments

4.1 Multi-Agent Environments

We evaluate the proposed method in three multi-agent environments (one cooperative, one mixed, one
competitive): double speaker-listener [Mordatch and Abbeel, 2017], level-based foraging [Albrecht
and Stone, 2017, Papoudakis et al., 2021], and a version of predator-prey proposed by [BÃ¶hmer et al.,
2020]. For each environment, we create ten different policies which are used for training (set Î ).
More details about the process of generating the ï¬xed policies are presented in Appendix B.

Double speaker-listener (DSL): The environment consists of two agents and three designated
landmarks. At the start of each episode, the agents and landmarks are generated in random positions,
and are randomly assigned one of three possible colours - red, green, or blue. The task of each agent
is to navigate to the landmark that has the same colour. However, each agent cannot perceive its
colour. The colour of each agent can only be observed by the other agents. Each agent must learn
both to communicate a ï¬ve-dimensional message to the other agent as well as navigate to the correct
landmark. The controlled agentâs observation includes the relative positions of all landmarks and the
other agents as well as the communication message from the previous time step and the colour of
the other agent. The cooperative reward at each time step is the negative average Euclidean distance
between two agents and their corresponding correct landmark.

Level-based foraging (LBF): The environment is a 20 Ã 20 grid-world, consisting of two agents
and four food locations. The agents and the foods are assigned random levels and positions at the
beginning of an episode. The goal is for the agents to collect all foods. Agents can either move in
one of the four directions or attempt to pick up a food. A group of one or more agents successfully
pick a food if the agents are positioned in the adjacent cells to the food and if the sum of the agentsâ
levels is at least as high as the foodâs level. The controlled agent has to learn to cooperate to load
foods with a high level and at the same time act greedily for foods that have lower levels. The
environment has sparse rewards, representing the contribution of the agent in the gathering all foods
in the environment. For example, if the agent receives a food with level 2, and there are another
three foods with levels 1, 2 and 3 respectively, the reward of the agent is 2/(1 + 2 + 2 + 3) = 0.25.
The maximum cumulative reward that all agents can achieve is normalised to 1. The environment is
partially-observable, and the controlled agent perceives other agents and foods that are located up to
four grid cells in every direction. However, the modelled agents can observe the full environment
state. The episode terminates when all available foods have been loaded or after 50 time steps.

Predator Prey (PP): The environment consists of four agents and two large obstacles. Three of the
agents are predators and the other agent is the prey. Each agent has ï¬ve navigation actions. If only
one of the predators captures the prey then the predators receive reward â1 and the prey reward 1. If
two or more predators capture the prey, then the predators receive reward 1 and the prey reward â1.
In the experiments, we control the prey, while the predators sample their policies from the set of ï¬xed
policies. The environment is partially-observable where the prey can only observe other agents and
obstacles that are within its receptive ï¬eld, while the predators observe all agents and obstacles in the
environment. The episode terminates after 50 time steps.

5

4.2 Baselines

Our problem of learning to adapt to different ï¬xed policies can be viewed either as an agent modelling
problem, or as a single-agent task-adaptation RL problem in which we control one agent which has
to learn to adapt to multiple tasks, where each one of the ï¬xed policies deï¬nes a different task. Thus,
task adaptation algorithms can be used as baselines to address our problem. We compare our method
against ï¬ve baselines, two of which are indicative of the upper and the lower performance of LIAM
when it is evaluated against the policies from Î . All baselines are trained using the A2C algorithm.

Full Information Agent Model (FIAM): This baseline is indicative of the upper performance of
LIAM when it is evaluated against the ï¬xed policies from the set Î . FIAM utilises the trajectories of
the modelled agent during training as well as during execution and it is trained similarly to LIAM.
However, the encoder is conditioned on the actual trajectory of the modelled agent, and therefore
requires access to the modelled agentâs trajectory during execution. Since FIAM has access to more
information than LIAM during execution, we intuitively expect to achieve higher returns compared
to LIAM.

No Agent Model (NAM): This baseline does not use an explicit agent model, and it is indicative of
the lower performance of LIAM when it is evaluated against the ï¬xed policies from the set Î . It does,
however, use a recurrent policy network which receives as input the observation and action of the
controlled agent. By using a recurrent network we can compare returns achieved by implicit agent
modelling (that is done by the hidden states of the recurrent network) with the returns achieved by
explicit agent modelling that is done by LIAM. NAM is similar to the single-agent task-adaptation
algorithm RL2 [Wang et al., 2016, Duan et al., 2016]. In contrast to the original implementation of
RL2, we reset the hidden state of the recurrent network at the beginning of each episode, and we do
not use the agentâs reward as input to the policy to ensure consistency among the tested algorithms.
Since NAM does not have access to the modelled agentâs trajectory during training, we intuitively
expect to perform worse than LIAM in terms of achieved returns.

VariBad [Zintgraf et al., 2019]: This baseline trains a VAE to learn latent representations of the
observation and reward functions, as they are perceived by the controlled agent, conditioned on the
observation, action, reward triplet of the controlled agent. We include VariBad for two reasons: it
is an algorithm that learns to adapt to different tasks, and also it uses a recurrent encoder similarly
to LIAM, but does not utilise the trajectory of the modelled agent. As a result, we can observe
whether utilising the modelled agentâs trajectory during training, as is done in LIAM, results in higher
evaluation returns. To ensure consistency among the tested algorithms, we do not use the reward as
input in the encoder of VariBad.

Classiï¬cation-Based Agent Modelling (CBAM): This baseline learns to reconstruct the identity
of the policy that is used by the modelled agent. The policy of the controlled agent is conditioned on
the reconstructed identity concatenated with the local observation of the controlled agent. CBAM
consists of a recurrent network that receives as input the observation-action sequence of the controlled
agent. The output has a softmax activation function, and we train it to maximise the log-likelihood
of the identities of the policies that are used by the modelled agent. CBAM uses the ï¬xed policy
identities during training (that LIAM does not), but does not use the trajectories of the modelled agent
(that LIAM does).

Constrastive Agent Representation Learning (CARL): Finally, we evaluate a non-reconstruction
baseline based on contrastive learning [Oord et al., 2018]. CARL utilises the modelled agentâs
trajectories during training but during execution only the trajectories of the controlled agent are
used. Details of the implementation of this baseline are included in Appendix C. This baseline was
included because recent works [Laskin et al., 2020, Zhang et al., 2020] in state representation found
that non-reconstruction methods tend to perform better than reconstruction methods, especially in
domains that have pixel-based observations.

4.3 Evaluation of Returns

Figure 3 shows the average evaluation returns of all methods during training in the three multi-agent
environments. The returns of every evaluated method are averaged over ï¬ve runs with different
initial seeds, and the shadowed part represents a 95% conï¬dence interval (two standard errors of
the mean). We evaluate the policy, learned by each method, every 1000 training episodes for 100
episodes. During the evaluation, the agent follows the stochastic policy that is outputted from the

6

Figure 3: Episodic evaluation returns and 95% conï¬dence interval of the six evaluated methods
during training, against policies from Î .

policy network. We found that sampling the action from the policy distribution leads to signiï¬cantly
higher returns compared to following the greedy policy.

First of all, we observe that in all three evaluation environment the returns of LIAM are closer to the
upper baseline (FIAM) than to the lower baseline (NAM). The difference in the returns between LIAM
and VariBad can be attributed to the fact that LIAM utilises the modelled agentâs trajectories during
training, leading to better representations for RL. VariBad only learns how the current observation
of the controlled agent is related to the next observation and reward, while LIAM learns how the
local observation is related to the trajectory of the modelled agent. CARL performs worse compared
to LIAM because the generated embeddings contain less information compared to the embeddings
generated from LIAM. For example, in the level-based foraging environment, a speciï¬c time step
t the control agent observes that the modelled agent is at a speciï¬c cell. The observation of the
modelled agent also contains this type of information, and as a result, the contrastive task can easily
relate that these two embeddings are the two parts of a positive pair. Therefore the representation is
trained to only encode such information. On the other hand, LIAM encodes into z all information that
has been gathered so far in the interaction to be able to reconstruct the modelled agentâs trajectory. We
observe that LIAM achieves higher return than CBAM because identifying the agent identity is not
always an informative representation. LIAM utilises the modelled agentâs trajectory during training
and learns how the local trajectory corresponds to the trajectory of the modelled agent. In the double
speaker-listener environment, CBAMâs returns are higher compared to the rest of the baselines and
close to the returns of LIAM, because the different ï¬xed policies are discretely different (different
communication messages correspond to different colours) and the classiï¬er learns to accurately
identify the ï¬xed policy identity. However, in the other environments the classiï¬er is unable to
separate different ï¬xed policies, which results in lower average returns.

4.4 Model Evaluation

After comparing LIAM against ï¬ve baselines with respect to the average evaluation returns, we now
evaluate the encoder and the decoder of LIAM. First, we visualise the embedding space of LIAM and
we evaluate how fast the encoder learns to generate informative embeddings. Finally, we evaluate
whether the decoder of LIAM learns to accurately reconstruct the modelled agentâs actions.

We analyse the embeddings learned by LIAMâs encoder. Figure 4a presents the two-dimensional
projection, using the t-SNE method [Van der Maaten and Hinton, 2008], of the generated embeddings
at the 20th time step of the episode for all policies in Î  in the double speaker-listener environment,
where each colour represents a different policy in the set Î . We observe that clusters of similar
colours are generated, indicating the interactions with the same ï¬xed policy for the modelled agent
result in representations that are close to each other. However, we observe that each colour appears
in more than one cluster. We speculate that different clusters can represent the different modalities
that exist in the trajectory of the modelled agent, such as the colour of the controlled agent and the
message that is communicated from the modelled agent to the controlled agent.

Figure 4b presents how the action reconstruction accuracy, and the accuracy of the controlled agent
identifying its own colour (which is not observed by the controlled agent) in the double speaker-
listener environment are changing through time. The colour of the controlled agent is represented
as three-dimensional one-hot vector in the observation of the modelled agent, and the controlled
agent does not have direct access to this information during their interaction. At each time step t

7

0.00.51.01.52.02.53.03.54.0Time steps1e7â25.0â22.5â20.0â17.5â15.0â12.5â10.0â7.5ReturnsDouble Speaker ListenerLIAMFIAMNAMCBAMVariBadCARL0.00.20.40.60.81.0Time steps1e70.00.10.20.30.40.5ReturnsLevel-Based ForagingLIAMFIAMNAMCBAMVariBadCARL0.00.20.40.60.81.0Time steps1e7â10â505101520ReturnsPredator PreyLIAMFIAMNAMCBAMVariBadCARL(a)

(b)

(c)

Figure 4: (a) t-SNE projection of the embeddings in the double speaker-listener environment, where
different colours represent different ï¬xed policies and different points represent different episodes.
(b) Modelled agentâs actions and controlled agentâs colours reconstruction accuracy with respect to
the episode time steps. (c) Reconstruction accuracy of the modelled agentâs actions at the 20th time
step of the episode in the three evaluation environments (the conï¬dence interval is not visible in the
double speaker-listener bar plot).

we use the embedding zt to reconstruct the observation and the action of the modelled agent. In
the reconstructed observation we ï¬nd the three dimensional vector that corresponds to the colour of
the controlled agent, and we consider that the identiï¬ed colour corresponds to the element of the
three-dimensional vector that is closer to 1. Then, we compute the percentage of the times where the
reconstructed colour matches the colour of that is truly observed by the modelled agent. LIAM learns
to identify the modelled agentâs action as well as the colour of the controlled agent with accuracy
greater than 90% after 10 time steps.

The decoder of the controlled agent receives as input an embedding at each time step, and predicts the
observations and actions of the modelled agent. We evaluate the decoder based on action prediction
accuracy. At the 20th time step of the episode, we use the generated embeddings and the decoder to
reconstruct the modelled agentâs actions. Figure 4c presents the average action prediction accuracy
and the 95% conï¬dence interval in the three evaluation environments. The reduced action prediction
accuracy in level-based foraging and the predator-prey compared to double speaker-listener can be
explained by the fact that the controlled agent does not always observe the modelled agent, and as a
result it is harder to predict their actions.

4.5 Ablation Study

In this section we perform an ablation study to jus-
tify the design choices behind the architecture of
LIAM. We design and evaluate six different ablations
of LIAM in the double speaker-listener environment.
First, we evaluate two ablated versions of LIAMâs
decoder: LIAM-No-Act-Recon which reconstructs
only the observations of the modelled agent, LIAM-
No-Obs-Recon which reconstructs only the actions
of the modelled agent. Then, we evaluate how dif-
ferent data inputs in the encoder of LIAM affect the
returns in the double speaker-listener environment.
We consider the model LIAM-No-Act, where in the
encoder of LIAM we only input the observations of
the controlled agent, and the model LIAM-No-Obs,
where in the encoder of LIAM we only input the ac-
tions of the controlled agent. We also consider an
encoder-decoder model that does not utilise the tra-
jectory of the modelled agent during training. We call
this model LIAM-Local, and at each time step t the decoder learns to reconstruct the next observation
o1
t+1 and the action a1
t conditioned on the embedding zt. Additionally, we evaluate a variational
encoder-decoder model [Kingma and Welling, 2014] that performs inference to a Hidden Markov
Model (HMM), and we refer to it as LIAM-VAE. LIAM-VAE learns a prior (p(zt|ztâ1)) that express

Figure 5: Average returns comparison of
LIAM against six ablated versions of LIAM
in the double speaker-listener environment.

8

â40â30â20â10010203040â40â30â20â10010203040t-SNE Embeddings0510152025Episode Time Steps0.40.50.60.70.80.91.0AccuracyReconstruction AccuracyColor Reconstruction AccuracyAction Reconstruction AccuracyDSLLBFPP0.00.20.40.60.8AccuracyAction Reconstruction Accuracy0.00.51.01.52.02.53.03.54.0Time steps1e7â24â22â20â18â16â14â12â10ReturnsDouble Speaker ListenerLIAMLIAM-No-Act-ReconLIAM-No-Obs-ReconLIAM-No-ActLIAM-No-ObsLIAM-LocalLIAM-VAE(a) t = 0

(b) t = 7

(c) t = 20

(d)

Figure 6: (a, b, c) Snapshots of one episode of the double speaker-listener environment at time steps
0, 7 and 20 respectively. (d) Decoderâs reconstruction values of the colour of the controlled agent.

the temporal relationship of the latent distribution, similarly to Chung et al. [2015]. We discuss the
implementation of this model in Appendix C.

Figure 5 presents the average returns of LIAM and the aforementioned ablation baselines in the double
speaker-listener environment. First, we observe that non-reconstructing either the observation or the
action of the controlled agent leads to signiï¬cantly lower returns than the original implementation of
LIAM. We also observe that the lack of either the observation or the action of the controlled agent
from the encoder of LIAM results in signiï¬cant lower returns compared to full LIAM. The reason
is that the model observes how the modelled agent reacts (by observing the relative position of the
modelled agent that is included in the controlled agentâs observation) to different communication
messages (the communication message is one of the actions of the controlled agent) that the controlled
agent outputs, and learns to generate informative embeddings (see also Section 4.6). LIAM-Local
achieves the lowest returns compared to the other baselines. The main reason behind this result, is
that LIAM-Local does not learn the relationship between the local and the modelled agentâs trajectory.
Finally, we believe that the main reason that LIAM-VAE performs worse than LIAM is that the KL
regularisation is too restrictive, which results in less informative representations compared to LIAM.
To alleviate this issue, we can use a Î² coefï¬cient to weigh the KL loss, similarly to Higgins et al.
[2017] or use the MMD2 distance [Gretton et al., 2007] for regularisation, similarly to info-VAE
[Zhao et al., 2017]. In both cases the resulting objective is not a lower bound to the evidence. We did
not ï¬nd any important beneï¬t of using a Î² coefï¬cient or the MMD2 distance compared to the simpler
instantiation of LIAM that we present in this work.

4.6 Understanding LIAM

To better understand the working mechanisms of LIAM, we analyse how LIAM performs agent
modelling in one episode of the double speaker-listener environment (shown in Figures 6a to 6c).
In this episode, the controlled agent has colour red, while the modelled agent has colour blue. The
reconstruction of the colour of the controlled agent can be seen as a proxy of the belief about its
colour, that the controlled agent encodes into z at each time step. Note that the reconstructed values
presented in Figure 6d are not a probability distribution (can be lower than 0 or larger than 1) due to
unconstrained optimisation, but we consider that the value closest to 1 represents the belief of the
controlled agent about its colour. At the time step 0, all reconstructed colours have similar value
close to 0.3 meaning that the controlled agent does not have any information about its colour. The
controlled agent outputs different messages and observes how the modelled agent reacts to them
through the episode. We observe that at time step 7 the controlled agent believes that its colour is
blue and starts moving toward the blue landmark. After the 10th interaction, the controlled agent has
communicated several different messages to the modelled agent and by observing how the modelled
agent reacts to them, it understands that its belief about its colour was wrong, and it starts moving
toward the correct landmark.

5 Conclusion

We proposed LIAM, the idea of learning models about the trajectories of the modelled agent using the
trajectories of the controlled agent. LIAM concurrently trains the model with a decision policy, such
that the resulting agent model is conditioned on the local observations of the controlled agent. LIAM
is agnostic to the type of interactions in the environment (cooperative, competitive, mixed) and can

9

0510152025Episode Time Steps0.00.20.40.60.81.0Reconstructed ValueColour Reconstructionmodel an arbitrary number agents. Our results show that LIAM can signiï¬cantly improve the episodic
returns that the controlled agent achieves over baseline methods that do not use agent modelling.
The returns that are achieved by A2C when combined with LIAM in some cases nearly match the
returns that are achieved by the upper baseline FIAM which assumes access to the trajectories of the
modelled agent during execution.

We identify two main limitations of LIAM that are left unaddressed: (a) modelling non-reactive
agents, and (b) modelling agents from high-dimensional observations such images. First, a necessary
requirement for successfully applying LIAM is for the actions of the modelled agent to affect
the observations of the controlled agent. If this assumption does not hold, LIAM will not have
enough information to reason about the modelled agentâs trajectory and will just learn an average
representation over the possible trajectories of the modelled agent. Second, in this work, we focus on
environments with low-dimensional vector-based observations. Recent works on state representation
for RL in images [Laskin et al., 2020, Zhang et al., 2020] have shown that reconstruction-based
methods may learn low-quality representations because they weigh the reconstruction of each pixel
as equally important even though the vast majority of the pixels contain no relevant information to
the modelling problem. Extending LIAM to pixel-based observations can be achieved by using a
different architecture for extracting representations, such as CARL.

In the future, we would like to investigate how agent modelling from local information can be
extended to multi-agent reinforcement learning, where several agents are learning concurrently and
non-stationarity arises [Hernandez-Leal et al., 2017, Papoudakis et al., 2019]. We would also like to
explore notions of âsafetyâ to handle agents that aim to deceive and exploit the agent model [Shoham
et al., 2007, Ganzfried and Sandholm, 2011, 2015].

Funding Disclosure

This research was in part supported by the UK EPSRC Centre for Doctoral Training in Robotics and
Autonomous Systems (G.P., F.C.), and personal grants from the Royal Society and the Alan Turing
Institute (S.A.).

References

Stefano V. Albrecht and Peter Stone. Reasoning about hypothetical agent behaviours and their
parameters. In International Conference on Autonomous Agents and Multi-Agent Systems, 2017.

Stefano V Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive

survey and open problems. Artiï¬cial Intelligence, pages 66â95, 2018.

Nolan Bard, Michael Johanson, Neil Burch, and Michael Bowling. Online implicit agent modelling.

In International Conference on Autonomous Agents and Multi-Agent Systems, 2013.

Wendelin BÃ¶hmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International

Conference on Machine Learning, 2020.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
2020a.

Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big
self-supervised models are strong semi-supervised learners. In Advances in Neural Information
Processing Systems, 2020b.

Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, and Yoshua Bengio.
A recurrent latent variable model for sequential data. arXiv preprint arXiv:1506.02216, 2015.

Dane Corneil, Wulfram Gerstner, and Johanni Brea. Efï¬cient model-based deep reinforcement
learning with variational state tabulation. In International Conference on Machine Learning, 2018.

Finale Doshi-Velez and George Konidaris. Hidden parameter markov decision processes: A semi-
parametric regression approach for discovering latent task parametrizations. In International Joint
Conference on Artiï¬cial Intelligence, 2016.

10

Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl 2: Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.

Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In AAAI Conference on Artiï¬cial Intelligence, 2018.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model

uncertainty in deep learning. In International Conference on Machine Learning, 2016.

Sam Ganzfried and Tuomas Sandholm. Game theory-based opponent modeling in large imperfect-
information games. In International Conference on Autonomous Agents and Multiagent Systems,
2011.

Sam Ganzfried and Tuomas Sandholm. Safe opponent exploitation. ACM Transactions on Economics

and Computation, pages 1â28, 2015.

Carles Gelada, Saurabh Kumar, Jacob Buckman, Oï¬r Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, 2019.

Piotr J Gmytrasiewicz and Prashant Doshi. A framework for sequential planning in multi-agent

settings. Journal of Artiï¬cial Intelligence Research, pages 49â79, 2005.

Karol Gregor, George Papamakarios, Frederic Besse, Lars Buesing, and Theophane Weber. Temporal
difference variational auto-encoder. In International Conference on Learning Representations,
2019.

Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard SchÃ¶lkopf, and Alex J Smola. A kernel
method for the two-sample-problem. In Advances in Neural Information Processing Systems,
2007.

Aditya Grover, Maruan Al-Shedivat, Jayesh K Gupta, Yura Burda, and Harrison Edwards. Learning
policy representations in multiagent systems. In International Conference on Machine learning,
2018.

David Ha and JÃ¼rgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances

in Neural Information Processing Systems, 2018.

Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete

world models. In International Conference on Learning Representations, 2020.

Eric A Hansen, Daniel S Bernstein, and Shlomo Zilberstein. Dynamic programming for partially

observable stochastic games. In AAAI Conference on Artiï¬cial Intelligence, 2004.

Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller.
In International Conference on

Learning an embedding space for transferable robot skills.
Learning Representations, 2018.

He He, Jordan Boyd-Graber, Kevin Kwok, and Hal DaumÃ© III. Opponent modeling in deep reinforce-

ment learning. In International Conference on Machine Learning, 2016.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern
Recognition, 2020.

Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey of learn-
ing in multiagent environments: Dealing with non-stationarity. arXiv preprint arXiv:1707.09183,
2017.

Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017.

11

Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational
reinforcement learning for POMDPs. In International Conference on Machine Learning, 2018.

Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro A Ortega, DJ Strouse,
Joel Z Leibo, and Nando De Freitas. Social inï¬uence as intrinsic motivation for multi-agent deep
reinforcement learning. In International Conference on Machine Learning, 2019.

Michael Johanson, Martin Zinkevich, and Michael Bowling. Computing robust counter-strategies. In

Advances in Neural Information Processing Systems, 2008.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations, 2015.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference

on Learning Representations, 2014.

Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart Russell, and Pieter Abbeel. Learning plannable

representations with causal infogan. arXiv preprint arXiv:1807.09341, 2018.

Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations

for reinforcement learning. In International Conference on Machine Learning, 2020.

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic
for mixed cooperative-competitive environments. In Advances in Neural Information Processing
Systems, 2017.

Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiï¬er nonlinearities improve neural network

acoustic models. In International Conference on Machine Learning, 2013.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, 2016.

Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent

populations. In AAAI Conference on Artiï¬cial Intelligence, 2017.

Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive

coding. arXiv preprint arXiv:1807.03748, 2018.

Georgios Papoudakis, Filippos Christianos, Arrasy Rahman, and Stefano V Albrecht. Dealing with
non-stationarity in multi-agent deep reinforcement learning. arXiv preprint arXiv:1906.04737,
2019.

Georgios Papoudakis, Filippos Christianos, Lukas SchÃ¤fer, and Stefano V Albrecht. Benchmarking
multi-agent deep reinforcement learning algorithms in cooperative tasks. In Advances in Neural
Information Processing Systems Track on Datasets and Benchmarks, 2021.

Neil C Rabinowitz, Frank Perbet, H Francis Song, Chiyuan Zhang, SM Eslami, and Matthew
Botvinick. Machine theory of mind. In International Conference on Machine Learning, 2018.

Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in
multi-agent reinforcement learning. In International Conference on Machine Learning, 2018.

Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efï¬cient off-policy
meta-reinforcement learning via probabilistic context variables. In International Conference on
Machine Learning, 2019.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning, 2018.

JÃ¼rgen Schmidhuber and Sepp Hochreiter. Long short-term memory. Neural Computation, pages

1735â1780, 1997.

12

John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438,
2015.

Lloyd S Shapley. Stochastic games. Proceedings of the National Academy of Sciences, pages

1095â1100, 1953.

Yoav Shoham, Rob Powers, and Trond Grenager. If multi-agent learning is the answer, what is the

question? Artiï¬cial intelligence, pages 365â377, 2007.

P. Stone, G.A. Kaminka, S. Kraus, and J.S. Rosenschein. Ad hoc autonomous agent teams: collabora-

tion without pre-coordination. In AAAI Conference on Artiï¬cial Intelligence, 2010.

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning. In International Conference on Autonomous Agents
and Multi-Agent Systems, 2018.

Andrea Tacchetti, H Francis Song, Pedro AM Mediano, Vinicius Zambaldi, Neil C Rabinowitz, Thore
Graepel, Matthew Botvinick, and Peter W Battaglia. Relational forward models for multi-agent
learning. International Conference on Learning Representations, 2019.

Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine

Learning Research, pages 2579â2605, 2008.

Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv
preprint arXiv:1611.05763, 2016.

Annie Xie, Dylan P Losey, Ryan Tolsma, Chelsea Finn, and Dorsa Sadigh. Learning latent represen-

tations to inï¬uence multi-agent interaction. In Conference on Robot Learning, 2020.

Amy Zhang, Harsh Satija, and Joelle Pineau. Decoupling dynamics and reward for transfer learning.

arXiv preprint arXiv:1804.10689, 2018.

Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. arXiv preprint
arXiv:2006.10742, 2020.

Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational

autoencoders. arXiv preprint arXiv:1706.02262, 2017.

Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and
Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep rl via meta-learning. In
International Conference on Learning Representations, 2019.

Luisa Zintgraf, Sam Devlin, Kamil Ciosek, Shimon Whiteson, and Katja Hofmann. Deep interactive
bayesian reinforcement learning via meta-learning. In International Conference on Autonomous
Agents and Multi-Agent Systems, 2021.

13

A Pseudocode of LIAM

Algorithm 1 shows the pseudocode of LIAM.

Algorithm 1 Pseudocode of LIAMâs algorithm

for m = 1, ..., M episodes do

Reset the hidden state of the encoder LSTM
Sample E ï¬xed policies from Î 
Create E parallel environments and
gather initial observations
a1
â1 â zero vectors
for t = 0, ..., H â 1 do

for every environment e in E do
t and oâ1

Get observations o1
Compute the embedding zt = fw(o1
Sample action a1
Sample modelled agentâs action aâ1
t
t+1, oâ1
Perform the actions and get o1

t â¼ Ï(a1

t , zt)

t |o1

t

end for
if t mod update_frequency = 0 then

t , a1

tâ1)

t+1, r1

t+1

Gather the sequences of all E environments in a single batch B
Perform a gradient step to minimise LED (Equation (2)) using B
Perform a gradient step to minimise LA2C (Equation (3)) using B

end if
end for

end for

B Fixed Policies

Double Speaker-Listener: Each policy consists of two sub-policies: the communication message
policy and the navigation action policy. For the communication message policy, we manually
created constant ï¬ve-dimensional one-hot communication messages that correspond to different
colours. We manually select different ï¬xed policies that communicate the same colours with different
communication messages. For the navigation policies, we then created ï¬ve pairs of agents and we
trained each pair to learn to navigate using the MADDPG algorithm [Lowe et al., 2017]. Each agent
on the pair learns to navigate based on the communication messages of the other agent in the pair.

Level-Based Foraging: The ï¬xed policies in level-based foraging consist of four heuristic policies,
three policies trained with IA2C and three policies trained with MADDPG. All modelled agent
policies condition their policies to the state of the environment. The heuristic agents were selected
to be as diverse as possible, while still being valid strategies. We used the strategies from Albrecht
and Stone [2017], which are: (i) going to the closest food, (ii) going to the food which is closest to
the centre of visible players, (iii) going to the closest compatible food, and (iv) going to the food
that is closest to all visible players such that the sum of their and the agentâs level is sufï¬cient to
load it. We also trained three policies with MADDPG by training two pairs of agents and extracting
the trained parameters of those agents. For the policies trained with MADDPG, we circumvent the
instability caused by deterministic policies in Level-based Foraging by enabling dropout in the policy
layers [Gal and Ghahramani, 2016] both during exploration and evaluation. We observe that by
creating stochastic policies the agents perform signiï¬cantly better.

Predator-Prey: The ï¬xed policies in the predator-prey consist of a combination of heuristic and
pretrained policies. First we created four heuristic policies, which are: (i) going after the prey, (ii)
going after one of the predators, (iii) going after the agent (predator or prey) that is closest, (iv)
going after the predator that is closest. Additionally, we create another six pretrained policies, by
training two sets of three agents using RL: one with the MADDPG algorithm and one with the IA2C
algorithm. To create the ten ï¬xed policies, where each ï¬xed policy consists of three sub-policies (one
for each predator), we randomly combine the four heuristic and the six pretrained policies.

14

C Baselines

C.1 Contrastive Agent Representation Learning (CARL)

CARL extracts representations about the modelled agent in the environment without reconstruction
from the local information provided to the controlled agents, the locally available observation, and
the action that the controlled agent previously performed. CARL has access to the trajectories of all
the other agents in the environment during training, but during execution only to the local trajectory.

To extract such representations, we use self-supervised learning based on recent advances on con-
trastive learning [Oord et al., 2018, He et al., 2020, Chen et al., 2020a,b]. We assume a batch
B of M number of global episodic trajectories B = {Ï glo,m}M â1
m=0 , where each global trajectory
consists of the trajectory of the controlled agent 1 and the trajectory of all the modelled agent â1,
Ï glo,m = {Ï 1,m, Ï â1,m}. The positive pairs are deï¬ned between the trajectory of the controlled
agent and the trajectory of the modelled agent at each episode m in the batch. The negative pairs are
deï¬ned between the trajectory of the controlled agent at the speciï¬c episode m and the trajectory of
the modelled, in all other episodes l (cid:54)= m in the batch, expect episode m.

pos = {Ï 1,m, Ï â1,m}
neg = {Ï 1,m, Ï â1,l}

(4)

We assume the existence of two encoders: the recurrent encoder that receives sequentially the
trajectory of the controlled agent fw : Ï 1 â Z and at each time step t generates the representation z1
t ,
and the recurrent encoder that receives sequentially the trajectory of the modelled agent fu : Ï â1 â Z
and at each time step t generates the representation zâ1
t is used as input in the
actor and the critic of A2C. During training and given a batch of episode trajectories we construct the
positive and negative pairs following Equation (4) and minimise the InfoNCE loss [Oord et al., 2018]
that attracts the positive pairs and repels the negative pairs.

. The representation z1

t

LCARL = â

Hâ1
(cid:88)

t=0

log

exp{cos(z1,m
j=0 exp{cos(z1,m

, zâ1,m
t

t

t

(cid:80)M â1

)/Ïtemp}

, zâ1,j
t

)/Ïtemp}

(5)

where cos is the cosine similarity and Ïtemp the temperature of the softmax function.

C.2 LIAM-VAE

Following the work of Chung et al. [2015] we can write the lower bound in the log-evidence of the
modelled agentâs trajectory as:

log p(Ï â1) â¥ E

zâ¼q(z|Ï 1
)

(cid:88)
[

t

[log p(Ï â1

t

|zt, Ï â1

:tâ1) â DKL(q(zt|z:tâ1, Ï 1

:t)(cid:107)p(zt|Ï 1

:tâ1, z:tâ1)]

(6)

We assume the following independence Ït|zt|=Ï:tâ1. This practically means that the latent variables
should hold enough information to reconstruct the trajectory at time step t. We deliberately make
this assumption that will lead to worse reconstruction but more informative latent variables. Since
the goal of LIAM-VAE is to learn representation about the modelled agent we prioritise informative
representations over good reconstruction. More speciï¬cally, consider that we want to reconstruct
modelled agentâs observation oâ1
at time step t, using the latent variable zt. The observation of the
modelled agent oâ1
tâ1 has as information the colour of the controlled agent. If we condition the decoder
both on the latent variable and the previous observation to reconstruct the current observation, then
the reconstruction of the colour of the controlled agent can be achieved by the decoder by looking at
the observation oâ1
tâ1. As a result, the latent variables will not encode this type of information that is
necessary to successfully solve this environment. Additionally, we assume that Ït|Ï:tâ1|=ztâ1. This
assumption holds because ztâ1 is generated from a distribution conditioned on Ï:tâ1 and ztâ1 holds
the same information as Ï:tâ1. As a result, we can write the lower bound as:

t

log p(Ï â1) â¥ Ezâ¼q(z|Ï 1)[

(cid:88)

[log p(Ï â1

t

|zt) â DKL(q(zt|Ï 1

:t)(cid:107)p(zt|Ï:tâ1)]

(7)

t

15

To optimise this lower bound, we deï¬ne:

â¢ The encoder qw with parameters w, which a recurrent network that receives as input the
observations and actions of the controlled agent sequential and generates the statistics (the
mean and the logarithmic variance) of a Gaussian distribution.

â¢ The decoder pu with parameters u that receives the latent variable zt and reconstructs the
modelled agentâs trajectory at each time step t. The implementation of the decoder is the
same as the decoder of LIAM described in Equation (2).

â¢ The prior pÏ with parameters Ï that models the temporal relationship between the latent
variables. We evaluated two different models for the prior: one that receives the hidden state
of the recurrent network of the encoder as input and outputs a Gaussian distribution, and
one that considers that the prior at each time step t is equal to the posterior at each time step
t â 1. Both prior choices led to similar episodic returns. In Figure 5 we present the average
returns for the later choice of prior.

We train LIAM-VAE similarly to LIAM. In the actor and the critic network we input the mean of the
Gaussian posterior.

D Implementation Details

All feed-forward neural networks have two hidden layers with ReLU [Maas et al., 2013] activation
function. The encoder consists of one LSTM [Schmidhuber and Hochreiter, 1997] and a linear layer
with ReLU activation function. All hidden layers consist of 128 nodes. The action reconstruction
output of the decoder is passed through a Softmax activation function to approximate the categorical
modelled agentâs policy. For a continuous action space, a Gaussian distribution can be used. For
the advantage computation, we use the Generalised Advantage Estimator [Schulman et al., 2015]
with Î»GAE = 0.95. We create 10 parallel environments to break the correlation between consecutive
samples. The actor and the critic share all hidden layers in the A2C implementation. We use the
Adam optimiser [Kingma and Ba, 2015] with learning rates 3 Ã 10â4 and 7 Ã 10â4 for the A2C and
the encoder-decoder loss respectively. We also clip the gradient norm to 0.5. We subtract the policy
entropy from the actor loss [Mnih et al., 2016] to ensure sufï¬cient exploration. The entropy weight Î²
is 10â2 in double speaker-listener and the predator-prey and 0.001 in the level-based foraging. We
train for 40 million time steps in the double speaker-listener environment and for 10 million time steps
in the rest of the environments. During the hyperparameter selection, we searched: (1) learning rates
in the range [10â4, 7 Ã 10â4] and [3 Ã 10â4, 10â3] for the parameters of RL and LIAM respectively,
(2) hidden size between 64 and 128, and (3) entropy regularisation in the range of [10â3, 10â2]. The
hyperparameters were optimised in the double speaker-listener environment and were kept constant
for the rest of the environments, except the entropy regularisation in level-based foraging, where we
saw signiï¬cant gains in the performance of all algorithms.

For the MPE environments, we use the version that can be found in this link: https://github.
com/shariqiqbal2810/multiagent-particle-envs. This version allows for better
visualisation of the communication messages in the double speaker-listener environment, as well as
seeding the environments. Our A2C implementation is based on the following well-known repos-
itory: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/tree/
master/a2c_ppo_acktr.

E Different Learning Rates

In this Section, we evaluate the ï¬nal achieved returns of LIAM with respect to different learning rates
for the RL and the encoder-decoder optimisation. Figure 7 presents a heat-map of LIAMâs achieved
returns in the double speaker-listener environment with respect to different combinations of the two
learning rates. We observe that LIAMâs achieved returns are close to the returns achieved by the best
conï¬guration for most of the evaluated learning rate conï¬gurations.

16

Figure 7: Heat-map with different learning rates for the RL and the encoder-decoder optimisation.

F Scalability in the Number of Fixed Policies

In this section we evaluate LIAM with respect to 100 different ï¬xed policies. Instead of combining
the two sub-policies in the double speaker-listener environment in one-to-one fashion, we take the
Cartesian product of the sub-policies which leads to 100 different combinations of ï¬xed policies.

Figure 8: Episodic evaluation returns and 95% conï¬dence interval of LIAM and CBAM during
training, against 100 ï¬xed policies.

Figure 8 presents the average returns achieved by LIAM and CBAM against 100 different ï¬xed
policies. In Section 4.3, we observed that in the double speaker-listener environment the average
returns of CBAM were very close to the returns of LIAM. We observe that with 100 ï¬xed policies
the difference in the returns between LIAM and CBAM increases signiï¬cantly. This does not come
as a surprise, since CBAM would require a very ï¬exible policy to successfully adapt to all 100 ï¬xed
policies. On the other hand, as we observed in Figure 4a the embeddings of LIAM are not explicitly
clustered based on the identities of the ï¬xed policies, but based on trajectory of the modelled agent
to encode its communication message and its observation. As long as the embeddings contain such
information, the RL procedure is able to learn a policy that achieves higher returns compared to
CBAM.

17

0.00010.00030.00050.0007ED Learning Rate0.00010.00020.0003RL Learning RateDouble Speaker Listenerâ35â30â25â20â150.00.51.01.52.02.53.03.54.0Time steps1e7â24â22â20â18â16â14ReturnsDouble Speaker ListenerLIAMCBAM