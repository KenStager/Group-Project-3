Mediated Multi-Agent Reinforcement Learning

Dmitry Ivanov
HSE University
Saint Petersburg, Russia
Technion
Haifa, Israel
divanov@campus.technion.ac.il

Ilya Zisman
HSE University
Saint Petersburg, Russia
iazisman@edu.hse.ru

Kirill Chernyshev
HSE University
Saint Petersburg, Russia
chernyshevk1212@gmail.com

3
2
0
2

n
u
J

4
1

]

A
M

.
s
c
[

1
v
9
1
4
8
0
.
6
0
3
2
:
v
i
X
r
a

ABSTRACT
The majority of Multi-Agent Reinforcement Learning (MARL) lit-
erature equates the cooperation of self-interested agents in mixed
environments to the problem of social welfare maximization, al-
lowing agents to arbitrarily share rewards and private information.
This results in agents that forgo their individual goals in favour
of social good, which can potentially be exploited by selfish de-
fectors. We argue that cooperation also requires agentsâ identities
and boundaries to be respected by making sure that the emergent
behaviour is an equilibrium, i.e., a convention that no agent can
deviate from and receive higher individual payoffs. Inspired by
advances in mechanism design, we propose to solve the problem
of cooperation, defined as finding socially beneficial equilibrium,
by using mediators. A mediator is a benevolent entity that may
act on behalf of agents, but only for the agents that agree to it. We
show how a mediator can be trained alongside agents with pol-
icy gradient to maximize social welfare subject to constraints that
encourage agents to cooperate through the mediator. Our experi-
ments in matrix and iterative games highlight the potential power
of applying mediators in MARL.1

KEYWORDS
Multi-Agent Reinforcement Learning; Cooperation; Mixed Envi-
ronments; Self-Interested Agents; Equilibrium; Mediators

ACM Reference Format:
Dmitry Ivanov, Ilya Zisman, and Kirill Chernyshev. 2023. Mediated Multi-
Agent Reinforcement Learning. In Proc. of the 22nd International Confer-
ence on Autonomous Agents and Multiagent Systems (AAMAS 2023), London,
United Kingdom, May 29 â June 2, 2023, IFAAMAS, 13 pages.

1 INTRODUCTION
The cooperation of self-interested agents is an elusive concept
to define and measure, especially in temporally and spatially ex-
tended environments typical for Multi-Agent Reinforcement Learn-
ing (MARL). In these environments, agents may have multiple
low-level actions available that may not be inherently cooperative
or competitive. The aggregated effect of these actions affects the
rewards of all agents and the state of the environment in ways
that may not be easy to disentangle. This is further complicated by
games lasting for multiple turns, over which the effect of agentsâ
actions accumulates. An approach alluring in its simplicity is to

1This is a slightly updated version of this [publication]. Please cite the published
version. The code is available [here].

Proc. of the 22nd International Conference on Autonomous Agents and Multiagent Sys-
tems (AAMAS 2023), A. Ricci, W. Yeoh, N. Agmon, B. An (eds.), May 29 â June 2, 2023,
London, United Kingdom. Â© 2023 International Foundation for Autonomous Agents
and Multiagent Systems (www.ifaamas.org). All rights reserved.

measure cooperation as social welfare, i.e., some aggregate (usually,
the sum total) of cumulative rewards of all agents [40]. This allows
complete freedom in the choice of training procedures, including ar-
bitrary reward manipulations, information sharing, and parameter
sharing (examples are provided in Section 1.1).

In this paper, we challenge this view. While maximizing social
welfare is a relevant problem of its own, not every solution should
count as a solution to cooperation. Unlike fully cooperative MARL
settings where all agents share a common objective, mixed (i.e.,
general-sum) settings imply self-interested agents with clear bound-
aries. Blending their interests and blurring their boundaries does
not make the agents more cooperative â it makes the setting itself
more cooperative. Instead, cooperation is only meaningful if it is a
consequence of the rational decision-making of strategic agents that
act in their own best interests. This is only possible if the agents act
in an equilibrium, i.e., a convention that none of the agents can devi-
ate from and increase their individual reward. Defining cooperation
as converging to socially beneficial equilibrium (like tit-for-tat) is
sometimes referred to as conditional cooperation, as opposed to
unconditional cooperation that focuses on maximizing social wel-
fare [54]. As we discuss later, very few existing approaches address
conditional cooperation.

Designing generic MARL algorithms that promote or create so-
cially beneficial equilibria is a complex and largely unsolved prob-
lem. Prospective solutions can come from the field of mechanism
design (and related fields, such as information design and contract
design) [38]. This field studies how to implement trusted entities
known as mechanisms that interact with self-interested agents in
ways that both consider their incentives and achieve desirable so-
cial outcomes. Taking auction design as an example, the agentsâ
incentives can be valuations over an item and a desirable social
outcome can be allocating the item to the agent with the highest
valuation. From the economic perspective, unconditional coopera-
tion is unrealistic since one cannot arbitrarily modify the rewards
or incentives of an economic agent the way one may be able with
an artificial agent. In our example, the designer cannot simply ask
agents to disclose their valuations and give away the item for free
to the highest number because the agents would have no incen-
tives to report truthfully. An example more relevant to MARL is
self-driving vehicles [10]: while we can encode the rewards of the
vehicles, we cannot encode the rewards of the people that use them.
Instead, desirable social outcomes should be aligned with agentsâ
incentives â a property known as Incentive-Compatibility (IC). A
vast literature on designing IC mechanisms is waiting to be adapted
to mixed MARL. This direction has recently gotten attention in
several papers that we discuss in Section 1.1.

 
 
 
 
 
 
Table 1: The effect of the mediator on prisonerâs dilemma. The mediator acts on behalf of all agents that agree to it, i.e., commit.
In prisonerâs dilemma (a), the dominant action is to defect. In the mediated prisonerâs dilemma (b), the dominant action is to
commit if the mediator adopts the following strategy: cooperate if both agents committed, otherwise defect.

(a) Prisonerâs Dilemma

(b) Mediated Prisonerâs Dilemma

Defect Cooperate

Defect
Cooperate

0, 0
2, 2

2, 2
4, 4

Defect
Cooperate
Commit

Defect Cooperate Commit
7, -5
2, 2
7, -5

0, 0
-5, 7
2, 2

0, 0
-5, 7
0, 0

In this paper, we show how conditional cooperation in MARL
can be solved via mediators [35]. A mediator is a benevolent entity
that can act on behalf of the agents that agree to it. In a mediated
augmentation of a game, each agent first decides whether act on its
own or to let the mediator act for it, an action that we call âcommitâ.
We call the set of committed agents a âcoalitionâ. Then, the original
game is played between the mediator acting for the coalition and the
rest of the agents. Note that the mediator can have a unique policy
for each coalition. Crucially, an agent always has the opportunity
to refuse to commit (serving as an opportunity to misreport), e.g., if
it finds the mediator incapable of achieving an acceptable reward or
if it wants to exploit other agents cooperating through the mediator.
The potential impact of mediators is illustrated in Table 1 on the
example of the prisonerâs dilemma.

We are interested in applying mediators to complex sequential
games where optimal policies are unknown in advance and have
to be learned. Specifically, we propose to train both agents and the
mediator jointly with MARL. This creates challenges atypical for
unconditional cooperation: not only the mediator has to learn a
policy that maximizes social welfare (for each coalition), but it also
has to encourage agents to commit, i.e., ensure compatibility with
their incentives. We show how to formulate this as a constrained
optimization problem, as well as how to solve this problem using
the method of Lagrange multipliers and dual gradient descent.

Additionally, we introduce the frequency of agentsâ commitment
as a hyperparameter that we denote as commitment window ð.
When ð = 1, agentsâ commitment lasts for a single time step. When
ð > 1, agents are only allowed to commit each ð-th time step and
the commitment lasts for ð time steps (i.e., the commitment is more
committing). As a result, the mediator has a higher potential to
maximize social welfare for larger ð.

There are multiple advantages to our approach. First, any emer-
gent behaviour is by design an equilibrium because agents act in
their own best interests (i.e., maximize their own rewards) and
can deviate if they find it beneficial. Second, Monderer and Ten-
nenholtz [35] show that in symmetric games there always exists
a mediator with a socially optimal strategy such that committing
is an equilibrium for the agents. This means that there is no loss
in achievable social welfare when using mediators compared to
unconditionally maximizing social welfare. For asymmetric games,
we empirically demonstrate the power of mediators. Third, the
mediatorâs strategy is fair in the sense that each committed agent
receives the expected payoffs that are at least as high as if it acted
on its own. This is unlike the existing techniques that artificially
encode fairness into a centralized objective [24, 57]. Fourth, the
mediator only promotes cooperation between the committed agents

and deters the free-riding of the rest. These are the properties of
reciprocity that naturally emerge from the Incentive-Compatibility
of our solution, which is unlike the existing attempts at training
reciprocal agents based on arbitrary reward sharing and heuris-
tics [1, 13, 30, 41]. Fifth, the rate of commitment to the mediator
presents a generic measure of cooperation.

We experimentally validate our procedure to train agents with
the mediator in several variants of prisonerâs dilemma [44] and
public good game [3], including one-shot and sequential games. For
each game, we first analyze the. We find that naively training the
mediator to maximize social welfare may result in agents refusing
to commit, either due to misalignment of their and the mediatorâs
interests or due to them trying to exploit other committed agents.
We show that this can be addressed by considering agentsâ incen-
tives through additional constraints when training the mediator.
Additionally, we investigate the effect of varying the commitment
window ð and find that higher ð gives the mediator more power.

1.1 Related Work
Zhao et al. [54] distinguish two kinds of cooperation in MARL set-
tings. Unconditional cooperation refers to cooperation independent
of what the opponents are doing. Reciprocity-based cooperation
refers to cooperation iff others cooperate, e.g., tit-for-tat in the
iterated prisonerâs dilemma.

We use a similar but more general classification of unconditional
and conditional cooperation that better reflects the literature. We de-
fine the problem of unconditional cooperation as the maximization
of social welfare. The problem of conditional cooperation addition-
ally has a condition that no agent has incentives to deviate, i.e., that
the agents are in equilibrium.

Unconditional cooperation. The majority of unconditional coop-
eration techniques are based on modifying or replacing agentsâ
rewards with âintrinsicâ preferences [1, 2, 9, 12, 13, 19, 21, 24, 30, 41â
43, 49, 51, 57]. The intrinsic preferences can be either rewards of
other agents or rewards learned by agents or a third party to guide
agents to social welfare maximizing outcomes. The most direct
approach is to train each agent to directly optimize social welfare,
but this is susceptible to issues like free-riding and credit assign-
ment that can be addressed by exploiting reward decomposition
available in mixed environments. These techniques hardwire other-
regarding preferences into agents and are typically not concerned
with equilibria from the perspective of maximizing the original
rewards.

The few alternative techniques typically require parameter or
information sharing. An example of the former is the parameteriza-
tion of all agents with identical neural networks [18]. This technique
sidesteps the conflict of interests by hardwiring reciprocity into
agents: trying to exploit automatically reflects. An example of the
latter is the social influence that uses a communication channel
to maximize impact on message recipients [23]. This technique
ignores the potential incentives of agents to manipulate each other
through communication channels.

While these techniques are typically framed as solutions to
cooperation, the problem they solve is more akin to the fully-
decentralized MARL [28, 52, 53]. In this setting, agents can have
varying reward functions, but the collective goal is to maximize the
globally averaged return over all agents, i.e., the social welfare. The
agent-specific reward functions serve as an instrument to address
credit assignment, so it indeed makes sense to not be concerned
with equilibria. In contrast, mixed MARL implies selfish interests,
and while social welfare is undoubtedly a useful performance mea-
sure, treating its unconditional maximization as a unanimously
shared goal is a misleading shortcut.

Conditional cooperation. Some approaches to conditional cooper-
ation look for existing equilibria with high social welfare. Learning
with Opponent-Learning Awareness (LOLA) and its modifications
[14, 50, 54] leverage alternative gradient updates that shape the
opponentâs learning to guide it to cooperative equilibria. As a result,
it can learn reciprocal strategies, e.g., tit-for-tat in the repeated pris-
onerâs dilemma. LOLA has multiple limitations: it is only applicable
to two-player games, requires access to the transition dynamics,
and assumes that the opponent learns using first-order gradient-
based methods. Furthermore, LOLA updates require read access
to the opponentâs parameters. While this can be circumvented by
learning the opponentâs model based on their behaviour, it comes
at the expense of performance.

Other approaches change the rules of the game such that self-
interested agents prefer to cooperate, i.e., such that outcomes with
high social welfare become equilibria. At this point, the line between
unconditional and conditional cooperation may become blurry. Af-
ter all, modifying an agentâs rewards with intrinsic preferences
could be reinterpreted as paying the agent extrinsically by a third
party. However, once additional rewards are considered extrin-
sic payments, a natural question is what is the minimal payment
scheme such that agents still cooperate. None of the papers de-
scribed in the unconditional cooperation section ask this question.
The question of optimal payments is central to the economic
field of contract design [16]. Adapting contract design to MARL is
attempted in the concurrent work of Christoffersen et al. [8]. This
work proposes for one of the agents to take the role of the principal
that may pay other agents. However, their empirical algorithm
assumes that the payment condition is pre-determined and only
learns the payment amount. Designing a generic algorithm that
can learn optimal payment schemes (both conditions and amounts)
by either one of the agents or a third party is an open problem.

Other solutions to conditional cooperation can be considered
reward redistribution [20], which can also be framed as taxation [55,
56]; and similarity-based cooperation [37], which is an extension
of program equilibrium [46, 48].

In recent years, there has been a rising interest in communication
under competition in MARL [4, 36]. In contrast to communication
in fully cooperative environments, mixed environments may in-
centivize self-interested agents to manipulate others through their
messages, preventing reliable and mutually beneficial communi-
cation from being established. A principled way to resolve this
issue could potentially come from the field of Bayesian Persuasion
[25]. There have been extensions of Bayesian Persuasion to online
multi-receiver settings [7], as well as to MARL [31].

2 PROBLEM SETUP
2.1 Markov Games
Markov game is a standard formalization of spatially and temporally
extended environments typical for MARL [32]. It is defined as a
tuple G = (ð, ð , (Að )ð âð , (Oð )ð âð ,ð , (ðð )ð âð ). Let ð be the set of
all possible states ð , ð be the set of agents, Að be the set of actions
ðð available to the agent ð in all states. Let Oð
: ð â ðð be the
observation function, where ðð is the set of observations ðð of
agent ð. Let ð : ð Ã (Að )ð âð â Î(ð) be the transition function,
where Î denotes a set of discrete probability distributions. This
function specifies the effect of the agentsâ actions on the state of the
environment. We enumerate the sequences of sampled transitions
with time-steps ð¡. Let ðð
: ð Ã (A ð ) ð âð â P (R) be the reward
function for each agent ð, where P is a set of continuous probability
distributions, R â R. Let Ëðð,ð¡ â¼ ðð (ð ð¡ , að¡ ) denote sampled rewards.

Let Ëðð,ð¡ =

(cid:104)
ð¾ð âð¡ Ëðð,ð

(cid:105)

be the discounted cumulative reward

a.k.a. the return, where ð¾ â [0, 1) denotes the discount factor.
: ðð â Î(Að ) be the policy of agent ð. Let ðð (ðð ) =
Let ðð
Eð,ðð,ð ,ð (ð  |ð,Oð (ð  )=ðð ) [ Ëðð ] be the value function. The agent seeks
the policy ðð that maximizes the value ðð in each observation.

â
(cid:205)
ð=ð¡

2.2 Mediators
A mediator can be viewed as an additional entity that may act in
the game on behalf of a subset of agents. The agents interact with
the mediator by optionally sending it messages, and the mediator
acts for those agents that sent it a message. Crucially, an agent may
refrain from sending a message and act independently from the
mediator. Formally, Monderer and Tennenholtz [35] define mediator
as a tuple M = ((ðð )ð âð , c = (cð¶ )ââ ð¶ âð ), where ðð is a finite set
of messages that agent ð may send to the mediator, ð¶ is a subset of
agents that sent messages to the mediator referred to as coalition,
and cð¶ : Mð¶ â Î((Að )ð âð¶ ) is the correlated strategy (the joint
policy) for the coalition ð¶. Each agent has a utility function over
the outcomes, the expectation of which it rationally maximizes.

A special case that we focus on is the minimal mediator. A me-
diator is called minimal if each message space ðð is a singleton,
meaning that agentsâ interaction with the mediator is limited to
agreeing to enter the coalition. We refer to this action as commit-
ting. To uniquely define a minimal mediator, specifying ðð becomes
unnecessary. A strategy of the mediator that makes a unanimous
commitment an equilibrium is called mediated equilibrium. Cru-
cially, Monderer and Tennenholtz [35] show that any mediated
equilibrium can be implemented by a minimal mediator, as well
as that mediated equilibrium that maximizes social welfare always
exists in symmetric games.

Note that an agent cannot misreport its commitment to the
mediator, i.e., deviate while pretending to commit. Instead, the
action of refusing to commit itself serves as misreporting. A weaker
variant of a mediator that only recommends actions is also explored
in the economic literature [11, 26, 27, 45]. Applying this idea to
MARL could be an interesting new direction.

It is also important to note that Monderer and Tennenholtz [35]
primarily explore mediators through the lens of strong mediated
equilibria, i.e., equilibria robust to deviations of groups of agents.
Since the number of different groups grows exponentially with the
number of agents, finding strong mediated equilibria with RL is a
challenging problem. We leave it to future work.

2.3 Markov Mediators
The approach of Monderer and Tennenholtz [35] implies fixed
strategies of the mediator. Instead, we treat the mediator as a sepa-
rate agent with its own goals and train it with RL alongside other
agents. To this end, we introduce Markov mediators.

Let M = ((Oð

ð¶ )ââ ð¶ âð , (ð ð

ð¶ )ââ ð¶ âð , ð) be minimal Markov me-
diator. Let Oð
ð¶ : ð â ðð
ð¶ be the mediatorâs observation function
for coalition ð¶, where ðð
ð¶ is the set of the mediatorâs observa-
tions. As the observations, we will use a tuple ðð
ð¶ = ((ðð )ð âð¶, ð¶).
Note that alternative choices of the mediatorâs observations like
ðð
ð¶ = ((ðð )ð âð , ð¶) or ðð
ð¶ = (ð , ð¶) create an asymmetry of informa-
tion between the mediator and the agents, which may serve as an
additional incentive for the agents to commit. However, this would
require access to additional information during execution.

Let ð ð

ð¶ : ð Ã(Að )ð âð â P (R) be the mediatorâs reward function
for coalition ð¶. We are only concerned with mediators with the goal
of increasing the utilitarian social welfare of the agents. For this
reason, as the mediatorâs reward we will use the sum of rewards
of the agents in the coalition: Ëð ð
ð¶,ð¡ = (cid:205)ð âð¶ Ëðð,ð¡ . Let ð â Z+ be the
commitment window. Each ð steps of the game, an agent may either
commit to the mediator or choose to play independently for the
next ð steps. Agents can only commit when ð¡ is divisible by ð.
ð¶ â Î((Að )ð âð¶ ), the return Ëðð
ð¶ ) = E[ Ëðð
ð¶ (ðð

â
(cid:205)
ð¶ ] =
ð=ð¡
(cid:205)ð âð¶ E[ Ëðð ] of the mediator for each coalition ð¶ similarly to those
of the agents. Notice that the return and the value of the mediator
decompose into the respective sums over the coalition.

We define the policy ð ð
(cid:104)
ð¾ð âð¡ Ëð ð
ð¶,ð

= (cid:205)ð âð¶ Ëðð,ð¡ , and the value ð ð

ð¶ : ðð

ð¶,ð¡ =

(cid:105)

3 ALGORITHM
We now discuss how to train the Markov mediator with RL. We first
describe our practical implementations of agents and the mediator,
including neural architectures and loss functions, and then dive
deeper into potential objectives for the mediator. Note that we
write all expressions as expectations, but in practice, these are
approximated as empirical averages over sampled transitions.

3.1 Practical Implementations
Both the agents and the mediator are trained via Actor-Critic frame-
works [29, 47]. The actor represents the policy ð (ð), whereas the
critic represents an approximation of the value function Ëð (ð). Both
actor and critic can be parameterized with neural networks [34].

The architectures are illustrated in Figures 1 and 2 and are described
below.

Agents. The agents are trained independently: an agent ð has
its own actor ððð (ðð ) and critic Ëððð (ðð ), respectively parameterized
by ðð and ðð , that are trained only on its own experience. The
respective loss functions ð¿ are the negated policy gradient for the
actor and the squared temporal difference for the critic:

ð¿(ðð ) = âE( Ëðð,ð¡ + ð¾ Ëððð (ðð,ð¡ +1) â Ëððð (ðð,ð¡ )) log ððð (ðð,ð¡ )

(1)

ð¿(ðð ) = E( Ëðð,ð¡ + ð¾ Ëððð (ðð,ð¡ +1) â Ëððð (ðð,ð¡ ))2
MARL literature routinely leverages parameter and experience
sharing for both actor [18] and critic [33], but we opt out of these
techniques to ensure that agents are selfish and individual, and
therefore that any cooperation observed is rational rather than a
consequence of hardwired reciprocity (as discussed earlier).

(2)

We apply minimal changes to adapt agents to the presence of
the mediator. When ð = 1, the only change is that each agentâs
actor is augmented with an additional action, i.e., to commit to the
mediator. The effect of this action is that the mediator takes control
over the agent for the current time-step.

When ð > 1, the mediator acts for the agent for ð steps at a time,
and the commitment action is only available to the agents every ð
time-steps. This necessitates several additional changes. First, the
current time-step ð¡ is concatenated to an agentâs observation to let it
know whether it can commit. Second, in the succeeding ð â 1 states
after an agent commits, it effectively acts off-policy and therefore
is not trained. Third, committing for ð steps effectively transitions
the agent from ð ð¡ directly to ð ð¡ +ð , and in the process yields the
total discounted reward of (cid:205)ð¡ +ð â1
ð¾ð âð¡ Ëðð,ð . For this reason, (only)
when an agent commits, the 1-step temporal difference in the loss
functions (1) and (2) is replaced with the k-step temporal difference:
( Ëðð,ð¡ + ð¾ Ëðð,ð¡ +1 â Ëðð,ð¡ ) â ((cid:205)ð¡ +ð â1

ð¾ð âð¡ Ëðð,ð + ð¾ð Ëðð,ð¡ +ð â Ëðð,ð¡ ).

ð=ð¡

ð=ð¡

Mediator. It is well-known that learning the joint policy in a
centralized way is unfeasible due to the exponential scaling of the
action space with the number of agents. For this reason, we fully
factorize the joint policy of the mediator for a coalition: ð ð
ð¶ ) =
(cid:206)ð âð¶ ð ð
ð ). The mediatorâs policies for all agents are parame-
ð
terized with a single neural network ð ð that receives as input the
observation ðð
ð = (ðð, ð¶) and the agentâs index ð. For convenience,
we denote the ð-th agent policy ðð ð ((ðð, ð¶, ð)) as ðð ð
(ðð, ð¶). The
objectives for ðð ð
are discussed in the next subsection.

ð¶ (ðð

(ðð

ð

The mediatorâs critic receives coalition ð¶ and observations of all
agents (ðð )ð âð as an input and simultaneously outputs values for
all agents, both in and out of the coalition. While this formulation
requires centralized access to all observations and rewards, note that
the critic is only required during training. Access to individual value
functions for each agent will be crucial in the next subsection when
estimating constrained objectives for the mediator. For convenience,
we denote a criticâs output Ëðð ð (((ðð )ð âð , ð¶, ð)) as Ëðð ð
(o, ð¶). As
usual, it is trained to minimize squared temporal difference:

ð

ð

ð¿(ðð

ð ) = E( Ëðð,ð¡ + ð¾ Ëðð ð

ð

(oð¡ +1, ð¶ð¡ +1) â Ëðð ð

ð

(oð¡ , ð¶ð¡ ))2

(3)

Figure 1: Schematic illustration of our architectures of the actor (left) and the critic (right) of the agents, ð = 1.

Figure 2: Schematic illustration of our architectures of the actor (left) and the critic (right) of the mediator, ð = 1.

Note that our implementation is intentionally minimalistic. The
literature suggests a multitude of sophisticated solutions for various
problems typical for MARL. Instead of limiting the mediator to a
fully factorized policy, sampling from a joint policy is possible with
techniques based on coordination graphs [5, 17]. Credit assignment
could be addressed by using specialized critics that isolate contri-
butions of each agent to social welfare [15, 21, 33]. While these
techniques could potentially improve the mediator, our focus is on
introducing mediators to MARL (and in turn promoting the idea
of applying mechanism design in MARL), not on searching for the
best possible implementation of the mediator. We intend to isolate
the implementation choices that are necessary for the mediator
to function, and complicating the mediator is in conflict with this
intent.

3.2 Objectives for the Mediator
Here, we first propose a mediator we denote as Naive that simply
maximizes the social welfare of the agents in the coalition. We then
derive the constraints necessary to incentivize the agents to commit,
as well as the training procedure that approximately satisfies these
constraints for ð = 1. Finally, we discuss how the training procedure
should be modified for ð > 1, as well as the potential effect of ð.

Naive Mediators. The Naive mediator is oblivious to the incen-
tives of the agents. Its goal is to greedily maximize the utilitarian
social welfare for any given coalition. For the mediatorâs policy
ð ð
for some agent ð, this goal can be formulated as the following
ð
objective:

âoð¡ , (ð¶ð¡ | ð â ð¶ð¡ ) : max
ð ð
ð

âï¸

ð âð¶ð¡

ðð (ð ð,ð¡ , ð¶ð¡ )

(4)

Each value function ðð (ð ð,ð¡ , ð¶ð¡ ) is approximated with the ð-th
ð of the

output of the mediatorâs critic Ëðð ð
mediator for the agent ð is trained via policy gradient:

(oð¡ , ð¶ð¡ ). The policy ð ð

ð

ð¿(ð ð

ð ) = âE[

âï¸

ð âð¶ð¡

( Ëð ð,ð¡ + ð¾ Ëðð ð

ð

(oð¡ +1, ð¶ð¡ +1)

â Ëðð ð

ð

(oð¡ , ð¶ð¡ ))] log ðð ð

ð

(ðð,ð¡ , ð¶ð¡ )

(5)

Constraints. Since the Naive mediator greedily optimizes social
welfare, there is no guarantee that its optimal policy is a mediated
equilibrium, i.e., that the agents always prefer committing. To fix
this, the mediatorâs policy should satisfy certain constraints. In-
tuitively, self-interested agents only commit if committing serves
their best selfish interests. In RL, the quantification of an agentâs
interests is a value function, so committing should yield a higher
value than not committing for each agent. Below we mathemati-
cally express this condition. For convenience, we divide the agents
into two groups: those in and those outside the coalition.

On the one hand, an agent that enters the coalition should benefit
from it and receive payoffs at least as high as it (counterfactually)
would outside the coalition:

âð, ðð,ð¡ :

E(ð¶ð¡ |ð âð¶ð¡ ) [ðð (ðð,ð¡ | ð¶ð¡ )] â¥ E(ð¶ð¡ |ð âð¶ð¡ ) [ðð (ðð,ð¡ | ð¶ð¡ \{ð})]
(6)
On the other hand, an agent that chooses to act on its own should
not be able to exploit the mediator and should receive payoffs not
higher than if it had (counterfactually) committed:

âð, ðð,ð¡ :

E(ð¶ð¡ |ðâð¶ð¡ ) [ðð (ðð,ð¡ | ð¶ð¡ âª{ð})] â¥ E(ð¶ð¡ |ðâð¶ð¡ ) [ðð (ðð,ð¡ | ð¶ð¡ )]
(7)
We respectively refer to the constraints (6) and (7) as Incentive-
Compatibility (IC) and Encouragement (E) constraints. Notice that

ð¿(ð ð

ð ) = âE[

âï¸

ð âð¶ð¡

( Ëð ð,ð¡ + ð¾ Ëðð ð

ð

(oð¡ +1, ð¶ð¡ +1) â Ëðð ð

ð

(oð¡ , ð¶ð¡ ))

â

âï¸

ðâð¶ð¡

they are in expectation over the distribution of coalitions (gener-
ated by the agentsâ policies), rather than for each coalition. This is
because the agents choose actions before the coalition is formed.
Also, notice that the constraints are feasible since they can be
exactly satisfied by the mediator copying the policies of the agents
(thus having no effect). In other words, mediated equilibrium always
exists. We now discuss how to train a mediator that satisfies these
constraints while maximizing social welfare.

Incentive-Compatibility Constraint. To incorporate the constraints
(6) into the objective (4), we can apply the method of Lagrange mul-
tipliers, which results in the following dual objective:

âï¸

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ð âð¶ð¡

ðð (ð ð,ð¡ , ð¶ð¡ ) + ððð

ð (ðð,ð¡ )ðð (ðð,ð¡ , ð¶ð¡ )

âoð¡ , (ð¶ð¡ | ð â ð¶ð¡ ) : max
ð ð
ð

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»(8)
where ððð
ð (ðð,ð¡ ) â¥ 0 are Lagrange multipliers. Note that the coun-
terfactual value ðð (ðð,ð¡ | ð¶ð¡ \ {ð}) from (6) can be omitted from the
dual due to not depending on ð ð
. This objective can be maximized
ð
using policy gradient:

+ððð

ð (ðð,ð¡ ) ( Ëðð,ð¡ + ð¾ Ëðð ð

ð

(oð¡ +1, ð¶ð¡ +1) â Ëðð ð
log ðð ð

ð

ð

(9)

(oð¡ , ð¶ð¡ ))]
(ðð,ð¡ , ð¶ð¡ )

To mitigate credit assignment, we deliberately incorporate only
the IC constraint of the ð-th agent into the objective of ð ð
ð but
not the IC constraints of all agents. This way, the mediator only
has to ensure that its actions for an agent are compatible with the
incentives of this agent.

To find ððð

ð (ðð,ð¡ ), we employ dual gradient descent [6]:

ð

ð

log ððð

ð (ðð,ð¡ ) â log ððð

(oð¡ , ð¶ð¡ ) â Ëðð ð

ð (ðð,ð¡ ) â ð¼ [ Ëðð ð

(oð¡ , ð¶ð¡ \ {ð})]
(10)
where ð¼ is the learning rate. The intuition behind this update is
that the second term on the right-hand side on average equals zero
when the constraint is satisfied exactly. In practice, we update ððð
ð
once according to this update each time we update actors and critics.
To ensure that ððð
is non-negative, we update its logarithm instead.
ð
Furthermore, in our experiments, we find approximation as a scalar
ððð
ð (ðð,ð¡ ) = ððð
to be sufficient. Other examples of applying dual
ð
gradient descent to enforce a constraint can be found in [22, 39].

ð

Note that the update rule (10) involves a counterfactual value
Ëðð ð
(oð¡ , ð¶ð¡ \ {ð}), i.e., what the value of the ð-th agent would be if it
did not enter this specific coalition in this specific state. Estimating
it is only possible thanks to our implementation of the mediatorâs
critic, i.e., our choice to train it to simultaneously estimate values
of agents both in and out of the coalition (see Section 3.1).

Encouragement Constraint. Same derivations as for the IC con-
straint apply to the E constraint. The constraints (7) for each agent
outside the coalition can be incorporated into the policy gradient
(5) of each agent in the coalition ð â ð¶ð¡ :

ð¿(ð ð

ð ) = âE[

âï¸

ð âð¶ð¡

( Ëð ð,ð¡ + ð¾ Ëðð ð

ð

(oð¡ +1, ð¶ð¡ +1) â Ëðð ð

ð

(oð¡ , ð¶ð¡ ))

â

âï¸

ðâð¶ð¡

ðð
ð (ð ð,ð¡ )( Ëð ð,ð¡ + ð¾ Ëðð ð

ð

(oð¡ +1, ð¶ð¡ +1) â Ëðð ð

ð

(oð¡ , ð¶ð¡ ))]

(11)

log ðð ð

ð

(ðð,ð¡ , ð¶ð¡ )

The Lagrange multipliers ðð

ð (ðð,ð¡ ) are also learned via dual gra-

dient descent:

log ðð

ð (ð ð,ð¡ ) â log ðð

ð (ð ð,ð¡ ) â ð¼ [ Ëðð ð

(oð¡ , ð¶ð¡ âª { ð }) â Ëðð ð

ð

ð

(oð¡ , ð¶ð¡ )]
(12)

The Lagrange multipliers are also as scalars: ðð

ð (ðð,ð¡ ) = ðð
ð .

Constrained mediators. Both IC and E constraints should be ap-

plied simultaneously to train the Constrained mediator.

ð¿(ð ð

ð ) = âE[

âï¸

ð âð¶ð¡

( Ëð ð,ð¡ + ð¾ Ëðð ð

ð

(oð¡ +1, ð¶ð¡ +1) â Ëðð ð

ð

(oð¡ , ð¶ð¡ ))

+ððð

ð (ðð,ð¡ )( Ëðð,ð¡ + ð¾ Ëðð ð
ðð
ð (ð ð,ð¡ )( Ëð ð,ð¡ + ð¾ Ëðð ð

ð

ð

(oð¡ +1, ð¶ð¡ +1) â Ëðð ð

ð

(oð¡ , ð¶ð¡ ))

(oð¡ +1, ð¶ð¡ +1) â Ëðð ð

ð

(oð¡ , ð¶ð¡ ))]

(13)

log ðð ð

ð

(ðð,ð¡ , ð¶ð¡ )

ð,ð¡ Ëðð,ð¡ â (cid:205)ðâð¶ð¡

It is interesting to note that the loss (13), obtained as a dual
of a constrained objective, is also a (negated) policy gradient for
ðð
a mixture of rewards [(cid:205)ð âð¶ð¡ Ëð ð,ð¡ + ððð
ð,ð¡ Ëð ð,ð¡ ].2 One
implication is that socially beneficial equilibria can be found by
simply optimizing a weighted sum of rewards, albeit with non-
stationary weights. Another implication is that the E constraint
effectively lowers the rewards of agents outside the coalition. We
stress that this does not mean that the agents outside the coalition
are punished, but rather that the agents inside the coalition cooper-
ate less frequently if the coalition is not full. This ensures that an
agent cannot deviate to enjoy the cooperation of others, i.e., deters
free-riding.

Commitment Window ð. When ð = 1, unanimous commitment
requires both constraints to be satisfied at each time-step, which
may limit the margin of social welfare improvement over selfish
agents. In contrast, when ð > 1, the constraints only need to be
satisfied on average over the periods of ð time-steps, since agents
commit for these periods. On the example of IC constraint (6), ð > 1
requires constraint reformulation:

âð, (ðð,ð¡ | ð¡ mod ð = 0) :

E(ð¶ð¡ |ð âð¶ð¡ ) [ðð (ðð,ð¡ | ð¶ð¡ )] â¥ E(ð¶ð¡ |ð âð¶ð¡ ) [ðð (ðð,ð¡ | ð¶ð¡ \ ð)]
where the coalition ð¶ð¡ is fixed for the next ð time-steps: ð¶ð¡ = ð¶ð¡ +1 =
Â· Â· Â· = ð¶ð¡ +ð â1. This constraint implies that the same ððð
ð (ðð,ð¡ ) is used
in the dual objective (9) at the time-steps ð¡ â¤ ð < ð¡ + ð. The update
rules of the Lagrange multipliers are modified accordingly:

(14)

2To see this, apply the definition of the value function, change the order of expectation
and summation in the value functions, and rearrange terms.

logððð

ð (ðð,ð¡ ) â log ððð

ð (ðð,ð¡ )

â ð¼

ð¡ +ð â1
âï¸

ð=ð¡

ð¾ð âð¡ [ Ëðð ð

ð

(oð , ð¶ð ) â Ëðð ð

ð

(oð , ð¶ð \ {ð})]

logðð

ð (ð ð,ð¡ ) â log ðð

ð (ð ð,ð¡ )

â ð¼

ð¡ +ð â1
âï¸

ð=ð¡

ð¾ð âð¡ [ Ëðð ð

ð

(oð¡ , ð¶ð¡ âª { ð }) â Ëðð ð

ð

(oð¡ , ð¶ð¡ )]

(15)

(16)

In the extreme case when the commitment window covers the
entire episode (ð = inf) and the starting state ð 0 is deterministic,
the dependency of Lagrange multipliers on the observation can
be dropped altogether. We respectively denote the mediators that
require constraint satisfaction each time-step (ð = 1), each several
time-steps (ð > 1), and each episode (ð = inf) as ex-post, interim,
and ex-ante.

Notes on the training process. First, in our implementations of the
mediator, no specific learning dynamics are assumed to be adopted
by the agents. We only train the agents within the same RL frame-
work as the mediator out of convenience. The only assumption is
that the agents are myopically rational, i.e., increase the probabil-
ity of the beneficial actions as they learn (and thus commit when
are properly incentivized to do so). This is unlike approaches to
conditional cooperation like LOLA that assume specific learning
dynamics.

Second, because the mediatorâs policy is trained for each coalition
rather than only the full coalition (see equations 4, 8), our objective
formulation for the mediator is stronger than finding a socially
beneficial mediated equilibrium. Instead, the mediator learns to
maximize social welfare while satisfying constraints even for coali-
tions that are not full. On the one hand, this makes cooperation
induced by the mediator robust to rare defectors. On the other hand,
discovering that simultaneous commitment is beneficial requires
some (possibly spontaneous) coordination of the agents, which is
easier if non-unanimous commitment is also incentivized. In short,
this property is useful during both training and execution.

4 EXPERIMENTS
In this section, we experimentally investigate the capabilities of
different proposed versions of the mediator in Prisonerâs Dilemma
(PD), Public Good Game (PGG), and iterative PGG. Technical de-
tails and hyperparameters, as well as additional experiments, are
reported in the Appendix.

In the Introduction, we have extensively discussed the issues
that arise in mixed MARL, i.e., finding policies that maximize social
welfare and incentivizing agents to follow these policies, which we
respectively denote as Efficiency (Eff) and Incentive-Compatibility
(IC) for convenience. While unconditional cooperation only ad-
dresses Eff, conditional cooperation addresses both and thus is
substantially more difficult to resolve. Given that unconditional co-
operation is already widely explored in the literature, we primarily
focus on IC. To this end, we empirically investigate matrix games
and their sequential variants that are trivial from the perspective
of Eff but clearly highlight the specific challenges when dealing

Table 2: Results in one-step PD. ð denotes Cooperate in PD and
Contribute in PGG; ð denotes Commit; ð and ð ð respectively
Ëð denotes policy
denote policies of agents and mediator;
averaged over 100 sampled episodes; reward is normalized
between 0 and 1. The same caption applies to other tables.
(â) no mediator (â ) naive mediator

PD
ð (ð) (â)
ð (ð) (â )
ð (ð) (â )
ð ð (ð | [0, 1]) (â )
ð ð (ð | [1, 0]) (â )
ð ð (ð | [1, 1]) (â )

agent 0

agent 1

0.004
0.96
0.006
-
0.009
0.979

0.001
0.967
0.003
0.01
-
0.979

with IC, as well as the power of mediators in dealing with these
challenges.

A consequence of this decision is that comparing with baselines
becomes redundant. We design each investigated game such that
we know what the socially optimal policy looks like and how much
social welfare it achieves. By design, this policy is trivial to find
for any solution to unconditional cooperation. We note that the
next logical step is validating our mediators in more complex envi-
ronments where both Eff and IC issues are non-trivial to solve, in
which case comparing with baselines also makes sense. We leave
this direction as future work.

4.1 Prisonerâs Dilemma
The payoff matrix is presented in Table 1 (a). Despite the coopera-
tion being mutually beneficial, defecting is the dominant strategy
and mutual defection is the only equilibrium. Table 1 (b) presents
socially optimal mediated equilibrium in PD implemented by a me-
diator that cooperates only when both agents commit. Notice that
despite this mediator greedily maximizing social welfare for each
coalition, both IC and E constraints are satisfied for both agents
because committing is a dominant strategy. Therefore, one can
expect even a Naive mediator to establish cooperation.

The results are presented in Table 2. In accordance with our
expectations, in the absence of the mediator, both agents converge
to defection. When the game is augmented with a Naive mediator,
the agents almost exclusively commit, and the mediator learns to
only cooperate when both agents commit.

4.2 Public Good Game
In two-player games, the mediator either acts for one of the agents,
in which case the best it can do is maximize the agentâs welfare, or
acts for both agents, in which case no agent is outside the coalition.
The consequence is that mediatorâs actions are never beneficial
for agents outside the coalition and therefore the Encouragement
constraint is always satisfied in two-agent games. This is not the
case in games with more than two agents where as soon as some
two agents start cooperating, the rest of the agents may try to
exploit their cooperation for higher personal gains. This issue is
known as free-riding. Using PGG as an example, we illustrate how

Table 3: Results in one-step PGG
(â) no mediator (â ) naive mediator (â¡) constrained mediator

Table 4: Results in Iterative PGG. The reported policies are
empirical approximations of the marginal probabilities.
(â) no mediator (â ) naive mediator (â¡) constrained mediator

PGG
reward(â)
reward(â )
Ëð (ð) (â )
Ëð ð (ð) (â )
ð ð (ð | |ð¶ | = 2) (â )
ð ð (ð | |ð¶ | = 3) (â )
reward(â¡)
Ëð (ð) (â¡)
Ëð ð (ð) (â¡)
ð ð (ð | |ð¶ | = 2) (â¡)
ð ð (ð | |ð¶ | = 3) (â¡)

ð = 3 ð = 10 ð = 25

0.012
0.652
0.658
0.985
0.993
0.999
0.891
0.916
0.959
0.774
0.996

0.0
0.005
0.159
0.001
-
-
0.827
0.961
0.858
-
-

0.0
0.014
0.121
0.02
-
-
0.817
0.933
0.817
-
-

free-riding emerges when the game is augmented with a Naive
mediator, and how this issue can be mitigated by enforcing the
Encouragement constraint on the mediatorâs policy. We refer to
such a mediator as Constrained. We first investigate one-step PGG
and then move on to our variant of iterative PGG.

ð=1

One-step Public Good Game. ð agents are endowed with a unit
of utility and have a choice whether to contribute it to the pub-
lic good or to defect. The public good is formed as the total con-
tribution of agents, multiplied by some 1 < ð < ð , and is uni-
formly redistributed among all agents. The reward of each agent is
ðð = ð
ð

ð ð â ðð , where ðð = 1 iff ð contributes.

(cid:205)ð

Let ð = 3, ð = 2. Like in PD, the dominant strategy in the
absence of a mediator is to defect. Consider the Naive mediator.
Its strategy is to contribute with all agents in the coalition if it
consists of at least two agents: ð ð (ð | |ð¶ | = 1) = 0, ð ð (ð | |ð¶ | â¥
2) = 1. Given this mediator, the equilibrium is for only two of three
agents to commit and form a coalition. To see this, consider the
perspectives of all agents. From the perspective of an agent in this
coalition, committing causes it to get its share of the public good
equal to ð (ð) = 1/3, whereas defecting would lower the reward to
ð (ð) = 0 by causing the other agent to defect. From the perspective
of the agent outside the coalition, defecting lets it enjoy both its
endowment and its share of the public good ð (ð) = 4/3, which is
better than committing with other agents and receiving ð (ð) = 1.
Now consider the Constrained mediator. To deter free-riding, the
Constrained mediator contributes with a specific probability when
the coalition consists of two agents: ð ð (ð |
|ð¶ | = 1) = 0, ð ð (ð |
|ð¶ | = 2) = 0.75, ð ð (ð | |ð¶ | = 3) = 1. On the one hand, this results
in a lower expected reward for the two agents in the coalition:
ð (ð) = 0.25. On the other hand, this also lowers the expected
reward of the agent outside the coalition to the point where it is
indifferent whether it commits or not: ð (ð) = ð (ð) = 1. Notice that
further decreasing ð ð (ð | |ð¶ | = 2) results in lower social welfare
for the agents in the coalition, whereas increasing it encourages the
third agent to defect. Thus, this mediator implements the socially
optimal equilibrium. We now verify that our constrained objective
allows training such a mediator.

N=3
reward(â)
reward(â )
Ëð (ð) (â )
Ëð ð (ð) (â )
reward(â¡)
Ëð (ð) (â¡)
Ëð ð (ð) (â¡)

ð = 1

ð = 10

0.019
0.145
0.667
0.999
0.478
0.8
0.997

0.017
0.963
0.991
0.993
0.986
0.995
0.999

N=10
reward(â)
reward(â )
Ëð (ð) (â )
Ëð ð (ð) (â )
reward(â¡)
Ëð (ð) (â¡)
Ëð ð (ð) (â¡)

ð = 1

ð = 10

0.0
0.0
0.079
0.159
0.729
0.898
0.945

0.0
0.788
0.852
0.932
0.787
0.907
0.941

The results are presented in Table 3. For ð = 3, ð = 2, the learned
policies match the equilibrium derived above: without mediator
agents always defect, the Naive mediator encourages two agents
to commit but is exploited by the third agent, and the Constrained
mediator converges to the socially optimal equilibrium. It is espe-
cially surprising that the Constrained mediator learns the optimal
mixed policy so precisely, which is only possible in a non-stationary
environment where the moment the mediator deviates, it is cor-
rected by the agents trying to exploit it. For settings ð = 10, ð = 2
and ð = 25, ð = 5, the picture is generally the same: only the
Constrained mediator encourages commitment from all agents by
learning a reciprocal policy that punishes free-riding.

Iterative Public Good Game. The game lasts for 10 turns. In the
beginning, each agent is endowed with 1 unit of utility. An agentâs
observation is a tuple of its current endowment and the turn number.
Each turn, each agent chooses whether to contribute 50% of its
current endowment to the public good, and the resulting payoffs
are preserved throughout the turns. This creates a compounding
effect from contributing to the public good that can be exploited for
a massive increase in welfare over the duration of the game if all
agents consistently contribute. On the other hand, the state space
is no longer trivial, and more complex strategies may emerge.

The results are presented in Table 4. For ð = 3, ð = 2, the Naive
ex-post (ð = 1) mediator behaves similarly to the Naive mediator in
one-step PGG: it consistently encourages two agents to commit but
is exploited by the third agent. The Constrained ex-post mediator
mitigates this issue, but only partially, which might be due to our
approximation of Lagrange multipliers as constants, or simply due
to the limited capabilities of ex-post mediators. Conversely, both
the Naive and the Constrained ex-ante (ð = 10) mediators reliably
encourage all three agents to commit and establish robust coopera-
tion. For ð = 10, ð = 5, the results are similar, but since the game is
more complex, even the Constrained ex-ante mediator is not able
to ensure full commitment.

4.3 Prisonerâs Dilemma with Sacrifice
Prisonerâs Dilemma with Sacrifice (PDS) is an asymmetric modifi-
cation of PD. The payoff matrix is presented in Table 5 and differs
from PD in that the second player has an additional action available,
the effect of which is to sacrifice its payoffs for the higher utilitarian
social welfare.

Table 5: Payoffs in PD with Sacrifice

Defect Cooperate

Defect
Cooperate

1, 1
0, 3

3, 0
2, 2

Sacrifice
5, 0
5, 0

Table 6: Payoffs in PD with Sacrifice augmented with a
Naive mediator, the strategy of which is to sacrifice the
second agentâs payoffs when both agents commit and defect
otherwise

Defect Cooperate

Sacrifice Commit

Defect
Cooperate
Commit

1, 1
0, 3
1, 1

3, 0
2, 2
3, 0

5, 0
5, 0
5, 0

1, 1
0, 3
5, 0

Table 7: Payoffs in PD with Sacrifice augmented with a
Constrained mediator, the strategy of which is to mutually
cooperate when both agents commit and defect otherwise

Defect Cooperate

Sacrifice Commit

Defect
Cooperate
Commit

1, 1
0, 3
1, 1

3, 0
2, 2
3, 0

5, 0
5, 0
5, 0

1, 1
0, 3
2, 2

Like in PD, when agents play PDS without a mediator, the domi-
nant action for both is to defect. Unlike PD, this does not change
when the game is augmented with the Naive mediator. Since the
Naive mediator greedily maximizes social welfare, it sacrifices the
payoffs of the second agent, which encourages the second agent
to defect (Table 6). This is an example of the incompatibility of an
agentâs and the mediatorâs incentives. To fix this, the IC constraint
should be enforced, which will cause the mediator to choose mu-
tual cooperation over sacrificing an agentâs payoffs and, in turn,
encourage the agents to commit (Table 7). This Constrained media-
tor implements the mediated equilibrium, but due to the asymmetry
of the game, its strategy is not socially optimal. Note that this is
not the only mediator that satisfies the IC constraint, as mixing
mutual cooperation with sacrificing the second agentâs payoffs may
also be viable while further improving social welfare. The mediated
equilibrium that maximizes social welfare is to equally mix (c, c)
and (Â·, s) outcomes since at this point the second agent is indifferent
to whether it commits or defects.

We now investigate how our implementations of mediators be-

have in PDS. The experimental results are presented in Table 8.

In accordance with our expectations, agents converge to mutual
defection both without a mediator and with a Naive mediator. The
Naive mediator learns to sacrifice the second agentâs payoffs while
defecting with the first agent, which causes the second agent to
always defect. The first agent is then indifferent to whether it defects
itself or commits to the mediator that defects for it.

The constrained mediator performs much better. As discussed
earlier, its optimal strategy is to equally mix (c, c) and (Â·, s) outcomes.
Its learned strategy is close to the optimal but gives a slight edge

Table 8: Learned policies in PDS. Note that the mediatorâs
joint policy is not factorized. {Â·} denotes any action.
(â) no mediator (â ) naive mediator (â¡) constrained mediator

agents(â)
agents(â )

mediator(â )

agents(â¡)

mediator(â¡)

ð (ð)
ð (ð)
ð (ð)
ð ð (ð, ð |[1, 1])
ð ð (Â·, ð  |[1, 1])
ð (ð)
ð (ð)
ð ð (ð, ð |[1, 1])
ð ð (Â·, ð  |[1, 1])

agent 0

agent 1

0.002
0.003
0.607

0.001
0.002
0.002

0.001
0.85

0.001
0.995

0.001
0.982

0.601
0.398

to the (c, c) outcome to additionally encourage the commitment of
the second agent. As a result, both agents almost always commit.
The converged dynamics result in social welfare of approximately
4.35, which is close to the maximal achievable social welfare of 4.5.

5 CONCLUSION
In this paper, we challenge the dominant perspective in the MARL
literature on the problem of cooperation in mixed environments
and argue for convergence to equilibria as its essential property. As
a novel solution for conditional cooperation, we apply mediators.
Specifically, we adapt mediators to Markov games through the for-
malism of Markov mediators, describe how to practically implement
them, formulate a constrained objective that both improves social
welfare and encourages agents to commit, solve this objective using
the method of Lagrange multipliers and dual gradient descent, and
experimentally verify the effectiveness of our implementation in
the matrix and iterative games.

Despite our contributions, we only scratch the surface of the
mediatorsâ potential for MARL. First, to get a clear picture of me-
diatorsâ behaviour and advantages, we experiment with relatively
simple games, but it would also be exciting to apply mediators to
larger-scale environments. Second, our formulation of mediator
implies its centralized execution as a way to ensure that agents can-
not misreport their commitment, but as Monderer and Tennenholtz
[35] point out, it is interesting whether cryptographic technologies
could be applied as an alternative. Third, in our implementation,
the mediator acts based on the same information as an agent, but
providing the mediator with more information could serve as an
additional incentive to commit. Fourth, the literature also explores
mediators that recommend actions instead of acting on behalf of
agents [26] and adapting such mediators to MARL presents a sepa-
rate challenge. On a final note, our Markov mediator is only one
example of ideas from economics to MARL, and we are excited for
future research that intersects these two fields.

ACKNOWLEDGMENTS
This research was supported in part through computational re-
sources of HPC facilities at HSE University, Russian Federation.

Support from the Basic Research Program of the National Research
University Higher School of Economics is gratefully acknowledged.

REFERENCES
[1] Bowen Baker. 2020. Emergent reciprocity and team formation from randomized
uncertain social preferences. Advances in Neural Information Processing Systems
33 (2020), 15786â15799.

[2] Tobias Baumann, Thore Graepel, and John Shawe-Taylor. 2020. Adaptive mecha-
nism design: Learning to promote cooperation. In 2020 International Joint Con-
ference on Neural Networks (IJCNN). IEEE, 1â7.

[3] Theodore Bergstrom, Lawrence Blume, and Hal Varian. 1986. On the private
provision of public goods. Journal of public economics 29, 1 (1986), 25â49.
[4] Jan Blumenkamp and Amanda Prorok. 2021. The emergence of adversarial
communication in multi-agent reinforcement learning. In Conference on Robot
Learning. PMLR, 1394â1414.

[5] Wendelin BÃ¶hmer, Vitaly Kurin, and Shimon Whiteson. 2020. Deep coordination
graphs. In International Conference on Machine Learning. PMLR, 980â991.
[6] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. 2004. Convex opti-

mization. Cambridge university press.

[7] Matteo Castiglioni, Alberto Marchesi, Andrea Celli, and Nicola Gatti. 2021. Multi-
receiver online bayesian persuasion. In International Conference on Machine
Learning. PMLR, 1314â1323.

[8] Phillip J.K. Christoffersen, Andreas A. Haupt, and Dylan Hadfield-Menell. 2023.
Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL.
In Proceedings of the 2023 International Conference on Autonomous Agents and
Multiagent Systems (London, United Kingdom) (AAMAS â23). International Foun-
dation for Autonomous Agents and Multiagent Systems, Richland, SC, 448â456.
[9] David Cittern and Abbas Edalat. 2015. Reinforcement Learning for Nash Equilib-

rium Generation.. In AAMAS. 1727â1728.

[10] Vincent Conitzer. 2019. Designing preferences, beliefs, and identities for artificial
intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33.
9755â9759.

[11] Rachel Cummings, Michael Kearns, Aaron Roth, and Zhiwei Steven Wu. 2015.
Privacy and truthful equilibrium selection for aggregative games. In International
Conference on Web and Internet Economics. Springer, 286â299.

[12] Ishan Durugkar, Elad Liebman, and Peter Stone. 2020. Balancing individual
preferences and shared objectives in multiagent reinforcement learning. Good
Systems-Published Research (2020).

[13] Tom Eccles, Edward Hughes, JÃ¡nos KramÃ¡r, Steven Wheelwright, and Joel Z
Leibo. 2019. Learning reciprocity in complex sequential social dilemmas. arXiv
preprint arXiv:1903.08082 (2019).

[14] Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter
Abbeel, and Igor Mordatch. 2018. Learning with Opponent-Learning Awareness.
In Proceedings of the 17th International Conference on Autonomous Agents and
MultiAgent Systems. 122â130.

[15] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shi-
mon Whiteson. 2018. Counterfactual multi-agent policy gradients. In Proceedings
of the AAAI conference on artificial intelligence, Vol. 32.

[16] Sanford J Grossman and Oliver D Hart. 1992. An analysis of the principal-agent

problem. Springer.

[17] Carlos Guestrin, Daphne Koller, and Ronald Parr. 2002. Multiagent planning with
factored MDPs. In Advances in neural information processing systems. 1523â1530.
[18] Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. 2017. Cooperative
multi-agent control using deep reinforcement learning. In International conference
on autonomous agents and multiagent systems. Springer, 66â83.

[19] Edward Hughes, Joel Z Leibo, Matthew Phillips, Karl Tuyls, Edgar DueÃ±ez-
Guzman, Antonio GarcÃ­a CastaÃ±eda, Iain Dunning, Tina Zhu, Kevin McKee,
Raphael Koster, et al. 2018. Inequity aversion improves cooperation in intertem-
poral social dilemmas. In Advances in neural information processing systems.
3326â3336.

[20] Aly Ibrahim, Anirudha Jitani, Daoud Piracha, and Doina Precup. 2020. Reward
redistribution mechanisms in multi-agent reinforcement learning. In Adaptive
Learning Agents Workshop at the International Conference on Autonomous Agents
and Multiagent Systems.

[21] Dmitry Ivanov, Vladimir Egorov, and Aleksei Shpilman. 2021. Balancing Rational
and Other-Regarding Preferences in Cooperative-Competitive Environments.
In Proceedings of the 20th International Conference on Autonomous Agents and
MultiAgent Systems. 1536â1538.

[22] Dmitry Ivanov, Iskander Safiulin, Igor Filippov, and Ksenia Balabaeva. 2022.
Optimal-er Auctions through Attention. In Advances in Neural Information Pro-
cessing Systems, Vol. 35.

[23] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro
Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. 2019. Social influence as
intrinsic motivation for multi-agent deep reinforcement learning. In International
Conference on Machine Learning. PMLR, 3040â3049.

[24] Jiechuan Jiang and Zongqing Lu. 2019. Learning fairness in multi-agent systems.

Advances in Neural Information Processing Systems 32 (2019).

[25] Emir Kamenica and Matthew Gentzkow. 2011. Bayesian persuasion. American

Economic Review 101, 6 (2011), 2590â2615.

[26] Michael Kearns, Mallesh Pai, Aaron Roth, and Jonathan Ullman. 2014. Mechanism
design in large games: Incentives and privacy. In Proceedings of the 5th conference
on Innovations in theoretical computer science. 403â410.

[27] Michael Kearns, Mallesh M Pai, Ryan Rogers, Aaron Roth, and Jonathan Ullman.

2015. Robust mediators in large games. arXiv preprint arXiv:1512.02698 (2015).
[28] Sachin G Konan, Esmaeil Seraj, and Matthew Gombolay. 2022. Iterated Reasoning
with Mutual Information in Cooperative and Byzantine Decentralized Teaming.
In International Conference on Learning Representations.

[29] Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic algorithms. In Advances

in neural information processing systems. 1008â1014.

[30] Adam Lerer and Alexander Peysakhovich. 2017. Maintaining cooperation in
complex social dilemmas using deep reinforcement learning. arXiv preprint
arXiv:1707.01068 (2017).

[31] Yue Lin, Wenhao Li, Hongyuan Zha, and Baoxiang Wang. 2023. Information
Design in Multi-Agent Reinforcement Learning. arXiv preprint arXiv:2305.06807
(2023).

[32] Michael L Littman. 1994. Markov games as a framework for multi-agent rein-
forcement learning. In Machine learning proceedings 1994. Elsevier, 157â163.
[33] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor
Mordatch. 2017. Multi-agent actor-critic for mixed cooperative-competitive
environments. Advances in neural information processing systems 30 (2017).
[34] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim-
othy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn-
chronous methods for deep reinforcement learning. In International conference
on machine learning. 1928â1937.

[35] Dov Monderer and Moshe Tennenholtz. 2009. Strong mediated equilibrium.

Artificial Intelligence 173, 1 (2009), 180â195.

[36] Michael Noukhovitch, Travis LaCroix, Angeliki Lazaridou, and Aaron Courville.
2021. Emergent Communication under Competition. In Proceedings of the 20th
International Conference on Autonomous Agents and MultiAgent Systems. 974â982.
[37] Caspar Oesterheld, Johannes Treutlein, Roger Grosse, Vincent Conitzer, and Jakob
Foerster. 2022. Similarity-based Cooperation. arXiv preprint arXiv:2211.14468
(2022).

[38] David C Parkes and Michael P Wellman. 2015. Economic reasoning and artificial

intelligence. Science 349, 6245 (2015), 267â272.

[39] Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine.
2018. Variational Discriminator Bottleneck: Improving Imitation Learning, In-
verse RL, and GANs by Constraining Information Flow. In International Confer-
ence on Learning Representations.

[40] Julien Perolat, Joel Z Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, and
Thore Graepel. 2017. A multi-agent reinforcement learning model of common-
pool resource appropriation. Advances in Neural Information Processing Systems
30 (2017).

[41] Alexander Peysakhovich and Adam Lerer. 2018. Consequentialist conditional
cooperation in social dilemmas with imperfect information. In International
Conference on Learning Representations.

[42] Alexander Peysakhovich and Adam Lerer. 2018. Prosocial learning agents solve
generalized stag hunts better than selfish ones. In Proceedings of the 17th Interna-
tional Conference on Autonomous Agents and MultiAgent Systems. International
Foundation for Autonomous Agents and Multiagent Systems, 2043â2044.
[43] Thomy Phan, Felix Sommer, Philipp Altmann, Fabian Ritz, Lenz Belzner, and
Claudia Linnhoff-Popien. 2022. Emergent Cooperation from Mutual Acknowledg-
ment Exchange. In Proceedings of the 21st International Conference on Autonomous
Agents and Multiagent Systems. 1047â1055.

[44] Anatol Rapoport, Albert M Chammah, and Carol J Orwant. 1965. Prisonerâs
dilemma: A study in conflict and cooperation. Vol. 165. University of Michigan
press.

[45] Ryan M Rogers and Aaron Roth. 2014. Asymptotically truthful equilibrium
selection in large congestion games. In Proceedings of the fifteenth ACM conference
on Economics and computation. 771â782.

[46] Ariel Rubinstein. 1998. Modeling bounded rationality. MIT press.
[47] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-

duction. MIT press.

[48] Moshe Tennenholtz. 2004. Program equilibrium. Games and Economic Behavior

49, 2 (2004), 363â373.

[49] Jane X Wang, Edward Hughes, Chrisantha Fernando, Wojciech M Czarnecki,
Edgar A DuÃ©Ã±ez-GuzmÃ¡n, and Joel Z Leibo. 2019. Evolving intrinsic motiva-
tions for altruistic behavior. In Proceedings of the 18th International Conference
on Autonomous Agents and MultiAgent Systems. International Foundation for
Autonomous Agents and Multiagent Systems, 683â692.

[50] Timon Willi, Alistair Hp Letcher, Johannes Treutlein, and Jakob Foerster. 2022.
COLA: consistent learning with opponent-learning awareness. In International
Conference on Machine Learning. PMLR, 23804â23831.

[51] Jiachen Yang, Ang Li, Mehrdad Farajtabar, Peter Sunehag, Edward Hughes, and
Hongyuan Zha. 2020. Learning to incentivize other learning agents. Advances in
Neural Information Processing Systems 33 (2020), 15208â15219.

Table 10: Learned policies in Two-step PD
(â) no mediator (â ) ex-post naive mediator (ð = 1) (â¡) ex-ante
naive mediator (ð = 2)

STATE 0
ð (ð¶) (â)
ð (ð¶) (â )
ð (ð) (â )
ð ð (ð¶ | [0, 1]) (â ) â
ð ð (ð¶ | [1, 0]) (â )
ð ð (ð¶ | [1, 1]) (â )
ð (ð¶) (â¡)
ð (ð) (â¡)
ð ð (ð¶ | [0, 1]) (â¡) â
ð ð (ð¶ | [1, 0]) (â¡)
ð ð (ð¶ | [1, 1]) (â¡)

agent 0

0.0007
0
0.0128

0.0088
0.9987
0
0.9913

0.027
0.9985

agent 0

0.0005
0
0.9966

STATE 1
ð (ð¶) (â)
ð (ð¶) (â )
ð (ð) (â )
ð ð (ð¶ | [0, 1]) (â ) â
ð ð (ð¶ | [1, 0]) (â )
0.0067
ð ð (ð¶ | [1, 1]) (â )
0.9993
ð (ð¶) (â¡)
0
ð (ð) (â¡)
â
ð ð (ð¶ | [0, 1]) (â¡) â
ð ð (ð¶ | [1, 0]) (â¡)
ð ð (ð¶ | [1, 1]) (â¡)

0.0154
0.9948

agent 1

0.0006
0
0.9996
0.0005
â
0.9976
0
0.9995
0.0624
â
0.9982

agent 1

0.0006
0.0001
0.9293
0.0003
â
0.9977
0
â
0.0028
â
0.9933

Table 9: Two-step Prisonerâs Dilemma

STATE 0
Defect
Cooperate

Defect Cooperate

0, 0
-5, 7

7, -5
-1, 4

STATE 1
Defect
Cooperate

Defect Cooperate

0, 0
-5, 7

7, -5
2, 2

[52] Jiachen Yang, Alireza Nakhaei, David Isele, Kikuo Fujimura, and Hongyuan
Zha. 2020. CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement
Learning. In International Conference on Learning Representations.

[53] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. 2018.
Fully decentralized multi-agent reinforcement learning with networked agents.
In International Conference on Machine Learning. PMLR, 5872â5881.

[54] Stephen Zhao, Chris Lu, Roger Baker Grosse, and Jakob Nicolaus Foerster. 2022.
Proximal Learning With Opponent-Learning Awareness. Advances in Neural
Information Processing Systems 35 (2022).

[55] Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck,
David C Parkes, and Richard Socher. 2020. The ai economist: Improving equality
and productivity with ai-driven tax policies. arXiv preprint arXiv:2004.13332
(2020).

[56] Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C. Parkes, and Richard
Socher. 2022. The AI Economist: Taxation policy design via two-level deep
multiagent reinforcement learning. Science Advances 8, 18 (2022), eabk2607.
https://doi.org/10.1126/sciadv.abk2607

[57] Matthieu Zimmer, Claire Glanois, Umer Siddique, and Paul Weng. 2021. Learning
fair policies in decentralized cooperative multi-agent reinforcement learning. In
International Conference on Machine Learning. PMLR, 12967â12978.

A ADDITIONAL EXPERIMENTS
A.1 Two-step Asymmetric Prisonerâs Dilemma
This modification of PD lasts for two time-steps, the payoff matrix
for both of which is provided in Table 9. The second state coincides
with PD in the main text, but the first state is different in that
mutual cooperation is only beneficial for the second agent while
still providing maximal social welfare. Like in one-step PD, the only
equilibrium is to defect for both agents in the absence of mediator.
The ex-post Naive mediator chooses mutual cooperation in both
states, which both agents agree to in the second state but only the
second agent agrees to in the first state. The ex-ante Naive mediator
also chooses mutual cooperation in both states, but the agents can
only commit at the first state for the duration of the game. In this
case, commitment is beneficial for both agents since the cumulative
reward over two time-steps is higher from mutual cooperation than
from mutual defection even for the first agent.

The experimental results are presented in Table 10 and are fully
in accordance with our expectations. Agents always defect without
mediator; the second agent commits in both states while the first
agent only commits in the second state to the ex-post Naive me-
diator; both agents commit in the first state to the ex-ante Naive
mediator. This experiment clearly demonstrates how ex-ante medi-
ator has more potential to maximize social welfare because it only
requires to satisfy the constraints (to be compatible with the agentsâ
incentives) on average.

B TECHNICAL DETAILS AND
HYPERPARAMETERS

Prisonerâs Dilemma. In PD, agentsâ actor and critic receive a
constant dummy state, since it is a one-step game with single state.
Mediatorâs actor also receives coalition and ID of the agent that
the mediator acts for. Mediatorâs critic only receives coalition and
predicts values for both agents simultaneously. The algorithm of
inference is the same in all environments: first, agents choose an
action, then if any of them chose to cooperate, its ID alongside
with coalition are passed to the mediator, which takes actions for
these agents. After that, actions are sent to the environment to
obtain rewards. The training is performed in the usual manner for
Actor-Critic algorithms. The final result is averaged over 50 seeds.
Prisonerâs Dilemma with Sacrifice. We bound log ð to [â4; 4] to
avoid cases when the constraint is completely ignored or completely
dominates the main objective. The rest of the details are similar to
the PD. The final result is averaged over 50 seeds.

Two-step Asymmetric Prisonerâs Dilemma. Agentsâ actors and
critics receive the time-step ð¡. Also, actors receive an additional
value ð  â [â1, 0, 1] that indicates the coalition status of an agent:
["cannot join the coalition", "can choose to join the coalition", "in
coalition, acts according to mediator"]. Depending on the value,
we modify the logits predicted by actor according to the following
strategies. For ð  = â1, we mask the value corresponding to the
action "commit" by changing it to ââ (this happens when ð > 1,
ð¡ mod ð â  0, and the agent did not commit the last time it could).
The same applies for ð  = 1, but in this case we mask all actions but
"commit" (this happens when ð > 1, ð¡ mod ð â  0, and the agent
committed the last time it could). In case of ð  = 0, neither mask
is applied as all actions are available. During training, we mask

logits in the same manner according to the collected trajectories
to ensure unbiased on-policy learning. Agents are only trained on
the experience where ð  = â1 or ð  = 0 because when ð  = 1, agents
effectively act off-policy (as mediator acts for them). Note that ð  = 0
always if ð = 1.

Since we use ex-ante mediator, we employ a ð-step learning pro-
cedure for agents explained in the main text under section "Practical
Implementations of Agents and Mediator". The rest of the details
are similar to the PD. The final result is averaged over 50 seeds.

Public Good Game. Considering the high number of agents in
PGG ð â {3, 10, 25}, we changed the multi-headed mediatorâs
critic. Instead, the critic takes only the number of agents in coalition
(normalized by ð ), and outputs value ð corresponding to each agent
in the coalition. Likewise, the mediatorâs actor also doesnât return
a unique policy for each agent. Instead, it outputs the same policy
for all agents in the coalition. This way, we utilize the symmetry of

the game to reduce the space of solutions. It is important to note
that each agent still has its own actor and critic networks that do
not share parameters with other agents. The rest of the details are
similar to the PD. The final result is averaged over 10 seeds.

Iterative Public Good Game. In the Iterative PGG, we provide
the private observation ðð,ð¡ = (ðð,ð¡ , ð¡) to agentsâ actors and critics,
where ðð,ð¡ is the ð-th agent current endowment. Mediatorâs actor
receives a tuple (ðð,ð¡ , ð¶ð¡ , ð) consisting of agentâs private observation,
coalition, and agentâs ID, and outputs the policy for this agent.
Mediatorâs critic receives a tuple (ð ð¡ = (ðð,ð¡ )ð âð , ð¶) consisting of
the global state and the coalition and returns a vector of values ð
of all agents. The rest of the details (including masking logits for
ð > 1) are the same as in Two-step PD. The final result is averaged
over 10 seeds.

Hyperparameters. All hyperparameters are reported in Table 11.

Table 11: Hyperparameters for all environments

PD

Two-step PD Sacrifice PD

Batch Size
Iterations
Discount factor ð¾
Agent

Entropy Coef. Start
Entropy Decay Strategy
Entropy Decay
Entropy Steps
Min. Entropy Coef.
LR Critic
LR Actor
Hidden Layer size
# Layers

Mediator

Entropy Coef. Start
Entropy Decay Strategy
Entropy Decay
Entropy Steps
Min. Entropy Coef.
LR Critic
LR Actor
LR Lambda
# Hidden Layers
# Layers

128
2000
0.99

1
Linear
0.0005
â
0.001
8e-4
4e-4
8
2

1
Linear
0.0005
â
0.001
1e-3
8e-4
â
8
2

128
2000
0.99

1
Linear
0.0007
â
0.001
8e-4
4e-4
8
2

1
Linear
0.0007
â
0.001
1e-3
8e-4
â
8
2

128
10000
0.99

0.5
Linear
0.00004
â
0.01
1e-3
1e-3
16
2

0.5
Linear
0.00004
â
0.01
1e-3
1e-3
1e-3
32
2

PGG

128
20000
0.99

0.5

PGG-iter

128
20000
0.99

0.2

Exponential Exponential

20000
20000
0.01
1e-3
1e-3
16
2

0.5

10000
10000
0.001
1e-3
5e-4
16
2

0.2

Exponential Exponential

20000
20000
0.01
1e-3
1e-3
1e-3
16
2

10000
10000
0.001
1e-3
5e-4
1e-3
16
2

